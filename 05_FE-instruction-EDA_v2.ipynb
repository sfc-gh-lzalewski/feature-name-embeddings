{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from rich.markdown import Markdown\n",
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "from umap.umap_ import UMAP\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/code_FE_method_extraction/gpt-3.5-turbo-1106/v2/answers.json', 'r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m300\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1396</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1396\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(json_data))\n",
    "\n",
    "methods_list = []\n",
    "for record in json_data:\n",
    "    methods_list += record[\"methods\"]\n",
    "\n",
    "print(len(methods_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data type conversion <br>Converted data types of certain columns to optimize memory usage and improve performance.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [item[\"method\"] + \" <br>\" + item[\"execution\"] for item in methods_list]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1396 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1396/1396 [07:27<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "client = openai.Client()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   # text = text.replace(\"\\n\", \" \")\n",
    "   response = client.embeddings.create(input = [text], model=model)\n",
    "   return response.data[0].embedding\n",
    "\n",
    "embeddings = [get_embedding(text) for text in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "reducer = UMAP()\n",
    "embeddings_umap = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "%{hovertext}",
         "hovertext": [
          "Data type conversion <br>Converted data types of certain columns to optimize memory usage and improve performance.",
          "Feature creation <br>Created a new feature 'disc_sum' by summing up selected discrete features.",
          "Categorical feature manipulation <br>Created new features by comparing continuous features with their mean values and converting the comparisons to categorical values.",
          "Scaling using RobustScaler <br>The data is scaled using RobustScaler to handle outliers and standardize the features.",
          "Adding noise to the training data <br>A small amount of noise is added to the training data to reduce overfitting.",
          "Date conversion <br>The code converts the 'date' column to datetime format and then extracts date parts such as month, day, day of week, and hour as additional columns.",
          "One-Hot Encoding <br>The code uses one-hot encoding to convert categorical variables ('country', 'store', 'product') into numerical format for modeling.",
          "Principal Component Analysis (PCA) <br>The code applies PCA to reduce the dimensions of the one-hot encoded categorical variables to 2 components.",
          "Normalization <br>The code uses MinMaxScaler to normalize the numerical features in the dataset.",
          "Label Encoding <br>The 'LabelEncoder' from the 'sklearn.preprocessing' module is used to transform the target variable into numerical labels.",
          "Stratified K-Folds Cross-Validation <br>The 'StratifiedKFold' from the 'sklearn.model_selection' module is used to create stratified folds for cross-validation, ensuring that each fold has the same distribution of target classes as the whole dataset.",
          "Random Forest Classifier <br>The 'RandomForestClassifier' from the 'sklearn.ensemble' module is used for classification, which inherently performs feature selection and can handle non-linear relationships between features and the target variable.",
          "Checking for null values <br>The code checks for null values in the 'train' dataset using the isnull() method and sum() function.",
          "Removing redundant samples <br>The code removes redundant samples from the 'train' dataset using the drop_duplicates() method.",
          "Data visualization <br>The code uses various visualization libraries such as matplotlib, seaborn, and plotly to visualize the data distribution and relationships between features.",
          "Correlation analysis <br>The code calculates the correlation between features and the target variable using the corr() method.",
          "Standard Scaling <br>Applied to both training and test datasets using StandardScaler",
          "PCA (Principal Component Analysis) <br>Applied to identify the right number of features and to train and test datasets",
          "LDA (Linear Discriminant Analysis) <br>Code is present but commented out, not executed",
          "Kernel PCA (Kernel Principal Component Analysis) <br>Code is present but commented out, not executed",
          "MatchType mapping <br>The code applies a mapping function to the 'matchType' column to categorize the types into 'solo', 'duo', or 'squad'. This is achieved by using the lambda function and the apply method.",
          "Feature merging and ranking <br>The code merges dataframes, ranks the 'winPlacePerc' within each match, and calculates the adjusted percentage based on the rank and the number of groups in each match.",
          "Handling edge cases <br>The code deals with edge cases by setting 'winPlacePerc' to 0 or 1 for matches with maxPlace of 0 or 1, and adjusts 'winPlacePerc' based on the maxPlace and numGroups for other cases.",
          "Creating statistical features <br>Statistical features such as mean, standard deviation, max, min, quantiles, absolute values, moving averages, etc. are calculated from the raw input data.",
          "Signal processing features <br>Features such as Hilbert mean, Hann window mean, classic short-term/long-term average, rolling standard deviation, rolling mean, etc. are derived from signal processing techniques applied to the raw input data.",
          "Trend feature extraction <br>Trend features are extracted using linear regression to capture the trend in the raw input data.",
          "Outlier detection <br>Features related to outlier detection such as count of values exceeding a certain threshold, max-to-min ratio, etc. are calculated to identify outliers in the raw input data.",
          "Time series analysis <br>Features related to time series analysis such as mean change rate, mean change rate for specific segments, interquartile range, etc. are derived from the raw input data.",
          "Exponential moving average <br>Exponential moving average features are calculated to capture the trend and smooth out noise in the raw input data.",
          "Random initialization of virus locations <br>Randomly selects initial virus locations within the grid size",
          "Creating state matrix <br>Creates a matrix to represent the health state of individuals in the simulation",
          "Activity function <br>Calculates the activity level based on health state and incubation period",
          "Gaussian mechanism for virulence <br>Calculates the virulence based on the number of infected cases and grid size",
          "Spread probability calculation <br>Calculates the probability of virus spread to neighboring sites based on distance and health state",
          "Simulation of virus spread <br>Simulates the spread of the virus over time and saves the results",
          "Infection growth visualization <br>Visualizes the growth of infected cases over time",
          "Infection heatmap visualization <br>Visualizes the spread of infection as a heatmap over time",
          "Empirical data analysis <br>Analyzes the increase in confirmed cases from empirical data",
          "Comparison of simulation results with empirical data <br>Compares the simulated infection patterns with empirical data to classify regions into different strategies",
          "Prediction of confirmed cases <br>Predicts the number of confirmed cases based on simulated infection patterns",
          "Converting Date to datetime format <br>pd.to_datetime(data['Date'],format = '%Y-%m-%d')",
          "Mapping StateHoliday to numerical values <br>data_merged['StateHoliday']=data_merged['StateHoliday'].map({'0':0 , 0:0, 'a':1, 'b':2, 'c':3})",
          "Mapping Assortment to numerical values <br>data_merged['Assortment']=data_merged['Assortment'].map({ 'a':1, 'b':2, 'c':3})",
          "Mapping StoreType to numerical values <br>data_merged['StoreType'] = data_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})",
          "Mapping PromoInterval to numerical values <br>data_merged['PromoInterval'] = data_merged['PromoInterval'].map(map_promo)",
          "Handling missing values in store dataset <br>Filling missing values in various columns of the store dataset",
          "Creating new features 'day', 'month', and 'year' from Date <br>data_merged['day']=data_merged['Date'].dt.day, data_merged['month']=data_merged['Date'].dt.month, data_merged['year']=data_merged['Date'].dt.year",
          "Quantile binning <br>The code uses the pd.qcut function to perform quantile binning on the input data, creating 50 bins for each feature.",
          "Exponential transformation <br>Applied to combine predictions from different models by exponentiating the difference between the model's score and a reference score",
          "Geometric rounding <br>Applied to round the final predictions using a geometric rounding function based on the square root of the floor and ceiling of the predictions",
          "Label Encoding <br>Label encoding is applied to convert categorical event names into numerical values using sklearn's LabelEncoder.",
          "XGBoost Model <br>XGBoost model is used for predictive modeling, which involves feature engineering techniques such as handling missing values, categorical variable manipulation, and automatic feature extraction.",
          "Data loading <br>The code loads the data from CSV files using pandas read_csv method.",
          "Data cleaning <br>The code drops columns 'id', 'author', and 'geometry' from the dataset, which can be considered as a data cleaning process.",
          "Missing value imputation <br>The code uses KNNImputer from sklearn to impute missing values in the dataset.",
          "Feature selection <br>The code selects the 'x_e_out [-]' column from the dataset and uses it to create the submission file, indicating that this column is selected as a feature for the model.",
          "Handling missing values <br>The code fills missing values in the 'Province_State' column with an empty string using the fillna method.",
          "Creating new features <br>The code creates a new feature 'Region' by concatenating 'Country_Region' and 'Province_State' columns.",
          "Date manipulation <br>The code extracts day numbers from the date and calculates the start day for the test data relative to the train data.",
          "Curve fitting <br>The code uses curve fitting to fit the sigmoid function to the data for both confirmed cases and fatalities.",
          "Handling failed data <br>The code handles failed curve fitting by using mean coefficients and setting start values for failed cases.",
          "Handling missing data <br>Imputing missing values with the most frequent value for categorical features",
          "Label Encoding <br>Converting categorical variables to numerical format using LabelEncoder",
          "Mapping ordinal variables <br>Mapping ordinal variables to numerical values based on predefined mappings",
          "Cyclical feature transformation <br>Transforming day and month features into sine and cosine components",
          "Standardization <br>Standardizing numerical features using mean and standard deviation",
          "Creation of new features based on existing variables <br>The UrineAnalysis class is used to create new features such as Osmo_gravity, PH_calc, PH_urea, particle_calc, and particle_urea by performing calculations on existing variables.",
          "Data rescaling <br>The RescaleData class is used to rescale the data using standard scaling or min-max scaling.",
          "Data normalization <br>The NormalizeData class is used to normalize the data using L2 normalization.",
          "Feature selection <br>The FeatureSelection class is used to select the top k features using SelectKBest with f_regression as the scoring function.",
          "Data Loading <br>Loading data from CSV files using pandas read_csv method with specific data types and columns",
          "Data Type Conversion <br>Converting data types of specific columns using astype method",
          "Timestamp Mapping <br>Mapping timestamps to specific columns using map method",
          "Date Extraction <br>Extracting date information from specific columns using datetime and timedelta",
          "Histogram Plotting <br>Creating histogram plots with dual y-axes for visualization and analysis",
          "Density Plotting <br>Creating density plots and boxplots for visualization and analysis",
          "Feature Discretization <br>Discretizing features into bins for analysis and visualization",
          "Feature Importance Analysis <br>Analyzing feature importance using LGBMClassifier and feature_importances_ attribute",
          "Data Merging <br>Merging data from different sources using merge method",
          "Model Training <br>Training a LightGBM model for predictive analysis",
          "Prediction <br>Making predictions using the trained model and saving the results to a CSV file",
          "Dropping columns <br>Columns 'bin_0', 'ord_5', 'ord_5_2', 'nom_1', 'nom_3', 'day' were dropped from the dataset.",
          "Mapping categorical values to numerical values <br>Categorical values in 'bin_3', 'bin_4', 'ord_1', 'ord_2' were mapped to numerical values.",
          "Feature extraction from ordinal variable 'ord_5' <br>Extracted the first and second characters from the 'ord_5' column and dropped the original 'ord_5' column.",
          "Mapping ordinal values to numerical values <br>Ordinal values in 'ord_1' and 'ord_2' were mapped to numerical values.",
          "Mapping string values to ASCII index <br>Mapped string values in 'ord_3', 'ord_4', 'ord_5_1' to their corresponding ASCII index values.",
          "Handling XOR values in categorical columns <br>Identified and replaced XOR values in columns 'nom_7', 'nom_8', 'nom_9' with 'xor' label.",
          "Handling infrequent values <br>Replaced infrequent values in column 'nom_9' with 'value' label.",
          "Mapping values based on a predefined map <br>Mapped values in the 'day' column based on a predefined map.",
          "One-Hot Encoding <br>Performed one-hot encoding on categorical features 'nom_1', 'nom_3', 'day', 'nom_8'.",
          "Thermometer Encoding <br>Performed thermometer encoding on ordinal features 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'month', 'ord_5_1'.",
          "Basic statistics <br>Calculating mean, standard deviation, maximum, minimum, mean change, absolute maximum, absolute mean, absolute standard deviation, harmonic mean, geometric mean, k-statistic, moments, and aggregations on various slices of data.",
          "Trend feature <br>Calculating the trend of the data using linear regression.",
          "Rolling statistics <br>Calculating rolling mean and standard deviation for various window sizes, and then deriving features from these rolling statistics.",
          "Exponential rolling statistics <br>Calculating exponential moving average and standard deviation for various spans, and then deriving features from these exponential rolling statistics.",
          "Hann window mean <br>Calculating the mean of the signal convolved with Hann window for different window sizes.",
          "Classic STA/LTA <br>Calculating short-term average to long-term average ratios for different window sizes.",
          "Change rate <br>Calculating the change rate of the data and on slices of data.",
          "Percentiles <br>Calculating various percentiles on original and absolute values of the data.",
          "Absolute energy and sum of changes <br>Calculating absolute energy and absolute sum of changes of the data.",
          "Counting <br>Counting the number of values above and below mean, number of values larger than standard deviation, and number of crossings through zero.",
          "Binned entropy <br>Calculating binned entropy for different percentiles.",
          "Number of peaks <br>Counting the number of peaks in the data for different thresholds.",
          "Spectral features <br>Calculating spectral features such as Welch density and time reversal asymmetry statistic for different coefficients.",
          "Autocorrelation and C3 <br>Calculating autocorrelation and C3 for different lags.",
          "Range count <br>Counting the number of values within specified ranges.",
          "Handling missing values in D_63 and D_64 columns <br>Filled missing values in D_63 and D_64 columns with specific values and then created dummy variables for these columns",
          "Data cleaning and preprocessing <br>Filled missing values in the dataset with mean values for each column",
          "Feature creation <br>Created new features by applying get_dummies method to convert categorical variables into indicator variables",
          "Extracting features from a combined column <br>The code splits the 'EC1_EC2_EC3_EC4_EC5_EC6' column into individual columns for each EC value and assigns them to new columns 'EC1', 'EC2', 'EC3', 'EC4', 'EC5', and 'EC6'. The original combined column is then dropped from the dataset.",
          "Correlation analysis <br>The code calculates the correlation matrix for the features and visualizes it using a heatmap to identify relationships between variables.",
          "Histogram plotting <br>The code creates histograms for the numerical features in the dataset to visualize their distributions and identify patterns.",
          "Handling missing values <br>Filling null values of the billed columns with median, and filling null values of 'Age' with median",
          "Handling missing values <br>Filling null values of categorical columns with mode",
          "Outlier detection and treatment <br>Calculating IQR and replacing outliers with median for billed columns",
          "Data type conversion <br>Converting numerical columns to integer type",
          "Label encoding <br>Converting categorical variables to numerical values using LabelEncoder",
          "Feature scaling <br>Using MinMaxScaler and StandardScaler to scale numerical features",
          "Label Encoding <br>The 'encode_data' function is used to label encode the 'idhogar' field in the dataframe.",
          "Feature Creation <br>The 'do_features' function is used to create new features by performing division and subtraction operations on existing columns.",
          "Aggregation <br>The 'do_features' function is used to aggregate numerical and categorical features over the household and create new aggregated features.",
          "One-Hot Encoding to Label Encoding Conversion <br>The 'convert_OHE2LE' function is used to convert one hot encoded fields to label encoding.",
          "Geographical Aggregation <br>The 'convert_geo2aggs' function is used to aggregate features by geography and create new aggregated features.",
          "Feature Extraction <br>The 'extract_features' function is used to extract new features from existing columns.",
          "Handling missing values <br>Filled missing values in 'Age' and 'Embarked' columns with median and 'S' respectively. Also filled missing values in 'Fare' column in the test dataset with median.",
          "Binning numerical data <br>Binned 'Age' and 'Fare' columns into categories based on specific ranges.",
          "Creating new features <br>Created new feature 'Title' by extracting titles from 'Name' column. Also created 'Family' feature by combining 'Parch' and 'SibSp' columns.",
          "One-hot encoding <br>Performed one-hot encoding on categorical variables like 'Pclass', 'Sex', 'Family', 'Embarked', 'Age', 'Fare', and 'Title'.",
          "Handling missing data <br>The code handles missing data by filling null values in the 'brand_name' and 'item_description' columns with 'Not known' and 'No description given' respectively.",
          "Categorical variable manipulation <br>The code splits the 'category_name' column into three sub-categories ('sub1', 'sub2', 'sub3') and assigns the values to these new columns.",
          "Text preprocessing <br>The code preprocesses the text data in the 'item_description' column by removing special characters, converting to lowercase, and removing stopwords.",
          "Feature extraction using TF-IDF <br>The code uses TF-IDF vectorization to extract features from the preprocessed text data in the 'item_description' and 'name' columns.",
          "Label binarization <br>The code binarizes categorical variables like 'item_condition_id', 'shipping', 'brand_name', 'sub1', 'sub2', and 'sub3' using LabelBinarizer from sklearn.preprocessing.",
          "Lagged Rolling Average <br>The code applies a lagged rolling average to the 'saleCount' column, grouping by the 'id' column. It calculates the rolling average over different window sizes (1, 7, 28) and lag values (1, 7).",
          "Categorical Variable Manipulation <br>The code converts categorical variables to numerical codes using the 'cat.codes' method for the 'catboost' model.",
          "Feature Selection <br>The code drops specific columns from the dataset using the 'drop' method, such as 'cols_drop' and 'cols_ignore'.",
          "Additional Feature Creation <br>The code creates additional features such as 'Lag1_rmean1_id_div_Lag1_rmean7_id' and 'Lag1_rmean7_id_div_Lag1_rmean28_id' based on specific conditions.",
          "Dropping Columns <br>Columns 'id' and 'Product ID' were dropped from the train and test datasets.",
          "Calculating Power <br>A new feature 'Power[Kw]' was calculated using the formula (Torque [Nm] * (Rotational speed [rpm]/9.5488)/1000).",
          "Calculating Temperature Ratio <br>A new feature 'Temp_ratio' was calculated by dividing 'Process temperature [K]' by 'Air temperature [K]' and rounding to 2 decimal places.",
          "Converting Temperature to Celsius <br>New features 'Air_temp[c]' and 'Pro_temp[c]' were created by subtracting 273.15 from 'Air temperature [K]' and 'Process temperature [K]' respectively.",
          "Binning Numerical Features <br>The features 'Torque [Nm]' and 'Tool wear [min]' were binned into 4 categories using pd.qcut and labeled as 'Lowtorque', 'Mediumtorque', 'Hightorque', 'Veryhightorque' and 'Lowwear', 'Mediumwear', 'Highear', 'Veryhighwear' respectively.",
          "One-Hot Encoding <br>Categorical features 'wear_bin', 'torque_bin', and 'Type' were one-hot encoded using pd.get_dummies.",
          "Creating 'isTrain' column to differentiate between train and test data <br>Adding a new column 'isTrain' with boolean values to differentiate between train and test data",
          "Grouping and aggregating data based on 'GroupId' and 'PassengerId' <br>Creating new features like 'group_num', 'group_youngest', 'group_oldest', 'group_age_mean', 'group_age_std', and 'num_destination' based on group and passenger information",
          "Handling missing values <br>Filling missing values in columns like 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', and 'VIP' with appropriate values",
          "Encoding categorical variables <br>Mapping categorical variables like 'HomePlanet', 'CryoSleep', 'deck', 'side', and 'Destination' to numerical values",
          "Feature extraction from 'Name' column <br>Extracting 'last_name' from 'Name' and creating new features like 'same_name' and 'IsFamily' based on group and last name information",
          "Binning 'Age' into age groups <br>Creating a new feature 'binned_age' by binning 'Age' into different age groups",
          "Feature scaling <br>Scaling features like 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck' by filling missing values and creating new features like 'luxury' and 'luxurywithoutfood'",
          "Model training and evaluation <br>Training XGBoost and CatBoost models, evaluating accuracy, and using cross-validation for model performance assessment",
          "Adding population column <br>A new column 'population' is created by dividing 'active' by 'microbusiness_density' and rounding the result.",
          "Filling population for counties with zero active businesses <br>The population for counties with zero active businesses is filled by taking the mean population of non-zero active businesses in the same county.",
          "Adding standard deviation of microbusiness density column <br>A new column 'mbd_stdev' is created by calculating the standard deviation of microbusiness density for each county and repeating the value for 39 rows.",
          "Transforming microbusiness density into a 2D list <br>The microbusiness density column is transformed into a 2D list of 12 X values and 8 Y values for each row.",
          "Calculating median percentage change for each set of 12 prior months <br>The median percentage change for each set of 12 prior months is calculated and added to the dataframe as 'med_ch_pct'.",
          "Extrapolating Y values for lags 1 to 8 <br>Y values for lags 1 to 8 are extrapolated based on the median percentage change and added to a new dataframe 'yext_df'.",
          "Calculating difference array and percentage change array <br>Difference array and percentage change array are calculated and added to new dataframes 'diff_df' and 'pct_ch_df' respectively.",
          "Applying truncation ranges for each lag <br>Truncation ranges are applied to the extrapolated values for each lag, and the SMAPE is calculated for each range.",
          "Selecting month range for microbusiness density <br>A function is defined to select the microbusiness density date range from the dataframe for each county.",
          "Extrapolating time series based on median percentage change <br>The microbusiness density for November 2022 is extrapolated based on the median percentage change of the 12 prior months.",
          "Feature Blending <br>The code blends the predictions from multiple models using weighted averages to create a final submission. This is a form of feature engineering as it combines the predictions from different models to potentially improve the overall predictive performance.",
          "Dropping columns <br>Columns 'id' and 'sex' were dropped from both the training and test datasets.",
          "Train-Test Split <br>The dataset was split into training and validation sets using train_test_split from sklearn.model_selection.",
          "Ensembling <br>The code combines predictions from multiple models by taking the mean or median of the individual predictions, which is a form of ensembling to improve predictive performance.",
          "Outlier Handling <br>The function 'better_than_median' identifies outliers based on the spread of predictions and computes the mean for inliers and median for outliers, which is a method of handling outliers in the predictions.",
          "Handling missing values <br>Filling missing values in float features with a specific value and digitizing the features into buckets",
          "Train-Test Split <br>Splitting the training data into train and dev sets using train_test_split method",
          "K-Fold Cross Validation <br>Implementing K-Fold Cross Validation for model training and evaluation",
          "Feature Importances from XGBoost and LightGBM <br>Feature importances were extracted from XGBoost and LightGBM models to identify the most important features for prediction.",
          "Time Label Encoding <br>The 'Time' feature was transformed into categorical labels such as 'Early_Morning', 'Morning', 'Afternoon', and 'Night'.",
          "Time-based Data Filtering <br>Data points with 'Time' values outside the range of 10 to 24 were filtered and used for model training.",
          "Missing value imputation <br>KNNImputer and HuberRegressor were used to fill missing values in the dataset",
          "Feature creation <br>New features 'm3_missing', 'm5_missing', and 'area' were created based on existing columns",
          "Correlation-based feature selection <br>Selected the next 10 best correlated measurement columns sorted by correlation",
          "Categorical variable manipulation <br>Used WoEEncoder to encode the categorical variable 'attribute_0'",
          "Scaling <br>Applied MinMaxScaler for neural network model and StandardScaler for other models",
          "Converting date column to datetime format <br>pd.to_datetime(train['date']) and pd.to_datetime(test['date'])",
          "Creating new columns for year, month, and day from the date column <br>train['year'] = train.date.dt.year, train['month'] = train.date.dt.month, train['day'] = train.date.dt.day and test['year'] = test.date.dt.year, test['month'] = test.date.dt.month, test['day'] = test.date.dt.day",
          "Dropping the date column <br>train.drop('date' , axis=1 , inplace=True) and test.drop('date' , axis=1 , inplace=True)",
          "Label encoding of categorical columns <br>Using LabelEncoder to transform categorical columns in both train and test datasets",
          "One-Hot Encoding <br>The categorical variables in the 'City Group' column were transformed into binary variables using one-hot encoding.",
          "Feature Selection <br>The 'P1' to 'P37' features were analyzed for correlation with the target variable 'revenue', and features with absolute correlation less than 0.1 were dropped from the dataset.",
          "Feature Creation <br>New features 'kOpen Year', 'Open Month', and 'gOpen Year's' were created by extracting and manipulating information from the 'Open Date' column.",
          "Data Cleaning <br>Missing values in the dataset were filled with zeros.",
          "Feature Transformation <br>The 'Open Month' feature was transformed into categorical bins using the 'pd.cut' function.",
          "Rolling statistics <br>The code calculates rolling statistics such as minimum, maximum, standard deviation, median, variance, kurtosis, and skewness over a specified window size for the input data.",
          "Splitting data into experiments <br>The code splits the input data into separate experiments based on the occurrence of earthquakes, creating multiple sets of data for analysis.",
          "Ridge regression <br>The code uses Ridge regression for modeling the relationship between the input features and the target variable, with an alpha value of 0.5.",
          "Feature scaling <br>The code applies feature scaling to the input data, which is a common feature engineering method to standardize the range of independent variables or features.",
          "Handling overlaps in data <br>The code addresses overlaps in the input data by splitting the data into non-overlapping segments of a fixed size, ensuring that each segment is of the same length for analysis.",
          "Handling missing values <br>Filled missing values in 'MasVnrType', 'BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'GarageFinish', 'Fence', 'MasVnrArea' with appropriate values, and filled remaining missing values with mean of the column.",
          "One-hot encoding <br>Performed one-hot encoding for categorical columns using pd.get_dummies method.",
          "Feature creation <br>Created new features 'Total_SF', 'Total_No_Bathrooms', and 'Total_Porch_SF' based on existing features.",
          "Data scaling <br>Used RobustScaler for scaling the data.",
          "Model evaluation <br>Evaluated the model using various metrics such as MAE, MSE, RMSE, RMSLE, and visualized the predicted vs actual values.",
          "Outlier detection and removal <br>Detected outliers using z-score and removed them from the training set.",
          "Standard Scaling <br>The code applies standard scaling to the input features using the StandardScaler from sklearn.preprocessing. This method standardizes the features by removing the mean and scaling to unit variance.",
          "Converting date columns to datetime format <br>Using pd.to_datetime() method to convert date columns to datetime format",
          "Feature renaming <br>Using regular expressions to remove special characters and spaces from feature names",
          "Memory optimization <br>Optimizing memory usage of the dataframe by reducing the data types of columns and filling missing values with appropriate values",
          "Undersampling <br>The code balances the target classes by undersampling the majority class (target != 1) to match the size of the minority class (target = 1).",
          "Feature Shuffling <br>The code shuffles the values of features for both the minority and majority classes to create augmented data.",
          "Feature Prefixing <br>The code prefixes the feature names with 'var_' after creating the augmented data.",
          "QuantileTransformer <br>Applied to both GENES and CELLS columns to transform the data to a Gaussian distribution.",
          "Principal Component Analysis (PCA) <br>Performed on both GENES and CELLS columns to reduce dimensionality and create new features.",
          "VarianceThreshold <br>Used to remove features with low variance, likely for feature selection.",
          "OneHotEncoding <br>Performed on 'cp_time' and 'cp_dose' columns to convert categorical variables into binary vectors.",
          "Combining Subplace and Country to create Location feature <br>The code combines the 'Subplace' and 'Country' columns to create a new 'Location' feature using lambda function and apply method.",
          "Replacing implausible data <br>The code identifies and replaces implausible data in the dataset.",
          "Creating new features based on specific conditions <br>The code creates new features based on specific conditions such as date and location, and assigns values to them.",
          "Feature extraction from time series data <br>The code extracts features from time series data by calculating differences and ratios, and assigns weights to the features.",
          "Histogram plotting <br>Histograms of each feature's distribution in the train and test data are plotted to visualize the data distribution and identify any discrepancies between the two datasets.",
          "Stratified K-Fold Cross-Validation <br>Stratified K-Fold cross-validation is used to ensure that each fold has the same proportion of target classes as the entire dataset, which is important for maintaining the representativeness of the data in each fold.",
          "Hyperparameter tuning with Optuna <br>Optuna library is used for hyperparameter optimization to find the best set of hyperparameters for the CatBoostClassifier model.",
          "Feature Importance Visualization <br>The feature importances of the trained CatBoostClassifier models are visualized to understand the relative importance of each feature in making predictions.",
          "Converting column names to lower case <br>All column names in both train and test datasets were converted to lower case using the str.lower() method.",
          "Dropping unnecessary 'Id' column <br>The 'Id' column was identified as unnecessary for the prediction process and was dropped from both the train and test datasets.",
          "Handling missing values <br>The code checked for and reported the presence of any NULL values in both the train and test datasets. However, it did not explicitly handle the missing values.",
          "Aggregating features <br>The code added new aggregated features such as mean, median, max, variance, and standard deviation to the datasets.",
          "Clustering features <br>The code applied KMeans clustering to create new features based on the original features in the datasets.",
          "Principal Component Analysis (PCA) <br>The code performed PCA to reduce the dimensionality of the dataset and added the principal components as new features.",
          "Creating new features <br>The code creates new features such as 'area' by combining 'Country_Region' and 'Province_State', and 'cases_start_date', 'deaths_start_date', 'init_ConfirmedCases', 'init_Fatalities' by aggregating data from the 'train.csv' dataset.",
          "Data cleaning <br>The code cleans the data by removing unnecessary columns such as 'Id', 'Province_State', 'Country_Region' and filtering out rows with zero values for 'ConfirmedCases' and 'Fatalities'.",
          "Feature transformation <br>The code transforms the features by converting dates to datetime format using 'pd.to_datetime' and calculating the number of days from a reference date using 'pd.to_datetime(today)-area_info['cases_start_date']'.",
          "Automatic extraction of features <br>The code automatically extracts parameters for the logistic curve fitting using the 'curve_fit' function from 'scipy.optimize' library.",
          "Scaling <br>StandardScaler and MinMaxScaler were used to scale the original features and create new custom features based on the scaled data.",
          "Dimensionality Reduction <br>Principal Component Analysis (PCA) was used to reduce the dimensionality of the original features and create new custom features based on the reduced data.",
          "Outlier Detection <br>Local Outlier Factor (LOF) was used to detect anomalies in the train dataset, and the detected anomalies were removed from the dataset.",
          "Feature Transformation <br>Original features were transformed into custom categories by cutting them into custom intervals and mapping the intervals to numerical values.",
          "Feature Creation <br>New custom features were created based on the original features, such as maximum, mean, median, unique values, count of zero and non-zero values, and their fractions.",
          "Feature Selection <br>Duplicated rows with identical feature values but different target were identified and deleted from the dataset.",
          "Standard Deviation Calculation <br>Standard deviation of all 512*256 blocks is calculated to identify useful blocks.",
          "Boolean Identification of Useful Blocks <br>The standard deviation values are converted to booleans to identify useful blocks.",
          "One-Hot-Encoding <br>The 'wheezy-copper-turtle-magic' feature is one-hot-encoded.",
          "Standardization <br>Data is standardized using preprocessing.StandardScaler.",
          "Feature Selection <br>Variance threshold feature selection is applied to select a subset of features.",
          "Stratified K Fold <br>Stratified K Fold is used for cross-validation.",
          "Neural Network Model Building <br>A neural network model is built using Keras.",
          "Support Vector Machine Model Building <br>Support Vector Machine model is built using sklearn.svm.SVC.",
          "Ensemble Model Building <br>Ensemble of Neural Network and Support Vector Machine models is created.",
          "Label Encoding <br>Applied label encoding to the 'family' and 'day' columns in both the training and test datasets.",
          "One-Hot Encoding <br>Performed one-hot encoding on the categorical features in the training and test datasets.",
          "Datetime Features <br>Created new columns for 'month', 'period', 'day', and 'day_name' based on the date column in both the training and test datasets.",
          "Handling missing values in 'Cabin' column <br>Using lambda function to extract the first two characters of the 'Cabin' value, and replacing missing values with NaN",
          "Extracting ticket prefix <br>Using lambda function to split the 'Ticket' value and extract the first part as the ticket prefix",
          "Creating 'Family' feature <br>Summing 'SibSp' and 'Parch' columns to create a new 'Family' feature",
          "Converting object columns to category type <br>Using a function to iterate through columns and convert object type columns to category type",
          "Handling missing values <br>The code creates a new feature 'missing' which counts the number of missing values in each row for both the training and test datasets. It then plots the frequency of missing values for both claim=1 and claim=0.",
          "Skewness correction <br>The code identifies features with skewness greater than 1 and applies log or square root transformation to reduce skewness. It then fills remaining NA values with mean and scales the features using RobustScaler.",
          "Model training and feature importance <br>The code trains LightGBM, XGBoost, and CatBoost models, and then visualizes the feature importances for each model.",
          "Handling missing values <br>Missing values are handled using IterativeImputer from sklearn. The missing values are imputed using the iterative imputation method, which uses the other features to predict the missing values.",
          "Encoding categorical variables <br>The 'product_code' column is encoded using the ord() function to convert each character to its corresponding ASCII value. The 'attribute_0' and 'attribute_1' columns are manipulated to extract the material number from the strings.",
          "Feature creation <br>New features 'm_3_missing' and 'm_5_missing' are created to indicate whether the 'measurement_3' and 'measurement_5' columns have missing values, respectively.",
          "One-Hot Encoding <br>Encoded categorical variable 'Type' into binary columns",
          "Log Transformation <br>Applied log transformation to numerical features",
          "Power Transformation <br>Transformed numerical features using power transformation",
          "Quantile Transformation <br>Transformed numerical features using quantile transformation",
          "Square Root Transformation <br>Applied square root transformation to numerical features",
          "Scaling <br>Performed scaling using MaxAbsScaler, MinMaxScaler, RobustScaler, and Normalizer",
          "Polynomial Features <br>Generated polynomial features up to the 2nd degree",
          "Recursive Feature Elimination <br>Selected the best features using RFECV",
          "Removing features <br>The code uses the step_rm function from the recipes package to remove the 'id' column from the training data.",
          "Correlation-based feature selection <br>The code uses the step_corr function from the recipes package to remove highly correlated numeric features with a threshold of 0.98.",
          "Box-Cox transformation <br>The code uses the step_BoxCox function from the recipes package to perform Box-Cox transformation on all numeric features.",
          "Normalization <br>The code uses the step_normalize function from the recipes package to normalize all numeric features.",
          "Categorical variable manipulation <br>The code converts the categorical variables to integer factors using lapply and as.integer(factor(x)) functions.",
          "K-means clustering <br>The code uses the kmeans function to perform k-means clustering on the features and adds a new column 'segmento' to the dataset based on the cluster assignment.",
          "Standard Scaling <br>The StandardScaler from sklearn.preprocessing was used to scale the input features to have a mean of 0 and a standard deviation of 1.",
          "Train-Test Split <br>The train_test_split function from sklearn.model_selection was used to split the dataset into training and validation sets for model evaluation.",
          "Loss Function Selection <br>The Mean Squared Error (MSE) loss function was selected for both the TensorFlow and PyTorch models to measure the difference between predicted and actual values.",
          "Optimizer Selection <br>The Adam optimizer with a specified learning rate was used for the TensorFlow model, while the Stochastic Gradient Descent (SGD) optimizer with a different learning rate was used for the PyTorch model.",
          "Feature Transformation <br>The input features were transformed using the StandardScaler to ensure that the features have a similar scale, which is important for many machine learning algorithms.",
          "Converting dates to Date format <br>The code converts the 'visit_date' column in both the training and test sets to the Date format using the 'as.Date' function.",
          "Creating wide format data <br>The code uses the 'dcast' function to create wide format data from the training and test sets, with 'air_store_id' as the key and 'visit_date' as the columns, and 'visitors' as the values.",
          "Using Prophet for time series forecasting <br>The code uses the 'prophet' package to perform time series forecasting on the training data, creating forecasts for future dates based on historical data.",
          "Calculating day of week <br>The code uses the 'wday' function to calculate the day of the week from the 'visit_date' in the training set and assigns it to a new column 'dow'.",
          "Creating a dictionary for median visitors <br>The code creates a dictionary for median visitors by 'air_store_id' and day of the week using the 'median' function and 'by' argument in data.table.",
          "One-Hot Encoding <br>The code identifies categorical variables with a small number of unique values and applies one-hot encoding to convert them into numerical format for modeling.",
          "Missing Value Imputation <br>The code fills missing values in categorical variables with the mode and in numerical variables with the median to ensure completeness of the dataset.",
          "Cabin Column Treatment <br>The code splits the 'Cabin' column into 'CabinDeck', 'CabinNum', and 'CabinSide', and then fills missing values in 'CabinDeck' and 'CabinSide' with the mode.",
          "Box-Cox Transformation <br>The code applies the Box-Cox transformation to correct and standardize numerical variables, addressing the right-skewed distribution.",
          "Feature Selection <br>The code uses SelectKBest to select the top k features based on ANOVA F-value for classification, which helps in feature selection.",
          "Train-Test Split <br>The code splits the dataset into training and testing sets for model evaluation and validation.",
          "Model Evaluation <br>The code defines a function for model evaluation, including confusion matrix, classification report, and learning curve analysis.",
          "Pipeline Construction <br>The code constructs pipelines for various machine learning models, including preprocessing and model building steps.",
          "Hyperparameter Tuning <br>The code performs hyperparameter tuning using GridSearchCV to find the best parameters for the Gradient Boosting model.",
          "Label Encoding <br>The code uses label encoding for categorical features, converting them into numerical values for models like XGBoost and RandomForest.",
          "Ordinal Encoding <br>The code encodes ordinal categorical features with ordered levels using a specified mapping of levels to numerical values.",
          "Imputation <br>The code fills missing values in numeric features with 0 and in categorical features with 'None'.",
          "Mathematical Transforms <br>The code creates new features by performing mathematical operations on existing features, such as creating a ratio and calculating spaciousness.",
          "Interactions <br>The code generates interaction features by creating dummy variables for a categorical feature and multiplying it with a numeric feature.",
          "Counts <br>The code creates a new feature by counting the number of non-zero values across multiple porch-related features.",
          "Group Transforms <br>The code creates a new feature by applying a group-wise transformation, calculating the median of 'GrLivArea' within each neighborhood.",
          "PCA (Principal Component Analysis) <br>The code applies PCA to create new features based on linear combinations of existing features, capturing the most important information in the data.",
          "Target Encoding <br>The code uses target encoding with cross-fold validation to encode the 'MSSubClass' feature based on the target variable 'SalePrice'.",
          "Text cleaning <br>The code applies text cleaning techniques such as removing emojis, converting text to lowercase, removing HTML tags, removing HTTP links, lemmatizing, and removing punctuation and extra spaces.",
          "Text slicing <br>The code extracts a portion of the text data by selecting the title and the last 400-500 words from the boilerplate, and then concatenates them to create a new feature.",
          "Dropping columns <br>Columns 'Id', 'Soil_Type15', and 'Soil_Type7' were dropped from both the training and test datasets.",
          "Creating new features <br>Several new features were created by combining existing features, such as distances and elevations, and calculating means and differences between them.",
          "Standardization <br>The features were standardized using the StandardScaler from sklearn.preprocessing.",
          "Handling missing values <br>The code does not explicitly handle missing values, but it's a common feature engineering method to impute or remove missing values from the dataset.",
          "Feature selection <br>The code selects specific columns for training and testing datasets, indicating a form of feature selection.",
          "Class imbalance handling <br>The code calculates class weights to handle class imbalance, which is a feature engineering method to address skewed class distributions.",
          "Categorical variable manipulation <br>The code does not explicitly manipulate categorical variables, but it's a common feature engineering method to encode categorical variables for model training.",
          "Dropping columns <br>Columns 'id' and 'Product ID' were dropped from both the train and test datasets.",
          "Memory optimization <br>A function 'reduce_mem_usage' was applied to optimize the memory usage of the dataframes.",
          "Column renaming <br>Columns were renamed using regular expressions to remove special characters and spaces.",
          "One-hot encoding <br>Categorical variables were one-hot encoded using the 'pd.get_dummies' function.",
          "SMOTE oversampling <br>The SMOTE technique was used to oversample the minority class in the training data.",
          "Handling missing values <br>Filled missing values with median for 'v2a1', 'meaneduc', 'SQBmeaned', and with -1 for 'v18q1' and 'rez_esc'.",
          "Handling yes/no values <br>Replaced 'yes' with 1, 'no' with 0, and other values with -1 for columns 'edjefe', 'edjefa', and 'dependency'.",
          "Feature creation <br>Created a new feature 'num_over_18' representing the number of people over 18 in each household.",
          "Feature creation <br>Extracted several new features related to household characteristics such as bedrooms to rooms ratio, rent to rooms ratio, etc.",
          "Time series aggregation <br>The code aggregates the sales data by date to calculate the mean sales for each day.",
          "Moving average <br>The code calculates the rolling mean of the aggregated sales data with a window of 7 days.",
          "Fourier terms <br>The code uses Fourier terms to capture seasonal patterns in the time series data.",
          "Holiday feature engineering <br>The code engineers features related to holidays, such as identifying weekdays, bridge/transfer days, and work days.",
          "School season feature <br>The code creates a binary feature to indicate whether a date falls within the school season (April, May, August, September).",
          "Text cleaning <br>The code uses the cleantext library to clean the text data by removing HTML tags, extra spaces, stemming, stopwords, lowercase conversion, numbers, and punctuation. This is done to standardize the text data and improve the quality of features extracted from the text.",
          "Text preprocessing <br>The code preprocesses the text data by removing duplicate words, applying text cleaning, and lemmatizing the words. This helps in standardizing the text data and reducing the dimensionality of the feature space.",
          "TF-IDF Vectorization <br>The code uses the TfidfVectorizer from scikit-learn to convert the preprocessed text data into numerical vectors. This is a feature extraction method that represents the text data as numerical features based on the term frequency-inverse document frequency (TF-IDF) values, which helps in capturing the importance of words in the documents.",
          "Creating new features from date column (month, day, day_name, year) <br>Using pd.to_datetime() to extract month, day, day_name, and year from the date column",
          "Pivoting and visualizing sales data to understand patterns (e.g., sales across months, impact of different item families on sales, impact of promotion on sales, impact of day of the week on sales, impact of year on sales) <br>Using pd.pivot_table() and visualization libraries like seaborn and plotly.express to analyze and visualize sales data",
          "Mapping and grouping item families based on sales proportions <br>Creating a new feature 'new_family' by mapping and grouping item families based on their sales proportions",
          "Handling outliers in sales data <br>Removing outlier sales values for each store using quantile-based filtering",
          "Merging store and holiday data with training data <br>Merging store and holiday data with training data based on date, city, and state",
          "One-hot encoding categorical variables <br>Using pd.get_dummies() to one-hot encode categorical variables like 'day_name' and 'new_family'",
          "Imputation of missing categorical values based on passenger group size <br>The fill_categorical function is used to impute missing values in the 'Cabin' and 'HomePlanet' features based on the size of the passenger group. It fills solo travelers with an arbitrary value and uses forward-fill and back-fill to fill values in the same group. It also performs outlier cleanup by filling edge cases with a specified fill value.",
          "Creation of new features from existing data <br>New features such as 'Passenger_Group', 'Group_Size', 'Surname', 'Cabin_Deck', 'Cabin_Num', 'Cabin_Side', 'Destination', 'Total_Spend', and 'VIP' are created from existing data in the dataset.",
          "Handling of missing numerical values <br>Missing numerical values in features such as 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck' are filled with the median value of each feature.",
          "Adjustment of inconsistent data based on other features <br>Inconsistencies in the 'HomePlanet' and 'Cabin_Deck' features are adjusted based on potential inconsistencies due to imputation.",
          "One-hot encoding of categorical features <br>Categorical features are one-hot encoded using the get_dummies function to convert them into a format suitable for machine learning models.",
          "Mean, Standard Deviation, Maximum, Minimum, Kurtosis, Skewness <br>Basic statistical features were calculated for the acoustic data, including mean, standard deviation, maximum, minimum, kurtosis, and skewness.",
          "Quantiles <br>Quantiles such as 0.001, 0.01, 0.05, 0.95, 0.99, and 0.999 were calculated for the acoustic data.",
          "Trend Feature <br>A linear regression was used to calculate the trend feature for the acoustic data.",
          "Absolute Values <br>Absolute maximum, mean, standard deviation, sum, trend, median, and quantiles were calculated for the absolute values of the acoustic data.",
          "ANOVA Test <br>An F-test and p-value were calculated using a one-way ANOVA test on different segments of the acoustic data.",
          "Rolling Features <br>Rolling statistics such as mean, standard deviation, maximum, minimum, and quantiles were calculated for different window sizes (10, 100, 1000) of the acoustic data.",
          "Dropping a feature <br>The code drops the 'cont6' feature from the dataset as it has high correlation with several other variables.",
          "One-Hot Encoding <br>Applied one-hot encoding to the 'Sex' column to convert categorical data into numerical format.",
          "Feature Scaling <br>Used StandardScaler to scale the input features for the deep learning model.",
          "Feature Generation <br>Generated new features by comparing and replacing values based on certain conditions for 'Shell Weight', 'Viscera Weight', and 'Shucked Weight'.",
          "Dropping columns <br>The 'Id' column is dropped from the train and test dataframes using the drop() method with axis=1.",
          "Handling missing values <br>The 'EJ' column is filled with 'B' for missing values, and then mapped to numerical values using a dictionary mapping. Other columns with missing values are filled with the mean of the column using fillna() method.",
          "Under-sampling <br>The RandomUnderSampler from imblearn library is used to under-sample the majority class in the target variable 'y' to balance the class distribution.",
          "Sum, mean, standard deviation, kurtosis, and skewness calculation <br>Applied to both gene expression and cell viability features",
          "Label encoding <br>Applied to categorical variables",
          "Robust scaling <br>Applied to gene expression and cell viability features",
          "Principal Component Analysis (PCA) <br>Applied to reduce dimensionality of gene expression and cell viability features",
          "Mapping of target variable <br>The unique values of the target variable are mapped to a range of integers, creating a numerical representation of the target variable.",
          "Train-test split <br>The data is split into training and testing sets using the train_test_split function from scikit-learn.",
          "Optuna hyperparameter optimization <br>Optuna library is used to perform hyperparameter optimization for the XGBoost model. The objective function is defined to minimize the mean squared error, and Optuna is used to search for the best hyperparameters.",
          "K-Fold cross-validation <br>The KFold function from scikit-learn is used to perform K-Fold cross-validation for model evaluation and prediction.",
          "Drop columns <br>Columns 'EC3', 'EC4', 'EC5', 'EC6' were dropped from the 'train' dataframe.",
          "Data type optimization <br>The function 'reduce_mem_usage' was used to optimize the data types of columns in the dataframes to reduce memory usage.",
          "Feature selection <br>Selected specific columns for 'train_EC1', 'train_EC2', and 'test' dataframes for further analysis.",
          "Duplicate removal <br>Removed duplicate rows from 'train_EC1' and 'train_EC2' dataframes.",
          "Date format conversion <br>The 'Date' column is converted from string format to datetime format using pd.to_datetime() method.",
          "Dropping unnecessary columns <br>The columns 'County', 'Province_State', 'Country_Region', and 'Target' are dropped from the 'train' and 'test' dataframes using the drop() method.",
          "Data splitting <br>The 'Date' column is split into year, month, and day components using the dt.strftime() method, and then converted to integer format.",
          "Ordinal encoding <br>Ordinal encoding is applied to categorical variables using the OrdinalEncoder from sklearn.preprocessing.",
          "Checking for null values <br>The code checks for null values in the dataframe using the 'isnull().any()' method.",
          "Checking number of categorical and numerical values <br>The code uses 'select_dtypes' to separate categorical and numerical columns and then prints the counts of each type of column.",
          "Imputing missing values using mean <br>The code uses the 'fillna' method to fill missing values with the mean of the respective columns.",
          "Column-wise mean replacement <br>The code iterates through each column and replaces missing values with the mean of that column using a custom function.",
          "Row-wise mean calculation <br>The code calculates the mean of each row using the 'median(axis=1)' method.",
          "Log transformation <br>Applied np.log1p to normalize the sales prices for prediction",
          "Skew correction <br>Identified and corrected skewness in certain columns by applying np.log1p transformation",
          "Feature creation <br>Created new features such as 'totalSF', 'YrBltAndRemod', 'Porch_SF', 'Total_Bathrooms', 'hpool', 'h2ndfloor', 'hgarage', 'hbsmt', and 'hfireplace'",
          "Categorical variable manipulation <br>Encoded categorical variables using one-hot encoding with pd.get_dummies()",
          "Missing value imputation <br>Filled missing values with most common value, zero value, or 'None' based on the feature",
          "Data loading <br>Loading the train and test data using pandas read_csv method",
          "Data exploration <br>Checking the shape, null values, info, and descriptive statistics of the train and test data",
          "Data visualization <br>Visualizing the distribution of mean and standard deviation values per row and column in the train and test datasets",
          "Feature grouping <br>Calculating mean values per row and column in the train set grouped by the target variable",
          "Correlation analysis <br>Analyzing the correlation between features in the train and test datasets",
          "Model training <br>Training a Light GBM model using Stratified K-Fold cross-validation",
          "Model evaluation <br>Evaluating the model using ROC AUC score and generating predictions",
          "Machine learning models <br>Training and evaluating Random Forest, Decision Tree, and Logistic Regression models",
          "Submission file creation <br>Creating submission files for the predictions of each model",
          "One-Hot Encoding <br>Performed using pd.get_dummies() to convert categorical variable 'Type' into binary columns",
          "String Manipulation <br>Removed the first character from the 'Product ID' column using str[1:]",
          "Categorical variable manipulation <br>The target variable was converted to a factor using the 'asfactor' method.",
          "Feature transformation <br>The 'np.exp' method was used to transform the predicted probabilities into their exponential form.",
          "Label Encoding <br>The code applies label encoding to the 'cp_dose' and 'cp_time' columns in the train and test dataframes.",
          "Categorical Feature Handling <br>The code specifies the categorical features 'cp_dose' and 'cp_time' for use in the LightGBM model.",
          "Train-Test Split <br>The code uses train_test_split from sklearn.model_selection to split the training data into a training set and a validation set with a 70-30 ratio.",
          "Feature Selection <br>The code selects the features for training and validation by excluding the 'target' and 'ID_code' columns from the dataset.",
          "Calculating mean of multiple features <br>pdf[feats].mean(axis=1)",
          "Calculating median of multiple features <br>pdf[feats].median(axis=1)",
          "Calculating maximum value of multiple features <br>pdf[feats].max(axis=1)",
          "Feature transformation using custom function <br>pdf['class_99'] = GenUnknown(y)",
          "Handling missing values <br>The code fills missing values in the 'test' dataframe with 0 for latitude and longitude columns.",
          "Distance calculation <br>A function 'distance' is defined to calculate the distance between two sets of latitude and longitude coordinates using the Haversine formula.",
          "Spatial indexing <br>The code uses a KDTree from the scipy library to create a spatial index for the latitude and longitude coordinates in the 'test' dataframe.",
          "String similarity comparison <br>The code uses the SequenceMatcher from the difflib library to compare the similarity between strings of categories for matching locations.",
          "Feature selection <br>The code selects specific columns from the input data for modeling, using the 'columns' variable to extract the relevant features.",
          "Feature transformation <br>The code transforms the target variables by reshaping them into a suitable format for modeling, using the 'reshape' method.",
          "Capping or clipping <br>The code applies capping or clipping to the predicted and actual target values, ensuring that they fall within specific ranges.",
          "Sequential feature creation <br>The 'sequential' function is used to create a new feature 'quake_id' based on the 'time_to_failure' column. It assigns a unique 'quake_id' to each segment of data based on the change in 'time_to_failure'.",
          "Removing overlapping segments <br>After creating the 'quake_id' feature, the code removes overlapping segments by keeping only the non-overlapping segments based on the 'remove_idx' column.",
          "Feature importance calculation <br>The code calculates feature importance using LightGBM's feature importances and stores the results in the 'feature_importance' array.",
          "Threshold-based classification <br>The 'ttf_classifier' function creates a binary classification model based on a threshold and evaluates the AUC for each threshold range. It then generates predictions for the train and test data based on the models created.",
          "Feature importance visualization <br>The code visualizes the mean feature importance using a bar plot, showing the importance of each feature in the model.",
          "Threshold-based inequality classification <br>The 'ttf_ineq_classifier' function creates a binary classification model based on a threshold for inequality (greater than) and evaluates the AUC for each threshold range.",
          "Handling missing values <br>The code fills missing values in the 'Embarked' and 'Fare' columns with the mode and median values, respectively. It also fills missing ages with the median age for each title.",
          "Creating new features <br>The code creates new features such as 'AgeGroup', 'CabinBool', 'Title', 'AgeBin', 'FareBin', 'Surname', 'TicketPrefix', 'Surname_Ticket', 'IsFamily', 'Child', 'FamilyId', and 'FamilySurvival' based on existing data.",
          "Encoding categorical features <br>The code uses LabelEncoder to transform categorical features like 'Sex', 'Embarked', and 'Title' into numerical values.",
          "Binning continuous features <br>The code bins continuous features 'Age' and 'Fare' into categories using qcut, and then encodes the bins using LabelEncoder.",
          "Feature selection <br>The code selects a subset of features for modeling, including 'Pclass', 'Sex', 'Parch', 'Embarked', 'CabinBool', 'Title', 'AgeBin', 'FareBin', and 'FamilySurvival'.",
          "Handling Missing Values <br>The 'MiscFeature', 'GarageType', 'PoolArea', porch areas, 'Fireplaces', 'Fence', 'MasVnrType', '2ndFlrSF', 'BsmtCond', 'YearRemodAdd', 'LotFrontage', 'BsmtFinSF1', 'BsmtFinSF2' columns were used to create new binary features indicating the presence of certain features or conditions.",
          "Creating New Features <br>New features were created such as 'House_Age', 'TotalSF', 'TotalBath', 'TotalPorch', 'LivLotRatio', 'Spaciousness', 'TotalLot', 'TotalBsmtFin', 'PCA_Feature1', 'PCA_Feature2' by combining or transforming existing features.",
          "Dimensionality Reduction <br>Columns 'PoolQC', 'MoSold', 'YearBuilt', 'YrSold' were dropped for dimensionality reduction.",
          "One Hot Encoding <br>Categorical columns were one-hot encoded using the category_encoders library.",
          "Skewed Variable Transformation <br>Skewed numerical variables were transformed using log transformation.",
          "Normalization <br>preprocessing.normalize(train)",
          "Standardization <br>StandardScaler()",
          "Polynomial Feature Generation <br>PolynomialFeatures(degree = 3)",
          "Label Encoding <br>The 'target' column was encoded from categorical values to integer values using LabelEncoder from sklearn.preprocessing.",
          "Hyperparameter Tuning <br>RandomizedSearchCV was used to search for the best hyperparameters for the XGBClassifier model.",
          "Concatenation and removal of duplicates <br>The train and external data are concatenated and duplicates are removed to create a combined dataset.",
          "Feature scaling <br>MinMaxScaler is used to scale the features in the dataset.",
          "Coordinate transformation <br>New features 'rot_15_x', 'rot_15_y', 'rot_30_x', 'rot_30_y', 'rot_45_x', 'rot_45_y' are created by transforming the latitude and longitude coordinates.",
          "Standard Scaling <br>The data is scaled using StandardScaler to standardize the features by removing the mean and scaling to unit variance.",
          "Logistic Regression <br>Logistic regression model is used for classification tasks, and it is trained and validated on the data.",
          "Variance Threshold <br>Feature selection method is applied to remove features with low variance, which are likely to be less informative for the model.",
          "Stratified K-Fold Cross Validation <br>The data is split into k-folds while maintaining the same target distribution in each fold, ensuring robust validation.",
          "Support Vector Machine (SVM) <br>SVM model is used for classification tasks, and it is trained and validated on the data.",
          "Handling missing values <br>The code identifies missing values in the data using the isnull() method and then applies SimpleImputer to impute the missing values with the mean of the respective columns. It also creates new features 'mvl_row', 'min_row', and 'std_row' based on the presence of missing values, minimum values, and standard deviation of each row.",
          "Scaling features <br>The code uses MinMaxScaler and StandardScaler to scale the features in the dataset, ensuring that all features have the same scale for modeling purposes.",
          "Ensembling <br>The code implements an ensembling method that combines predictions from multiple models using a weighted average. It also visualizes the ensembling process and the distribution of predictions from different models.",
          "Data cleaning <br>The code reads in a CSV file and removes any missing or inconsistent data that could affect the analysis.",
          "Feature selection <br>The code selects specific columns from the dataset to be used as features for the analysis.",
          "Feature transformation <br>The code transforms the features using the Rank2D visualizer to analyze the covariance between features and the target variable.",
          "Categorical variable manipulation <br>The code uses seaborn to plot the distribution of each feature, which can be useful for understanding the relationship between categorical variables and the target variable.",
          "Automatic extraction of features <br>The code uses the XGBoost model to automatically extract feature importances and visualize them using the plot_importance function.",
          "Creation of new features <br>New features like 'total_children', 'num_children_at_home', 'avg_cars_at home(approx).1', 'gross_weight', 'recyclable_package', 'low_fat', 'units_per_case', 'store_sqft', 'coffee_bar', 'video_store', 'salad_bar', 'prepared_food', 'florist' were created or used for feature engineering.",
          "Data cleaning <br>The code includes data cleaning methods such as handling missing values, removing unnecessary columns, and ensuring data consistency.",
          "Categorical variable manipulation <br>The code manipulates categorical variables such as 'recyclable_package', 'low_fat', 'coffee_bar', 'video_store', 'salad_bar', 'prepared_food', 'florist' for feature engineering purposes.",
          "Feature transformation <br>The code applies feature transformation techniques such as log transformation on the target variable 'cost' for modeling purposes.",
          "Categorical variable mapping <br>The categorical variable 'EJ' was mapped to numerical values 'A':0 and 'B':1.",
          "Missing value imputation <br>Missing values in features were filled using the median value of the respective feature.",
          "Regression model for prediction <br>A regression model was used to predict missing values for the feature 'EL'.",
          "Feature selection using XGBoost <br>XGBoost was used to select the 10 most important features for prediction.",
          "Polynomial feature generation <br>Polynomial features were generated from the input data.",
          "Feature selection using featurewiz <br>Feature selection was performed using the featurewiz library.",
          "Custom log loss function <br>A custom log loss function was defined for model evaluation.",
          "Hyperparameter optimization using GridSearchCV <br>GridSearchCV was used to find the best hyperparameters for XGBoost and LGBM models.",
          "Feature importance visualization <br>Feature importances were visualized using a bar plot.",
          "Feature transformation using power transform <br>Power transform was applied to transform the features.",
          "Elevation and distance interaction <br>The 'EHiElv' and 'EViElv' features are created by multiplying 'Elevation' with 'Horizontal_Distance_To_Roadways' and 'Vertical_Distance_To_Hydrology' respectively.",
          "Aspect transformation <br>The 'Aspect' feature is transformed by applying a function to adjust its values and ensure they are within a specific range.",
          "Hillshade correction <br>The 'Hillshade' features are corrected to ensure they are within a specific range and non-negative.",
          "Additional distance features <br>Various distance-related features are created such as 'EVDtH', 'EHDtH', 'Euclidean_Distance_to_Hydrolody', 'Manhattan_Distance_to_Hydrolody', 'Hydro_Fire_1', 'Hydro_Fire_2', 'Hydro_Road_1', 'Hydro_Road_2', 'Fire_Road_1', and 'Fire_Road_2'.",
          "Highwater feature <br>The 'Highwater' feature is created based on the condition of 'Vertical_Distance_To_Hydrology' being less than 0.",
          "Summed features <br>The count of 'Soil_Type' and 'Wilderness_Area' features are created and added as new features.",
          "Standardization <br>The features are standardized using the StandardScaler from scikit-learn.",
          "Handling missing values in 'Embarked' column <br>The code drops rows with missing values in the 'Embarked' column using the dropna() method.",
          "Extracting first letter from 'Cabin' column <br>The code creates a new column 'Cabin_first_letter' by extracting the first letter from the 'Cabin' column.",
          "Extracting digits from 'Ticket' column <br>The code extracts digits from the 'Ticket' column and creates a new column 'Ticket_digits'.",
          "Consolidating titles in 'Name' column <br>The code consolidates titles in the 'Name' column into broader categories such as 'Miss', 'Mrs', and 'Mr'.",
          "Creating new feature 'Fam_size' based on 'SibSp' and 'Parch' <br>The code creates a new feature 'Fam_size' by adding 'SibSp' and 'Parch' and categorizes it into 'Solo', 'Small', 'Big', and 'Very big' groups.",
          "Extracting length and first characters from 'Ticket' column <br>The code extracts the length and first characters from the 'Ticket' column and creates new features 'Ticket_len' and 'Ticket_let'.",
          "Creating new feature 'Ticket_first_digit' <br>The code extracts the first digit from the 'Ticket_digits' and creates a new feature 'Ticket_first_digit'.",
          "Grouping 'Ticket_first_digit' into classes <br>The code groups 'Ticket_first_digit' into classes 'First_class', 'Second_class', and 'Third_class' and creates dummy variables for these classes.",
          "One-Hot Encoding <br>The 'matchType' column is one-hot encoded using pd.get_dummies() method.",
          "Data Cleaning <br>The 'train' dataframe is cleaned by dropping rows with missing values using the dropna() method.",
          "Replacing categorical values with numerical values <br>The 'EJ' column is replaced with numerical values 0 and 1.",
          "Handling missing values <br>Missing values are filled with the mean of the respective columns.",
          "Rebalancing classes <br>The rebalance function is used to adjust the predicted probabilities to rebalance the classes.",
          "Time series decomposition <br>The code uses seasonal_decompose from statsmodels.tsa.seasonal to decompose the time series data into trend, seasonal, and residual components.",
          "Holt-Winters Exponential Smoothing <br>The code applies ExponentialSmoothing from statsmodels.tsa.holtwinters to model and forecast the time series data using Holt-Winters exponential smoothing.",
          "Parameter Optimization <br>The code uses parameter optimization to find the best combination of trend model, seasonal model, damped trend, and use_boxcox for the Holt-Winters model.",
          "Feature extraction <br>The code drops the 'ID_code' and 'target' columns from the input data, indicating that these columns are not used as features for the model. This can be considered a form of feature engineering as it involves selecting and extracting relevant features for the model.",
          "Data cleaning <br>There is no explicit data cleaning method applied in the provided code. It's possible that data cleaning was performed prior to this code snippet, but based on the given code, there is no specific data cleaning method evident.",
          "Removing constant columns <br>The code identifies and removes columns from the dataset that have zero standard deviation, indicating that they have constant values and do not provide any useful information for prediction.",
          "Removing duplicate columns <br>The code identifies and removes duplicate columns from the dataset, which can reduce redundancy and improve model performance.",
          "Feature selection using SelectFpr <br>The code uses SelectFpr from sklearn's feature_selection module to select features based on their p-values, with a specified alpha threshold. This helps in selecting the most relevant features for prediction.",
          "Adding Manhattan Distance as features <br>A function add_travel_vector_features(df) is defined to calculate the absolute difference in longitude and latitude between pickup and dropoff locations, and these values are added as new columns in the dataframe.",
          "Removing rows with null values <br>The code checks for null values in the dataframe and removes rows with any null values using the dropna() function.",
          "Filtering data based on Manhattan Distance <br>The code filters the dataframe based on the absolute difference in longitude and latitude, keeping only the rows where both values are less than 5.0.",
          "Removing constant columns <br>Columns with zero standard deviation are identified and removed from the data.",
          "Removing duplicate columns <br>Identifying and removing duplicate columns from the dataset.",
          "One-hot encoding <br>Converting categorical variables into dummy/indicator variables using one-hot encoding.",
          "Sparse array conversion <br>Converting the dataframe into a sparse array to reduce memory usage.",
          "Feature selection based on unique values <br>Dropping columns with less than 2 unique values from both train and test datasets.",
          "Handling missing values <br>Filled missing values in 'category_name', 'brand_name', and 'item_description' columns with 'missing'",
          "Text data preprocessing <br>Split 'category_name' into subcategories and created new columns for subcategories. Also, used CountVectorizer and TfidfVectorizer to transform text data into numerical features",
          "Visual data exploration <br>Used histograms and violin plots to visualize the distribution of 'price' and 'log(Price)', and used WordCloud to visualize the 'item_description'",
          "One-hot encoding <br>Performed one-hot encoding on categorical variables like 'item_condition_id' and 'shipping'",
          "Sparse matrix creation <br>Created sparse matrices for various features using scipy.sparse library",
          "Model training <br>Trained Ridge and LightGBM models for prediction",
          "Data Normalization <br>The code includes a section for normalizing the data by subtracting the mean and dividing by the standard deviation for each feature.",
          "Random Over Sampling <br>The code uses the RandomOverSampler from the imblearn library to address class imbalance by creating synthetic samples of the minority class.",
          "Feature Creation - Count and Frequency <br>New features are created by counting the occurrences of each value in a feature and then calculating the frequency of each value in the dataset.",
          "Random Feature Manipulation <br>The code includes a function 'swapping' to randomly change two variables, and an 'appl' function to apply a value based on a condition. These functions are used to manipulate the features in a random manner.",
          "Label Encoding <br>Performed label encoding for categorical features using integer codes.",
          "Ordinal Encoding <br>Encoded ordinal categorical features with ordered levels using CategoricalDtype.",
          "Imputation <br>Filled missing values in numeric features with 0 and in categorical features with 'None'.",
          "Mathematical Transforms <br>Created new features using mathematical operations on existing features.",
          "Interactions <br>Generated new features by interacting existing features.",
          "Counts <br>Created new features by counting the occurrences of certain conditions or values.",
          "Break Down <br>Split a feature into multiple features based on certain criteria.",
          "Group Transforms <br>Generated new features by applying group-level transformations.",
          "Clustering <br>Applied clustering to create new features based on similarity of instances.",
          "PCA (Principal Component Analysis) <br>Performed PCA to create new features representing the original features in a transformed space.",
          "Target Encoding <br>Encoded categorical features based on the target variable using techniques like MEstimateEncoder and CatBoostEncoder.",
          "Feature Selection <br>The 'Id' and 'Cover_Type' columns are dropped from the training dataset to remove unnecessary features.",
          "Train-Test Split <br>The dataset is split into training and testing sets using the train_test_split method from sklearn.model_selection.",
          "Adding Squared Features <br>The NonLinearTransformer class is used to add squared features to the input data. It creates new features by squaring the existing numerical features, excluding the 'id' and 'target' columns.",
          "Dropping columns <br>Columns 'subject' and 'step' are dropped from the train and test dataframes.",
          "Grouping and Aggregation <br>The data is grouped by 'sequence' and the mean of the grouped data is calculated.",
          "Standard Scaling <br>The features are standardized using StandardScaler from sklearn.preprocessing.",
          "Normalization <br>Min-max normalization was applied to the selected columns to scale the values between 0 and 1.",
          "Label Binarization <br>The target variable 'Cover_Type' was binarized using LabelBinarizer to convert it into a binary matrix representation.",
          "Memory reduction <br>The 'reduce_memory_usage' function is used to reduce the memory usage of the data by converting data types to lower memory usage types.",
          "Cluster features creation <br>The 'create_cluster_features' function is used to create cluster features using KMeans clustering algorithm.",
          "Min-max scaling <br>The 'MinMaxScaler' is used to scale the features to a specified range.",
          "Date extraction <br>Extracting features like 'Date', 'n_days', 'Day', 'DayOfWeek', 'Month', 'Year', 'Hour', 'Minute' from the 'Dates' column.",
          "Address feature manipulation <br>Creating a new feature 'Block' based on whether the 'Address' contains the word 'block'.",
          "Coordinate manipulation <br>Creating new features 'X-Y' and 'XY' based on the 'X' and 'Y' coordinates.",
          "Adding new feature 'is_generated' <br>A new feature 'is_generated' is added to both the training and test datasets with a value of 1 for all records.",
          "Feature Scaling <br>RobustScaler is used to scale the input features.",
          "Cross-Validation <br>RepeatedStratifiedKFold is used for cross-validation to ensure that each fold has the same proportion of class labels as the entire dataset.",
          "Ensemble Modeling <br>Multiple models (CatBoost, XGBoost, LightGBM, GradientBoosting, Lasso) are trained and their predictions are combined using weighted averaging.",
          "Hyperparameter Optimization <br>Optuna library is used for hyperparameter optimization of the ensemble model.",
          "RobustScaler <br>Applied to scale the input features for CatBoost, LightGBM, and XGBoost models",
          "Train-Test Split <br>Performed to split the data into training and testing sets for model evaluation",
          "Stratified Sampling <br>Used to ensure that the target variable's distribution is preserved in the training and testing sets",
          "Categorical Variable Handling <br>Not explicitly mentioned, but likely handled within the models or preprocessing steps",
          "Label Encoding <br>The method is applied to convert categorical features into numerical values using label encoding.",
          "Ordinal Encoding <br>The method is applied to encode ordinal categorical features with ordered levels.",
          "Imputation <br>The method is applied to fill missing values in numeric and categorical features.",
          "Mathematical Transforms <br>New features are created using mathematical operations on existing features.",
          "Interactions <br>New features are created by interacting existing features.",
          "Counts <br>New features are created by counting the occurrences of certain conditions in existing features.",
          "Group Transforms <br>New features are created by applying group-level transformations to existing features.",
          "Cluster Labels <br>New features are created by clustering the data based on certain features and assigning cluster labels.",
          "Cluster Distance <br>New features are created by measuring the distance of data points to cluster centroids.",
          "PCA (Principal Component Analysis) <br>New features are created by applying PCA to the existing features.",
          "Outlier Indication <br>New features are created to indicate outliers based on certain conditions.",
          "Target Encoding <br>The method is applied to encode categorical features based on the target variable.",
          "Memory optimization <br>The code uses a function to iterate through all the columns of a dataframe and modify the data type to reduce memory usage. It checks the data type of each column and converts it to a more memory-efficient type if possible.",
          "Feature creation using count <br>The code creates new features by counting the occurrences of certain values within groups. It uses the 'count_transform' function to create new columns based on the count of occurrences of specific values within groups.",
          "Feature creation using ratios <br>The code creates new features by calculating ratios between different numerical columns. It uses the 'per_dist_stats' and 'LogWalk' to create new columns representing the ratio of certain statistics to the logarithm of walk distance.",
          "Feature creation using group size <br>The code creates a new feature by calculating the ratio of group size to the number of groups in a match. It uses the 'grpSizeMult' column to represent this ratio.",
          "Feature creation using relative statistics <br>The code creates new features by calculating the relative statistics within each match. It uses the 'match_stats' to create new columns representing the ratio of each statistic to the mean of that statistic within the match.",
          "Aggregation of statistics <br>The code aggregates statistics for each group using mean, min, max, and standard deviation. It creates new columns for each statistic with these aggregated values.",
          "Logarithmic transformation <br>The 'squareMeters' feature was highly skewed, so it was transformed to a logarithmic scale and a new feature 'squareMeters_log' was added to the dataset.",
          "One-Hot Encoding <br>Categorical variables were converted into numerical representation using one-hot encoding to prepare for modeling.",
          "Drop Columns <br>Columns 'EC3', 'EC4', 'EC5', 'EC6' were dropped from the dataframe df1.",
          "Null Values Handling <br>Checked for null values in df1 and df2.",
          "Correlation Analysis <br>Performed correlation analysis using heatmap for df_EC1 and df_EC2.",
          "Feature Selection <br>Dropped specific columns from df_EC1, df_EC2, and df2.",
          "Importing data <br>The code imports the training and test data using H2O's import_file method and pandas' read_csv method.",
          "Data splitting <br>The code splits the training data into training and validation sets using the split_frame method.",
          "Target variable identification <br>The code identifies the target variable 'stroke' for the prediction task.",
          "Filling missing values <br>Filled missing values in specific columns with appropriate values, such as filling NaNs with 0s for certain features.",
          "Encoding features <br>Encoded features with more than one data type, such as converting 'yes' and 'no' to '1' and '0', and using LabelEncoder for specific columns.",
          "Adding and modifying features <br>Added new features based on existing features, such as creating proportions and ratios, and modified existing features to obtain proportion features.",
          "Dropping Columns <br>The code drops the 'id' column from both the train and test datasets using the drop() method.",
          "One-Hot Encoding <br>The code uses the get_dummies() method to convert categorical variables into dummy/indicator variables for both the train and test datasets.",
          "Univariate Analysis <br>The code defines a function dist_box() to plot a combined graph for univariate analysis of continuous variables to check spread, central tendency, dispersion, and outliers.",
          "Log normalization and scaling <br>The LogScaler class is used to log normalize and scale the data. It is applied to the training set, validation set, and test set.",
          "DeepInsight Transform <br>The DeepInsightTransformer class is used to transform features to an image matrix using dimensionality reduction. It is applied to the training set, validation set, and test set.",
          "Multilabel Stratified K-Fold Cross-Validation <br>The code uses MultilabelStratifiedKFold for cross-validation, ensuring that each fold has a balanced distribution of the target labels.",
          "Label Encoding <br>The 'matchType' column is label encoded using sklearn's LabelEncoder.",
          "Aggregation <br>Aggregate features are created by grouping data based on 'matchId', 'groupId', and 'Id' columns and calculating sum, mean, and standard deviation for features like 'killPlace', 'walkDistance', 'numGroups', 'maxPlace', 'kills', 'longestKill', and 'weaponsAcquired'.",
          "Missing Value Removal <br>Columns with 65% or more missing values are removed from the dataset.",
          "Handling missing values <br>The code checks for missing values using 'isnull().sum()' and handles them appropriately.",
          "Outlier removal <br>Outliers are identified and removed from columns 'honeybee', 'osmia', 'fruitset', and 'fruitmass'.",
          "Feature scaling <br>StandardScaler is used to scale the features 'X_train' and 'X_val'.",
          "Hyperparameter tuning <br>Hyperparameter tuning is performed for the XGBRegressor model by adjusting parameters like 'n_estimators', 'learning_rate', 'max_depth', 'gamma', 'min_child_weight', 'tree_method', and 'subsample'.",
          "Creating new column for total amount spent in different services <br>A new column 'total_spent' is created by summing up the values from columns 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck'.",
          "Splitting 'PassengerId' into 'psng_group' and 'psng_num' <br>The 'PassengerId' column is split into two new columns 'psng_group' and 'psng_num', where 'psng_group' represents the group and 'psng_num' represents the number of passengers.",
          "Splitting 'Cabin' into 'cabin_deck', 'cabin_num', and 'cabin_side' <br>The 'Cabin' column is split into three new columns 'cabin_deck', 'cabin_num', and 'cabin_side', representing the deck, number, and side of the cabin respectively.",
          "Handling missing values <br>Identifying and filling missing values in the dataset using methods such as mode, mean, and median for both numerical and categorical features.",
          "Label Encoding <br>Converting categorical variables into numerical format using LabelEncoder from scikit-learn.",
          "One-Hot Encoding <br>Creating dummy variables for categorical features using get_dummies method from pandas.",
          "Feature Scaling <br>Normalizing numerical features using MinMaxScaler from scikit-learn.",
          "Logistic Regression <br>Applying logistic regression model for classification task.",
          "Grid Search <br>Performing hyperparameter tuning using GridSearchCV from scikit-learn to find the best parameters for the logistic regression model.",
          "Statistical features extraction <br>Statistical features such as mean, variance, standard deviation, maximum, minimum, and quantiles (0.25, 0.5, 0.75) are extracted from the 'acoustic_data' column of the training dataset.",
          "Importing data <br>The code imports the training and test data using h2o.import_file() method.",
          "Converting target variable to factor <br>The code converts the target variable in the training data to a factor using the asfactor() method.",
          "Adding new feature 'class_1' <br>test_df['class_1'] = 1",
          "Adding new feature 'class_0' <br>test_df['class_0'] = 0",
          "Selecting specific columns for submission <br>test_df[list(submission_df)].to_csv('submission.csv', index=False)",
          "Filtering by date <br>Filtering the training data to include only the records with dates containing '-08-'",
          "Filtering by year <br>Further filtering the data to separate records for the year 2016 and 2017",
          "Filtering by day of month <br>Splitting the data into two sets based on the day of the month (1-15 and 16-31) for the year 2016",
          "Combining dataframes <br>Combining the filtered dataframes into a single dataframe based on the 'id' column",
          "Conditional feature creation <br>Creating a new feature 'sales4' based on conditions involving 'sales1', 'sales2', and 'sales3'",
          "Handling missing values <br>Filling missing values in 'sales4' with a calculated value based on 'sales2'",
          "Renaming columns <br>Columns 'Country_Region' and 'Province_State' were renamed to 'Country' and 'State' respectively.",
          "Handling missing values <br>Filled missing values in 'State' column with the corresponding 'Country' value.",
          "Date manipulation <br>Converted the 'Date' column to datetime format and then extracted the month and day as a numerical value.",
          "Label encoding <br>Encoded categorical variables 'Country' and 'State' using LabelEncoder from scikit-learn.",
          "Label Encoding <br>Categorical features are label encoded using sklearn's LabelEncoder, and previously unseen values are handled by filling missing values with a placeholder and then transforming the data.",
          "Concatenation of Datasets <br>Test data is concatenated with training data to handle previously unseen values while inferencing.",
          "Stratified K-Fold Cross-Validation <br>The dataset is split into k-folds while maintaining the same target distribution in each fold, ensuring that each fold is representative of the whole dataset.",
          "Random Forest Model Training <br>Random Forest classifiers are trained for each fold using the RandomForestClassifier from sklearn.ensemble, and the predictions are averaged out to obtain the final output.",
          "Keras Model Training <br>A Keras model is created and trained using categorical features, with early stopping and learning rate reduction callbacks. The predictions are averaged out to obtain the final output.",
          "Handling missing values <br>Filling missing values with mean and 0 for specific columns",
          "Data type conversion <br>Converting certain columns to float data type",
          "Statistical analysis <br>Calculating skewness and kurtosis for each column",
          "Correlation analysis <br>Identifying important variables based on correlation with the target variable",
          "Model evaluation <br>Using accuracy, F1 score, and confusion matrix for model evaluation",
          "Feature Dropping <br>Columns 'id', 'EC3', 'EC4', 'EC5', 'EC6' are dropped from the 'train' dataset.",
          "Train-Test Split <br>The data is split into training and test sets using train_test_split function from sklearn.model_selection.",
          "Multilabel Stratified K-Fold Cross-Validation <br>MultilabelStratifiedKFold is used for cross-validation to ensure that each fold has the same proportion of target label classes as the entire dataset.",
          "Binary Relevance <br>BinaryRelevance method is used to transform the multilabel problem into multiple binary classification problems.",
          "Standard Scaling <br>The StandardScaler from sklearn.preprocessing was used to scale the input features X and X_test.",
          "Stratified K-Fold Cross-Validation <br>StratifiedKFold from sklearn.model_selection was used to split the data into 5 folds for model training and evaluation.",
          "LightGBM Model Training <br>The LGBMRegressor from lightgbm library was used to train a LightGBM model for regression.",
          "Feature Importance Selection <br>The feature_importances_ attribute of the trained LightGBM model was used to select important features for further processing.",
          "K-Nearest Neighbors (KNN) Model Training <br>The KNeighborsClassifier from sklearn.neighbors was used to train a KNN model for classification.",
          "Scaling <br>The 'minMaxScale' function is used to scale the input vector to a range between 0 and 1.",
          "Group-wise Operations <br>The 'scaleDatasetByGroup' function performs group-wise operations using dplyr's 'group_by' and 'mutate' functions to calculate various statistics for each group.",
          "Replacing NAs <br>The code replaces any NA values in the dataset with 0 using the 'is.na' function.",
          "Per Match Operations <br>The 'scaleDatasetByMatch' function performs operations on the dataset grouped by matchId, calculating statistics and scaling features for each match.",
          "Training Random Forest Algorithm <br>The 'runModel' function trains a random forest algorithm using the 'ranger' package, and handles feature selection by removing certain columns from the training data.",
          "Predicting test data results <br>The 'predictOutcome' function predicts the test data results using the trained model and performs further calculations to adjust the predicted outcomes.",
          "Creating lag features <br>The code creates lag features by shifting the sales data for different time periods (7 and 28 days) to capture historical patterns and trends.",
          "Creating rolling mean features <br>The code calculates rolling mean features for different lag periods (7 and 28 days) to capture the average sales over a specific time window.",
          "Extracting date features <br>The code extracts various date-related features such as weekday, week of year, month, quarter, year, and day to incorporate time-related patterns into the model.",
          "Summarizing features <br>The code calculates the sum, minimum, maximum, mean, standard deviation, skewness, kurtosis, median, and variance of the features and adds them as new features to both the train and test datasets.",
          "Rounding features <br>The code rounds each feature to two decimal places and one decimal place, creating new rounded features for both the train and test datasets.",
          "Standard Scaling <br>The code applies standard scaling to the features, transforming them to have a mean of 0 and a standard deviation of 1.",
          "Grouping and Aggregation <br>The code groups the 'train' data by 'category_name' and counts the occurrences. It then calculates the volume rate for each category and identifies the top 20 categories with the highest occurrence. Finally, it merges the original 'train' data with the top 20 categories based on 'category_name'.",
          "Data Visualization <br>The code uses a violin plot to visualize the distribution of prices for each category. This helps in understanding the distribution of prices within each category and identifying potential outliers.",
          "Handling Missing Values <br>The code replaces null values in the 'category_name' column with 'others' to handle missing categorical data.",
          "Feature Creation <br>The code creates a new feature 'base_price' by calculating the median price for each category in the 'train' data. This feature is then used for imputing missing prices in the 'test' data.",
          "Dropping columns <br>Dropping the 'S_2' column from the dataset",
          "Ordinal Encoding <br>Converting categorical features to numerical using OrdinalEncoder",
          "Feature Selection <br>Selecting specific columns for training and testing data",
          "Feature Transformation <br>Transforming the target variable for weighted Gini calculation",
          "Feature Creation <br>Creating a new feature 'prediction' based on model predictions",
          "Data Loading <br>The code uses pandas to load data from CSV files into dataframes.",
          "Data Sorting <br>The code sorts the dataframes by 'customer_ID' to ensure consistency and facilitate further processing.",
          "Clipping <br>The code uses np.clip to limit the 'prediction' values between 0 and 1, which can be considered as a form of feature transformation.",
          "Weighted Averaging <br>The code combines predictions from multiple dataframes using weighted averaging, which is a form of feature combination and transformation.",
          "Rank Transformation <br>The code uses scipy.stats.rankdata to transform 'prediction' values into ranks, which can be considered as a feature transformation method.",
          "Handling missing values <br>The code fills missing values for various columns with specific values such as 'NA', 'None', or the median of the column.",
          "Data transformation <br>The code applies Box-Cox transformation to skewed numerical features to make their distribution more normal.",
          "Feature scaling <br>The code uses RobustScaler to scale the numerical features in the dataset.",
          "Categorical variable manipulation <br>The code converts categorical variables into dummy variables using one-hot encoding.",
          "Ordinal encoding <br>The code replaces categorical ordinal variables with numerical values based on a predefined mapping.",
          "Ensemble modeling <br>The code combines predictions from multiple models using weighted averaging to generate the final prediction.",
          "One Hot Encoding <br>Categorical variables were encoded using One Hot Encoding to convert them into a numerical format for model training.",
          "Label Encoding <br>Label Encoding was used to convert non-numeric data into numeric format for model training.",
          "Calendar Features Creation <br>Calendar features such as year, month, week, quarter, and day of the week were created from the date column to capture temporal patterns in the data.",
          "Moving Average Calculation <br>A moving average was calculated for the oil price data to capture trends and smooth out short-term fluctuations.",
          "Feature Selection <br>Low cardinality categorical columns with less than 15 unique values were selected for One Hot Encoding, while the rest of the non-numeric columns were dropped from the dataset.",
          "Deterministic Process Modeling <br>A deterministic process model was used to capture linear trend and seasonal features for time series analysis.",
          "Checking for null values <br>The code checks for null values in the train and test datasets and prints the number of null values if any.",
          "Handling Duplicates <br>The code checks for and prints the number of duplicate rows in the train and test datasets.",
          "Histogram Plotting <br>The code creates histograms for each column in the train and test datasets, comparing the distribution of values between the two datasets.",
          "Feature Creation - 'hasBlastFurnaceSlag', 'hasFlyAshComponent', 'hasSuperplasticizerComponent' <br>The code creates new binary features based on the presence of certain components in the data.",
          "Correlation Analysis <br>The code calculates the correlation matrix for the features in the train dataset and visualizes it using a heatmap.",
          "Feature Creation - 'clippedAge' <br>The code creates a new feature by clipping the 'AgeInDays' feature to a specific range.",
          "Feature Creation - 'clippedWater' <br>The code creates a new feature by clipping the 'WaterComponent' feature to a specific range.",
          "Handling missing values <br>The code identifies missing values in the train_identity and train_transaction datasets and visualizes the counts of missing values using bar plots. It then creates new dataframes by dropping rows with missing values in specific columns.",
          "Correlation analysis <br>The code calculates the correlation matrix for the train_identity_new dataframe and visualizes it using a heatmap.",
          "Bar chart visualization <br>The code creates multiple bar charts to visualize the distribution of categorical variables in the train_identity_new dataframe.",
          "Feature selection <br>The code selects a subset of columns from the train_identity dataframe and creates a new dataframe with only those columns.",
          "Label encoding <br>The code uses LabelEncoder from sklearn.preprocessing to transform categorical variables into numerical values in the train_transaction_new and test_transaction_new dataframes.",
          "Handling missing values (again) <br>The code fills missing values in the train_transaction_new and test_transaction_new dataframes with the median of each column.",
          "Logistic Regression model <br>The code trains a Logistic Regression model on the train_transaction_new data and evaluates its accuracy on a test set.",
          "LightGBM model <br>The code trains a LightGBM model on the train_transaction_new data and uses it to make predictions on the test_transaction_new data.",
          "Handling missing values <br>Filled missing values in 'Province_State' and 'Country_Region' columns with 'no_value' in train, test, and lat_long DataFrames",
          "Creating a new feature 'place' <br>Concatenated 'Province_State' and 'Country_Region' columns to create a new feature 'place' in train, test, and lat_long DataFrames",
          "Cumulative maximum calculation <br>Calculated cumulative maximum for 'Fatalities' and 'ConfirmedCases' grouped by 'place' in the train DataFrame",
          "Merging DataFrames <br>Merged train and lat_long DataFrames on 'place' and test and lat_long DataFrames on 'place' to add 'Lat' and 'Long' information to train and test DataFrames",
          "Creating lag features <br>Created lag features 'shift_1_cc' and 'shift_1_ft' by shifting 'ConfirmedCases' and 'Fatalities' columns by 3 days grouped by 'place' in the train DataFrame",
          "Calculating differences <br>Calculated differences in 'ConfirmedCases' and 'Fatalities' using lag features and divided by 3 in the train DataFrame",
          "Splitting test data <br>Split the test data into two parts based on the date",
          "Merging with lag features <br>Merged test1 and test2 DataFrames with tmin and tmax DataFrames to add lag features 'diff_1_cc' and 'diff_1_ft' based on 'place'",
          "Cumulative sum calculation <br>Calculated cumulative sum for 'ConfirmedCases' and 'Fatalities' in test1 and test2 DataFrames based on 'diff_1_cc' and 'diff_1_ft'",
          "Handling initial date values <br>Handled initial date values by assigning values from the train DataFrame to the test2 DataFrame for 'ConfirmedCases' and 'Fatalities'",
          "Standard Scaling <br>The code applies standard scaling to the features using the StandardScaler from sklearn.preprocessing. This method standardizes the features by removing the mean and scaling to unit variance.",
          "Stratified K-Fold Cross Validation <br>The code uses StratifiedKFold from sklearn.model_selection to perform cross-validation. This method ensures that each fold has the same proportion of target classes as the entire dataset, which is important for imbalanced classification tasks.",
          "Ensembling <br>The code combines predictions from three different models using mean ensembling with weights [0.8, 0.15, 0.05]. This is a feature engineering method to improve predictive performance by leveraging the strengths of multiple models.",
          "Date conversion to datetime format <br>The 'Date' column in both the 'train' and 'test' datasets is converted to datetime format using the 'pd.to_datetime' function with the 'infer_datetime_format' parameter set to True.",
          "Date feature extraction <br>The 'Date' column is further manipulated to extract the year, month, and day as separate integer features using the 'dt.strftime' function and then converting the resulting string to an integer.",
          "Handling missing values in 'Province_State' <br>The missing values in the 'Province_State' column are filled with the string 'nan' using the 'fillna' method.",
          "Label encoding of categorical variables <br>The 'Province_State' and 'Country_Region' columns are label encoded using the 'LabelEncoder' from the 'sklearn.preprocessing' module.",
          "Creating new features from existing data <br>The code reads in data from CSV files and manipulates the data to create a new DataFrame for submission. It also assigns a value to the private test set, which can be considered as creating new features for the submission.",
          "Data cleaning <br>The code sets a value of 0 for the 'ctl_vehicle' samples in the submission DataFrame, which can be considered as a form of data cleaning to ensure the submission is accurate and consistent.",
          "Handling missing values <br>The 'get_state' function is used to handle missing values in the 'State' column by replacing them with the corresponding 'Country' value.",
          "Date manipulation <br>The 'Date' column is converted to datetime format and then manipulated to extract the month and day as new features.",
          "Label encoding <br>Label encoding is applied to the 'Country' and 'State' columns to convert categorical values into numerical format for model training.",
          "Polynomial feature generation <br>Polynomial features are generated using the 'np.polyfit' and 'np.poly1d' functions to create new features for modeling.",
          "Log transformation <br>Applied log transformation to the target variable 'SalePrice' to make its distribution more normal.",
          "Handling outliers <br>Identified and removed outliers in the 'GrLivArea' feature to improve model performance.",
          "Missing value imputation <br>Filled missing values in categorical features with 'None' and in numerical features with 0 or mode values.",
          "Feature normalization <br>Normalized skewed features using box-cox transformation to make their distribution more normal.",
          "One-hot encoding <br>Applied one-hot encoding to convert categorical variables into numerical format for model training.",
          "Handling missing values <br>The code checks for missing values in the dataset and replaces them with appropriate values, such as 'None' for categorical variables and 0 for numerical variables.",
          "Feature transformation <br>The code transforms features such as 'LotArea' using log transformation to make the data more normally distributed.",
          "Feature creation <br>The code creates new features such as 'TotalSF', 'FlrsSF', 'QualScore', and 'BedBath' by combining existing features to capture more information about the properties.",
          "Categorical variable manipulation <br>The code encodes categorical variables using label encoding and one-hot encoding to convert them into a format suitable for machine learning models.",
          "Dropping columns <br>Columns 'Name' and 'PassengerId' were dropped from the dataset.",
          "Splitting and extracting information from 'Cabin' column <br>The 'Cabin' column was split into 'deck', 'num', and 'side' columns, and the original 'Cabin' column was dropped.",
          "Filling missing values <br>Missing values in columns 'HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'deck', and 'side' were filled with appropriate values.",
          "Binning numerical data <br>The 'Age' and 'num' columns were binned into categories based on specific ranges.",
          "One-hot encoding <br>Categorical variables in the dataset were one-hot encoded using the 'get_dummies' method.",
          "Standardization <br>The numerical features 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck' were standardized using the 'StandardScaler' from sklearn.",
          "Date formatting <br>The 'Date' column is formatted by removing the hyphens and converting the resulting string to an integer for both the 'data' and 'test_data' DataFrames.",
          "Creating a new key <br>A new column 'key' is created in both 'data' and 'test_data' DataFrames by concatenating 'Country_Region' and 'Province_State' columns as strings.",
          "Pivot tables <br>Pivot tables are created using the 'pivot_table' function to reshape the data for analysis.",
          "Correlation calculation <br>The correlation between 'ConfirmedCases' and 'Fatalities' columns is calculated using the 'corr' function.",
          "Shift calculation <br>A shift is applied to the 'ConfirmedCases' and 'Fatalities' data to calculate the difference between consecutive days.",
          "Curve fitting <br>Curve fitting is performed using the 'curve_fit' function to fit exponential, linear, and sigmoid functions to the data.",
          "Feature transformation <br>New functions are defined to transform the features based on the curve fitting results.",
          "Standard Scaling <br>The 'StandardScaler' from the 'sklearn.preprocessing' module is used to scale the numerical features in the 'train' and 'test' datasets.",
          "Grouping and Aggregation <br>The code groups the 'train' dataset by certain features and calculates the mean of the 'loss' variable for each group. This is done for features 'f1' and 'f86'.",
          "Heatmap Visualization <br>The code uses a heatmap to visualize the mean of features grouped by the target variable 'loss' in the 'train' dataset.",
          "Kernel Density Estimation (KDE) Plot <br>The code creates KDE plots for each feature in the 'train' dataset to visualize the distribution of each feature.",
          "Boxplot Visualization <br>The code uses boxplots to visualize the distribution of each feature in the 'train' dataset.",
          "Correlation Analysis <br>The code calculates and visualizes the correlation between features in the 'train' dataset using a heatmap.",
          "Min-Max Scaling <br>The 'MinMaxScaler' from the 'sklearn.preprocessing' module is used to scale the numerical features in the 'train' and 'test' datasets.",
          "Handling non-normality in numerical features <br>The code identifies skewness in numerical features and applies log transformation for positive skewness and exponential transformation for negative skewness.",
          "Creating synthetic features <br>New features like 'Components', 'FCRatio', 'AggCmtRatio', and 'WtrCmtRatio' are created based on existing features.",
          "Scaling numerical features <br>MinMaxScaler is used to scale the numerical features.",
          "Standard Scaling <br>The StandardScaler from sklearn.preprocessing was used to scale the input features to have a mean of 0 and a standard deviation of 1.",
          "Min-Max Scaling <br>The MinMaxScaler from sklearn.preprocessing was used to scale the input features to a specified range, typically between 0 and 1.",
          "Blend of two kernels <br>Two different models are combined by taking a weighted average of their predictions. This is a form of ensembling, where the predictions of multiple models are combined to improve overall performance.",
          "Weighted sum of predictions <br>The predictions from the two models are combined using weighted averages, where the weights are determined based on the individual model scores. This allows for a more nuanced combination of predictions, giving more weight to the model with higher performance.",
          "Dropping columns <br>Columns 'target' and 'ID_code' are dropped from the 'train' dataset.",
          "Data cleaning <br>The code reads the 'train' and 'test' datasets using fread function, which is faster than read.csv. This can be considered as a form of data cleaning to efficiently load large datasets.",
          "Feature transformation <br>The 'dataset' is transformed into xgb.DMatrix format, which is a specific data structure used by the xgboost package for efficient processing and training of the model.",
          "Cumulative sum <br>The cumulative sum of the 'u_in' feature is calculated for each 'breath_id' in both the train and test datasets.",
          "Last value transformation <br>The last value of the 'u_in' feature within each 'breath_id' group is calculated and added as a new feature.",
          "Lag transformation <br>The lag of the 'u_in' and 'u_out' features is calculated and added as new features, with missing values filled with 0.",
          "Difference calculation <br>The difference between consecutive values of 'u_in' and 'u_out' features is calculated and added as new features.",
          "Group-wise maximum and mean difference <br>The maximum and mean differences between the 'u_in' feature and its group-wise maximum and mean values are calculated and added as new features.",
          "Label Encoding <br>Performed label encoding for sparse features using sklearn's LabelEncoder",
          "Min-Max Scaling <br>Applied Min-Max scaling to dense features using sklearn's MinMaxScaler",
          "Handling Missing Values <br>Filled missing values in both sparse and dense features with specific values ('null' for sparse features and -999 for dense features)",
          "Data Type Conversion <br>Converted sparse features to string data type",
          "Feature Concatenation <br>Concatenated train and test data to create a single data set for further processing",
          "Handling missing values <br>Replacing all the Province_State that are null by the Country_Region values",
          "Handling the Date column <br>Converting the object type column into datetime type and extracting Date and Month from the datetime and converting the feature as int",
          "Label Encoding <br>Using LabelEncoder to transform categorical variables into numerical format",
          "Model Training <br>Using BaggingRegressor with DecisionTreeRegressor as base estimator for training the model",
          "PCA <br>Principal Component Analysis (PCA) was applied to create new components 'PCA_Lat' and 'PCA_Long' from the original latitude and longitude features.",
          "Outlier Removal <br>Outliers were identified and removed based on latitude and longitude values to improve the quality of the data.",
          "Clustering <br>KMeans and Bayesian Gaussian Mixture models were used to create clusters based on latitude and longitude prior to scaling.",
          "Feature Scaling <br>StandardScaler was used to scale the numerical features in the dataset.",
          "Feature Selection <br>LassoCV was used to select important features and eliminate less important ones based on the L1 regularization.",
          "One-Hot Encoding <br>Categorical variables were dummy encoded using OneHotEncoder to prepare them for modeling.",
          "Converting date-time to datetime format <br>The code uses the 'pd.to_datetime' function to convert the 'date_time' column in both the training and test datasets to datetime format.",
          "Train-Validation Split <br>The code uses the 'train_test_split' function from sklearn to split the training data into a training set and a validation set for model evaluation.",
          "XGBoost Model Training <br>The code uses the XGBoost regressor to train separate models for each target variable: 'target_carbon_monoxide', 'target_benzene', and 'target_nitrogen_oxides'.",
          "Feature Selection <br>The code selects specific features for training the XGBoost models, as indicated by the 'features_list' variable.",
          "Prediction Visualization <br>The code visualizes the model predictions against the actual target values for each target variable, providing insights into the model performance.",
          "Model Training on Full Dataset <br>After evaluating the models, the code retrains the XGBoost models on the entire training dataset before making predictions for the test dataset.",
          "Handling outliers <br>Dropping houses with more than 4000 sq ft living area from the training data",
          "Imputation <br>Filling missing values for GarageQual, GarageCond, GarageFinish, and GarageYrBlt in the test set",
          "Imputation <br>Assuming no garage for the test example with missing GarageType",
          "Imputation <br>Filling missing LotFrontage values by the median LotFrontage of the neighborhood",
          "Label Encoding <br>Converting categorical features into ordinal numbers using LabelEncoder",
          "Feature Creation <br>Creating new features based on existing features, such as TotalArea, Age, TimeSinceSold, etc.",
          "One-Hot Encoding <br>Converting categorical features into binary vectors using one-hot encoding",
          "Log Transformation <br>Taking log transformation of skewed numeric features to make them more normal",
          "Standardization <br>Scaling the numeric features to have mean 0 and variance 1",
          "Standardization <br>The code applies standard scaling to the input features using sklearn's StandardScaler.",
          "One-Hot Encoding <br>The code uses one-hot encoding to convert categorical variables 'cp_time' and 'cp_dose' into numerical format.",
          "Feature Creation <br>The code creates new statistical features such as sum, mean, standard deviation, kurtosis, and skewness for the 'g-' and 'c-' features.",
          "Handling missing values <br>Filling missing values with 'None' for categorical features and with median for numerical features",
          "One-hot encoding <br>Converting categorical variables into dummy/indicator variables",
          "Creating new features <br>Creating new features like 'Age_House', 'TotalBsmtBath', 'TotalBath', 'TotalSA' based on existing numerical features",
          "Mapping categorical variables to numerical values <br>Mapping ordinal categorical variables to numerical values using predefined mapping dictionaries",
          "Standardization <br>StandardScaler() is used to standardize the features by removing the mean and scaling to unit variance.",
          "Support Vector Machine (SVM) <br>LinearSVC is used to find the separating hyperplane for the data.",
          "Principal Component Analysis (PCA) <br>PCA is not directly used in the code, but the data may have been preprocessed using PCA as a feature transformation method.",
          "Feature Creation <br>New features are created based on the 'chunk' column of the test data, and the 'f93' column of the test data is used to create new features in the 'submission_probed_f93.csv' file.",
          "Grouping and Aggregation <br>The code groups the data by 'breath_id' and calculates the maximum 'time_step' for both train and test datasets. It also groups the data by 'R' and 'C' and calculates the mean, max, and count of 'time_step' for both train and test datasets.",
          "Data Visualization <br>The code uses various visualization techniques such as KDE plot, line plot, and bar plot to analyze the distribution and relationship of features like 'time_step', 'pressure', 'u_in', and 'u_out' based on different conditions of 'R' and 'C'. It also visualizes the average pressure over time for different combinations of 'R' and 'C'.",
          "Feature Creation <br>The code creates a new feature 'rounded_time_step' by rounding the 'time_step' to two decimal places. This new feature is used for further analysis and visualization.",
          "Creating lag features <br>Lag features are created by shifting the sales data for each item by a certain number of days, such as 28, 35, 42, etc. This allows the model to capture the historical patterns and trends in the data.",
          "Categorical variable encoding <br>Categorical variables like 'item_id', 'dept_id', 'cat_id', 'store_id', and 'state_id' are encoded using integer codes to represent the categories. This is done to convert categorical data into a format that can be provided to ML algorithms for training.",
          "Memory optimization <br>The 'reduce_mem_usage' function is used to optimize the memory usage of the dataframe by downcasting the data types of numeric columns to lower memory-consuming types, such as int16, int8, float16, etc.",
          "Creating time-based features <br>Features related to time, such as day of the week, event names, event types, and week number, are extracted from the calendar data and merged with the main dataframe. These features can provide valuable information for predicting sales patterns.",
          "Dropping invalid fare amount <br>Dropping rows with fare_amount less than 2.5",
          "Dropping missing values <br>Dropping rows with missing values",
          "Dropping passenger count outliers <br>Dropping rows with passenger_count greater than 7 or less than 1",
          "Dropping taxi fare outliers <br>Dropping rows with fare_amount greater than 250",
          "Restricting latitude and longitude values <br>Filtering rows based on specific latitude and longitude ranges",
          "Calculating distance between pickup and dropoff points <br>Using Haversine formula to calculate distance in kilometers",
          "Extracting time features <br>Extracting hour of day, month, year, and weekday from pickup_datetime",
          "Creating lag features <br>Lag features are created by shifting the time series data by a certain number of time steps to capture historical patterns and trends.",
          "Rolling statistics <br>Calculating rolling mean and standard deviation to capture short-term and long-term trends and seasonality in the time series data.",
          "Price change calculation <br>Calculating the percentage change in prices over time to capture the impact of price fluctuations on sales.",
          "Encoding categorical variables <br>Label encoding categorical variables to convert them into numerical format for model training.",
          "Incorporating calendar events <br>Using calendar events such as holidays, special days, etc., as features to capture their impact on sales.",
          "State-specific features <br>Incorporating state-specific variables such as 'snap_CA', 'snap_TX', etc., to capture regional influences on sales.",
          "Creating Family_Size column <br>Adding the 'SibSp' and 'Parch' columns to create a new 'Family_Size' column in both the training and test datasets.",
          "Creating Fare_Per_Person column <br>Dividing the 'Fare' column by the sum of 'Family_Size' and 1 to create a new 'Fare_Per_Person' column in both the training and test datasets.",
          "Extracting Deck Floor from Cabin <br>Using regular expressions to extract the first letter from the 'Cabin' column and creating a new 'Deck' column in both the training and test datasets.",
          "Extracting Cabin Number from Cabin <br>Using regular expressions to extract the numeric part from the 'Cabin' column and creating a new 'Cabin_Number' column in both the training and test datasets.",
          "Extracting Title Prefix from Name <br>Using regular expressions to extract the title prefix from the 'Name' column and creating a new 'Title' column in both the training and test datasets.",
          "Grouping low cardinality titles <br>Replacing specific titles with 'Other' in the 'Title' column in both the training and test datasets.",
          "Replacing categorical values with numerical values <br>Using the replace() method to convert categorical values to numerical values",
          "Filling missing values <br>Using the fillna() method to fill missing values in the 'Age' and 'Embarked' columns",
          "Creating new feature 'Salutation' <br>Extracting salutation from 'Name' and mapping it to numerical values, also replacing some salutations with 'Rare' and combining similar salutations",
          "Creating new feature 'Ticket_Lett' <br>Extracting the first letter from 'Ticket' and mapping it to numerical values",
          "Creating new feature 'Ticket_Len' <br>Calculating the length of 'Ticket'",
          "Creating new feature 'Cabin_Lett' <br>Extracting the first letter from 'Cabin' and mapping it to numerical values",
          "Creating new feature 'FamilySize' <br>Calculating the family size by adding 'SibSp' and 'Parch' and 1",
          "Creating new feature 'IsAlone' <br>Setting 'IsAlone' to 1 if 'FamilySize' is 1, else 0",
          "Creating new feature 'Age' categories <br>Categorizing 'Age' into different age groups",
          "Removing features with high percentage of zeroes <br>Features with more than 98% zeroes are removed from the dataset",
          "Removing constant columns <br>Columns with constant values are removed from the dataset",
          "Removing duplicate columns <br>Duplicate columns are identified and removed from the dataset",
          "Adding new feature 'SumZeros' <br>A new feature is added to count the number of zeros in each row",
          "Adding new feature 'SumValues' <br>A new feature is added to count the number of non-zero values in each row",
          "Adding new aggregated features <br>Mean, Median, Mode, Max, Variance, and Standard Deviation are calculated and added as new features",
          "Applying Principal Component Analysis (PCA) <br>PCA is used to reduce the dimensionality of the dataset",
          "Applying Truncated Singular Value Decomposition (tSVD) <br>tSVD is used to reduce the dimensionality of the dataset",
          "Applying Independent Component Analysis (ICA) <br>ICA is used to separate independent sources from a mixed signal",
          "Applying Gaussian Random Projection (GRP) <br>GRP is used to reduce the dimensionality of the dataset",
          "Applying Sparse Random Projection (SRP) <br>SRP is used to reduce the dimensionality of the dataset",
          "Label Encoding <br>The code uses LabelEncoder to transform categorical variables into numerical values.",
          "Feature Creation <br>New features are created by performing mathematical operations on existing features, such as multiplication of 'cont8' with 'cont0', 'cont13', 'cont11', and combinations of other continuous variables.",
          "Dropping columns with high missing values <br>Columns with missing values greater than 40% are identified and dropped from the dataset.",
          "Handling missing values <br>Missing values are filled with a specific value (-999) using the fillna method.",
          "Feature selection <br>Columns 'M6' and 'TransactionID' are dropped from the dataset, indicating a form of feature selection.",
          "Dropping columns <br>Columns 'winPlacePerc' and 'Id' are dropped from the training data.",
          "Standard Scaling <br>The data is standardized using StandardScaler, but it is commented out in the code.",
          "LightGBM Model Training <br>A LightGBM model is trained using the provided parameters and the training data.",
          "Datetime conversion <br>The 'date_time' column in both 'data_1' and 'data_2' is converted to datetime format using pd.to_datetime() method.",
          "Season extraction <br>A new feature 'season' is created based on the month of the 'date_time' column in both 'data_1' and 'data_2'.",
          "Day extraction <br>A new feature 'day' is created based on the weekday of the 'date_time' column in 'data_2'.",
          "Derived feature creation <br>A new feature 'Nc' is created by calculating the ratio of 'absolute_humidity' to 'relative_humidity' in both 'data_1' and 'data_2'.",
          "Time series decomposition <br>Applied to decompose time series data into trend, seasonality, and remainder components for each feature and target variable.",
          "Anomaly detection <br>Anomalies were detected using the Generalized ESD Test for Outliers (GESD) method for each feature and target variable.",
          "Linear regression modeling <br>Fitted linear regression models using the fable.prophet package to fix anomalies in the data for each feature.",
          "Feature lagging <br>Lagged features were created using the step_lag function in the recipe_base to capture historical patterns in the data.",
          "K-means clustering <br>Applied k-means clustering to group similar data points together for the features deg_C, relative_humidity, absolute_humidity, sensor_1, sensor_2, sensor_3, sensor_4, and sensor_5.",
          "Handling missing values <br>The code checks for null values in the combined dataset and handles them appropriately.",
          "Data visualization <br>The code uses various plots to visualize the distribution of numerical and categorical features with respect to the target variable.",
          "Outlier detection and removal <br>The code uses z-score to detect and remove outliers from the numerical features.",
          "Encoding categorical variables <br>The code uses OrdinalEncoder to transform categorical variables into numerical values for modeling.",
          "Feature scaling <br>The code uses StandardScaler to scale the numerical features for modeling.",
          "Ensemble modeling <br>The code applies ensemble modeling techniques such as VotingClassifier to combine predictions from multiple models.",
          "Feature combination <br>The code combines predictions from different models using weighted averages to create the final submission.",
          "Downcasting <br>The _down_cast function is used to downcast the data types of the columns in the dataframes to reduce memory usage.",
          "Data Preprocessing <br>The data_preprocessing function is used to preprocess the input data, including merging, reshaping, and creating new features.",
          "One-Hot Encoding <br>OneHotEncoder is used to convert categorical variables into binary vectors.",
          "Target Encoding <br>TargetEncoder is used to encode categorical variables based on the mean of the target variable.",
          "Feature Imputation <br>SimpleImputer is used to fill missing values in the features.",
          "Feature Selection <br>SelectPercentile is used to select features based on their importance using chi-squared test.",
          "Feature Union <br>FeatureUnion is used to combine multiple feature extraction and transformation methods into a single transformer.",
          "Custom Lag Feature <br>A custom LagTransformer is used to create lag features based on historical data.",
          "Custom Event Transformation <br>A custom EventTransformer is used to transform event-related features into binary vectors.",
          "Sorting features by mean value <br>train.describe().T[1:].sort_values(by='mean',ascending=False)",
          "Identifying binary features <br>for i in train.columns: if len(train[i].value_counts())<=2: features.append(i)",
          "Visualizing count and percentage of target variable <br>Bar plot of count and percentage of 'Cover_Type'",
          "Creating lists for count of binary features in train and test datasets <br>Looping through columns and counting occurrences of 0 and 1",
          "Visualizing count of binary features in train and test datasets <br>Bar plot of count of binary features in train and test datasets",
          "Creating dictionaries for count of categorical features in train and test datasets <br>Looping through categorical features and creating dictionaries of value counts",
          "Visualizing count of categorical features in train and test datasets <br>Bar plot of count of categorical features in train and test datasets",
          "Visualizing distribution of numerical features in train and test datasets <br>Histograms of numerical features in train and test datasets",
          "Calculating correlation between features <br>train[features].corr()",
          "Date feature extraction <br>The code extracts useful date features from the input data by identifying unique date features and excluding redundant ones. It then returns the list of useful date features for further processing.",
          "Minimum date calculation <br>The code calculates the minimum date for each row in the dataset by iterating through the chunks of data and finding the minimum date value. It then combines the minimum date values from different chunks and returns the subset of data with the minimum date values.",
          "Minimum date difference calculation <br>The code calculates the difference between consecutive minimum date values for each row in the dataset. It then creates a new column to store these differences.",
          "Matthews correlation coefficient (MCC) calculation <br>The code defines a function to calculate the MCC, which is a measure of the quality of binary classifications. It uses true positive, true negative, false positive, and false negative values to compute the MCC for a given set of predictions and true labels.",
          "Leave-one-out (LOO) encoding <br>The code implements LOO encoding for categorical variables by calculating the mean response value for each category and using it to replace the original categorical values. This is done for both the training and test datasets.",
          "XGBoost model training <br>The code uses the XGBoost library to train a binary classification model on the engineered features. It sets up the model parameters, creates DMatrix for training and testing data, and trains the model using cross-validation. It also computes the importance of features and generates submission files based on the model predictions.",
          "Outlier removal <br>Removing outliers based on conditions related to 'OverallQual', 'GrLivArea', 'SalePrice', 'LotFrontage', 'LotArea', etc.",
          "Converting non-numeric predictors to strings <br>Converting 'MSSubClass', 'YrSold', 'MoSold' to string data type",
          "Filling missing values <br>Filling missing values for various predictors such as 'Alley', 'PoolQC', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence', 'Street', 'LotShape', 'LandContour', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'CentralAir', 'Electrical', 'MiscFeature', 'MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Functional', 'SaleType', 'SaleCondition', etc.",
          "Filling missing values for numeric predictors <br>Filling missing values for numeric predictors such as 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'",
          "Log transformation <br>Adding log-transformed predictors for various features such as 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'YearRemodAdd', 'TotalSF'",
          "Creating dummy variables <br>Creating dummy variables for categorical predictors such as 'MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'BsmtFinType2', 'Heating', 'HouseStyle', 'Foundation', 'MasVnrType', 'BsmtFinType1', 'Electrical', 'Functional', 'GarageType', 'Alley', 'Utilities', 'GarageCond', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition', 'LandSlope', 'CentralAir', 'GarageFinish', 'BsmtExposure', 'Street'",
          "Adding real valued features <br>Adding real valued features such as 'LotFrontage_log', 'LotArea_log', 'MasVnrArea_log', 'BsmtFinSF1_log', 'BsmtFinSF2_log', 'BsmtUnfSF_log', 'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'LowQualFinSF_log', 'GrLivArea_log', 'BsmtFullBath_log', 'BsmtHalfBath_log', 'FullBath_log', 'HalfBath_log', 'BedroomAbvGr_log', 'KitchenAbvGr_log', 'TotRmsAbvGrd_log', 'Fireplaces_log', 'GarageCars_log', 'GarageArea_log', 'PoolArea_log', 'MiscVal_log', 'YearRemodAdd', 'TotalSF_log', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'KitchenQual', 'HeatingQC', 'BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageQual', 'PoolQC', 'PavedDrive', 'HasWoodDeck', 'HasOpenPorch', 'HasEnclosedPorch', 'Has3SsnPorch', 'HasScreenPorch'",
          "Adding squared predictors <br>Adding squared predictors for features such as 'YearRemodAdd', 'LotFrontage_log', 'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log', 'GarageCars_log', 'GarageArea_log', 'OverallQual', 'ExterQual', 'BsmtQual', 'GarageQual', 'FireplaceQu', 'KitchenQual'",
          "Merging Data <br>Merging train data with train_labels based on 'sequence' column, and merging test data with aggregated features of test data based on 'sequence' and 'subject' columns.",
          "Aggregated Features Calculation <br>Calculating aggregated statistical features (mean, max, min, var, mad, sum, median) for sensor data grouped by 'sequence' and 'subject' for both train and test data.",
          "Feature Selection <br>Dropping unnecessary columns 'sequence', 'subject', and 'size' from train and test data.",
          "VarianceThreshold <br>Applied to remove features with low variance",
          "StratifiedKFold <br>Used for cross-validation to ensure each fold has a proportional representation of the target classes",
          "QuadraticDiscriminantAnalysis <br>Utilized for classification and feature engineering",
          "Boolean to Binary Conversion <br>The code replaces boolean values with 1's and 0's for the 'VIP', 'CryoSleep', and 'Transported' columns.",
          "Handling Missing Values <br>The code fills missing values in specific columns with 0, such as 'VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck'.",
          "Cabin Splitting <br>The code splits the 'Cabin' column into 'Floor', 'CabinNum', and 'Side' columns.",
          "Normalization <br>Standardization using mean and standard deviation for each feature",
          "Train, Test, Validation Split <br>Randomly splitting the data into train, test, and validation sets",
          "Label Encoding <br>Converting categorical variables to numerical values using LabelEncoder",
          "Missing Value Imputation <br>Filling missing values with the mean of the respective column",
          "Converting object type columns to numerical type <br>Identifying and converting columns with 'object' data type to numerical type using appropriate methods such as replacing yes/no values with 1/0, and using related columns for transformation.",
          "Handling missing values <br>Replacing missing values with appropriate values based on related columns or domain knowledge, such as replacing NaN values with 0 for specific conditions, and filling missing values with mean for certain columns.",
          "Feature transformation <br>Performing feature transformation by taking square root of certain columns, such as 'dependency' and 'edjefe', and using the transformed values for further analysis.",
          "Label Encoding <br>The categorical variables 'cut', 'color', and 'clarity' are encoded using predefined mapping dictionaries to convert them into numerical values.",
          "Outlier Removal <br>Outliers are removed from the numerical columns by filtering out values below the 1st percentile and above the 99th percentile.",
          "Standard Scaling <br>Standard scaling is applied to the numerical features using the StandardScaler to ensure that all features have the same scale.",
          "XGBoost Model Training <br>An XGBoost model is trained using the processed data to predict the target variable 'price'.",
          "Grid Search for Hyperparameter Tuning <br>Grid search is performed to find the best hyperparameters for the XGBoost model using the GridSearchCV method.",
          "Data Loading <br>The code loads the training and test data using pandas read_csv function.",
          "Data Visualization <br>The code uses seaborn and matplotlib to visualize the distribution of features and correlation between features.",
          "Principal Component Analysis (PCA) <br>The code applies PCA to reduce the dimensionality of the data by creating new features that are linear combinations of the original features.",
          "Train-Test Split <br>The code defines a custom train_test_split function to split the data into training and testing sets.",
          "Feature Column Generation <br>The code generates feature columns for the TensorFlow model using tf.feature_column.numeric_column and tf.feature_column.embedding_column.",
          "Data Preprocessing for TensorFlow <br>The code defines functions to generate train and test data in a format suitable for TensorFlow model training.",
          "Missing Data Analysis <br>The code calculates the total and percentage of missing values for each feature in the dataset.",
          "Data Visualization <br>The code uses various visualization techniques such as count plots, heatmaps, and distribution plots to analyze the distribution of features in the dataset.",
          "Feature Creation <br>New features such as sum, min, max, mean, std, skew, kurt, and med are created based on the existing features in the dataset.",
          "Noise Addition <br>The code adds noise to the dataset by applying random normal noise to the existing features.",
          "LightGBM Model Training <br>The code uses LightGBM to train a model for the Kaggle competition, which involves feature engineering techniques such as feature selection and transformation within the model training process.",
          "Correlation analysis <br>The code identifies features that have high correlations with the target variable by calculating the correlation coefficient between each feature and the target variable. Features with correlation coefficients greater than 0.05 or less than -0.05 are selected for further analysis.",
          "Skewness analysis <br>The code identifies features with high skewness by calculating the skewness of each feature. Features with skewness greater than or equal to 0.5 or less than or equal to -0.5 are selected for further analysis.",
          "Featuretools library <br>The code uses the featuretools library to perform automated feature engineering. It creates an entity set, defines entities from the input data, and generates new features using specified transformation primitives such as 'multiply_numeric' and 'add_numeric'.",
          "Feature selection based on correlation <br>The code drops features that are highly correlated with each other by calculating the correlation matrix and removing features with a correlation greater than 0.7. The remaining features are selected for model training.",
          "Data loading <br>The code loads the training, test, and sample submission data using the read_csv function from the tidyverse package.",
          "Data structure and class inspection <br>The code inspects the structure and class of the loaded data using attributes(train)%>% names() and attributes(train)$class.",
          "Data dimensions <br>The code calculates the number of rows and columns in the training data using nrow(train) and ncol(train).",
          "Proportion of missing values <br>The code calculates the proportion of missing values in the data using mean(is.na(train)).",
          "Features correlation <br>The code attempts to identify correlated features and potentially remove them using correlation analysis and feature selection.",
          "Data splitting <br>The code splits the training data into train and validation sets using the sample function.",
          "Linear model fitting <br>The code fits a linear model to the training data using the lm function and performs feature selection based on model summary and ANOVA.",
          "Random forest model fitting <br>The code fits a random forest model to the training data using the randomForest function and assesses the model's performance with validation set.",
          "Gradient boosting model fitting <br>The code fits a gradient boosting model to the training data using the gbm function and evaluates the model's performance using various methods such as OOB, test set, and cross-validation.",
          "XGBoost model fitting <br>The code fits XGBoost models with both linear and tree boosters to the training data using the xgboost function.",
          "Support Vector Machine (SVM) model fitting <br>The code fits SVM models to the training data using the svm function with different kernel types and assesses the model's performance.",
          "H2O AutoML and model fitting <br>The code utilizes H2O's AutoML, trains and cross-validates Gradient Boosting Machine (GBM) and Random Forest (RF) models, and creates a stacked ensemble model for prediction.",
          "Feature Selection <br>The code selects specific features 'boosts', 'killPlace', 'kills', 'vehicleDestroys', and 'walkDistance' from the dataset for training the model.",
          "Standardization <br>The code uses StandardScaler from scikit-learn to standardize the input features before training the model.",
          "Adding new features <br>New features like Water_Cement, Coarse_Fine, Aggregate, Aggregate_Cement, Slag_Cement, Ash_Cement, Plastic_Cement, Age_Water were created based on existing features.",
          "Handling duplicate data <br>Duplicate data was handled by dropping duplicates based on a subset of columns and taking the median of the remaining data.",
          "Time series data extraction <br>The code extracts time series data from the 'sales_train' dataframe and assigns it to the 'temp_series' variable.",
          "Date-time index creation <br>The code creates a date-time index using the 'calendar_df' dataframe and assigns it to the 'temp_series' variable.",
          "Data transformation <br>The code transforms the 'temp_series' data into a suitable format for the Prophet model by resetting the index and renaming the columns to 'ds' and 'y'.",
          "Renaming columns <br>The code renames the 'UDI' column to 'id' in the 'original' dataframe.",
          "Dropping columns <br>The code drops the 'id' column from the 'train' and 'test' dataframes, and also drops the 'id' column from the 'original' dataframe.",
          "Concatenating dataframes <br>The code concatenates the 'train' and 'original' dataframes along the rows to create a new 'train' dataframe.",
          "One-hot encoding <br>The code uses the 'cat_features' parameter in the CatBoost Pool to specify the categorical columns 'Product ID' and 'Type', indicating that these columns should be treated as categorical variables.",
          "Data type conversion <br>The code converts the data types of certain columns to integer type using the 'astype' method.",
          "Feature aggregation <br>The code aggregates symptoms related columns to create new features such as 'similar_cluster', 'chikungunya_columns', 'lyme_columns', 'pain', 'skin', 'inflammation', 'bleeding', 'red_cols', 'orange_cols', and 'green_cols'.",
          "Feature scaling <br>The code uses the 'StandardScaler' and 'MinMaxScaler' from the 'sklearn.preprocessing' module to scale the features.",
          "Feature selection <br>The code uses permutation importance to evaluate feature importance and select relevant features for the model.",
          "Categorical variable mapping <br>The 'cp_time' and 'cp_dose' categorical variables are mapped to numerical values using a dictionary mapping.",
          "Creation of custom PyTorch Dataset classes <br>Custom TrainDataset and TestDataset classes are created to handle the numerical and categorical features, and labels for training and testing.",
          "Normalization of numerical features <br>The numerical features are normalized using torch.FloatTensor.",
          "Creation of a TabularNN model <br>A custom neural network model 'TabularNN' is created using PyTorch's nn.Module, which includes linear layers, batch normalization, dropout, and activation functions.",
          "Training and validation of the neural network model <br>The model is trained and validated using custom functions for training, validation, and inference, with the use of BCEWithLogitsLoss as the loss function.",
          "K-fold cross-validation <br>The K-fold cross-validation is implemented to train and validate the neural network model on different subsets of the data.",
          "Encoding categorical variables <br>The 'Sex' and 'Embarked' columns are encoded using numerical values, where 'male' is replaced with 1 and 'female' with 0 for 'Sex', and 'S' with 0, 'C' with 1, and 'Q' with 2 for 'Embarked'.",
          "Extracting titles from names <br>The titles are extracted from the 'Name' column using the comma and period as delimiters, and then the titles are categorized into 'Mr', 'Miss', 'Mrs', 'Master', and 'Rare'. These categories are then encoded with numerical values.",
          "Cabin feature engineering <br>The first letter of the 'Cabin' values is extracted and used to categorize the cabins. The categories are then encoded with numerical values.",
          "Dropping unnecessary columns <br>The 'Ticket' column is dropped from the datasets as it is considered unnecessary for the analysis.",
          "Iterative imputation <br>Iterative imputation is used to fill missing values in the dataset.",
          "Data loading <br>Loaded the training and test data using pd.read_csv()",
          "Data concatenation <br>Concatenated the training and test data into a single dataframe using append()",
          "Data visualization <br>Visualized the distributions of features in the training and test data using histograms",
          "Feature analysis <br>Analyzed the distribution and unique values of a specific feature using histograms and value counts",
          "Duplicate data identification <br>Identified and displayed duplicated rows in the training and combined data",
          "Feature normalization <br>Attempted mean normalization using StandardScaler, but the code was commented out",
          "Feature selection <br>Manually set predictions for a specific class in the sample submission file",
          "Feature transformation <br>Set predictions in the sample submission file based on the target variable distribution in the training data",
          "Memory optimization <br>The code includes a function 'reduce_mem_usage' that iterates through all the columns of a dataframe and modifies the data type to reduce memory usage. It skips datetime type or categorical type columns and optimizes the data types of other columns to reduce memory usage.",
          "Handling missing values in weather data <br>The code fills missing values in specific weather columns with the mean of the respective column.",
          "Feature creation from timestamp <br>The code extracts features like 'hour', 'dayofweek', 'weekday', 'day', and 'month' from the timestamp column in the raw_train data.",
          "Log transformation <br>The code applies a log transformation to the 'square_feet' and 'meter_reading' columns to normalize their distributions.",
          "Categorical variable manipulation <br>The code uses LabelEncoder to transform the 'primary_use' categorical variable into numerical values.",
          "Feature selection <br>The code selects specific features for the training data and drops unnecessary columns.",
          "JSON normalization <br>The code uses json_normalize to flatten JSON columns such as 'device', 'geoNetwork', 'totals', and 'trafficSource' into separate columns.",
          "Datetime feature extraction <br>The code extracts features such as month, quarter of the month, day, and weekday from the 'date' column using datetime operations.",
          "Handling missing values <br>The code fills missing values in columns like 'totals.pageviews', 'totals.newVisits', 'totals.bounces', and 'trafficSource.isTrueDirect' with appropriate values.",
          "Creating binary features <br>The code creates binary features like '_adContentGMC', '_withCampaign', and '_sourceGpmall' based on conditions from the 'trafficSource' and 'device' columns.",
          "Cumulative count feature <br>The code creates a cumulative count feature '_buyCount' based on the number of times a visitor has made a purchase.",
          "Mean and sum aggregation <br>The code calculates mean and sum of hits per day, weekday, and month, and creates new features '_meanHitsPerDay', '_meanHitsPerWeekday', '_meanHitsPerMonth', '_sumHitsPerDay', '_sumHitsPerWeekday', and '_sumHitsPerMonth'.",
          "One-hot encoding <br>The code performs one-hot encoding for categorical variables using pd.get_dummies.",
          "Label Encoding <br>Performed label encoding on categorical features using sklearn's LabelEncoder",
          "Random Forest Regression <br>Trained a Random Forest Regressor model for prediction",
          "Feature Selection <br>Dropped 'id' column from the training and test data",
          "Log transformation <br>The 'SalePrice' column is transformed using the natural logarithm function to normalize its distribution.",
          "Handling missing values <br>The missing values in the input data are filled with zeros using the fillna method.",
          "Feature creation - Square root transformation <br>A new set of features is created by taking the square root of the numeric columns in the input data.",
          "One-hot encoding <br>Categorical variables are transformed into binary vectors using the get_dummies method.",
          "Group mean feature <br>Calculated the mean of features for each group within a match",
          "Group max feature <br>Calculated the maximum value of features for each group within a match",
          "Group min feature <br>Calculated the minimum value of features for each group within a match",
          "Group size feature <br>Calculated the size of each group within a match",
          "Match mean feature <br>Calculated the mean of features for each match",
          "One-Hot Encoding <br>The code uses one-hot encoding to convert categorical features into numerical format, creating binary columns for each category within the feature.",
          "Feature Generation <br>The code generates new features by performing operations on existing features, such as creating new date-related features based on the TransactionDT column.",
          "Feature Selection <br>The code applies feature selection by removing columns with low importance based on a threshold value, and only keeping the important features for further analysis.",
          "Parsing datetime <br>The parse_datetime function is used to convert the 'datetime' column to type datetime and then split it into separate columns for year, month, day, hour, and weekday.",
          "Categorical variable mapping <br>The 'season' and 'weather' columns are mapped to more descriptive categories, and all categorical variables are converted to the 'category' data type.",
          "Outlier detection and removal <br>The detect_outliers function is used to identify outliers using the three-sigma rule, and the delete_outliers function is used to remove the identified outliers from the dataset.",
          "Log transformation <br>The 'count', 'casual', and 'registered' columns are transformed using the natural logarithm to handle skewness and improve model performance.",
          "Parsing Date <br>The 'date' column is split into 'year', 'month', and 'day' columns using the 'parse_date' function.",
          "Weekday or Weekend <br>A new column 'weekday' is created to identify whether a particular date is a weekday or a weekend using the 'weekend_or_weekday' function.",
          "One-Hot Encoding <br>Categorical columns 'country', 'store', and 'product' are one-hot encoded using the 'encoder_cat' function.",
          "Fourier Features <br>Fourier features are computed and added to capture seasonality and trend in the time series data.",
          "Parsing dates <br>The 'pickup_datetime' column is parsed as a datetime object using the 'parse_dates' parameter in the 'pd.read_csv' function.",
          "Random sampling <br>A random sampling method is applied to skip rows in the dataset using the 'skiprows' parameter in the 'pd.read_csv' function.",
          "Data cleaning <br>Rows with missing values are dropped using the 'dropna' method.",
          "Geospatial bounding box filtering <br>The dataset is filtered based on a geographical bounding box to include only the data within specific longitude and latitude ranges.",
          "Haversine distance calculation <br>The haversine distance between pickup and dropoff locations is calculated to create a new feature 'trip_distance'.",
          "Landmark distance calculation <br>Distances from dropoff locations to specific landmarks (e.g., airports) are calculated to create new features.",
          "Date-time feature extraction <br>Date-time features such as year, month, day, weekday, and hour are extracted from the 'pickup_datetime' column.",
          "Mapping categorical variables to numerical values <br>The code creates a dictionary to map unique country names to integer values and then applies this mapping to the 'Country/Region' column to create a new 'Country_int' column with numerical values.",
          "Handling date features <br>The code converts the 'Date' column to datetime format and then extracts day and month information into separate columns.",
          "Handling missing values in categorical data <br>The code checks for missing values in the 'Province/State' column and fills them with 0.",
          "Creating new features using mathematical operations <br>The code creates new 'X_Cord' and 'Y_Cord' columns by applying mathematical operations on the 'Lat' and 'Long' columns.",
          "Correlation analysis <br>The code calculates the correlation matrix and visualizes it using a heatmap to understand the relationships between different features.",
          "Model training and prediction <br>The code uses various regression models (Linear Regression, K Nearest Neighbors, Decision Tree, Random Forest, XGBoost) to train on the features and predict the target variables ('ConfirmedCases' and 'Fatalities').",
          "Handling Outliers <br>Identifying and removing extreme outliers in the 'GrLivArea' feature to improve model performance and accuracy.",
          "Log Transformation <br>Applying a log(1+x) transformation to the target variable 'SalePrice' to improve the normality of its distribution and model performance.",
          "Imputation <br>Imputing missing values in categorical features with appropriate values such as 'None' or mode, and imputing missing values in numerical features with 0 or median.",
          "Box-Cox Transformation <br>Applying Box-Cox transformation to skewed numerical features to improve their normality and model performance.",
          "Feature Creation <br>Creating new features such as 'HasPool', 'Has2ndfloor', 'HasGarage', 'HasBsmt', 'HasFireplace', 'YrBltAndRemod', 'TotalSF', 'Total_SQR_Footage', 'Total_Bathrooms', and 'Total_Porch_SF' to capture additional information and improve model performance.",
          "Dummy Variable Creation <br>Creating dummy variables for categorical features to convert them into a numerical format for model training.",
          "Dropping Columns <br>The code drops the 'id' column from the 'train' and 'test' datasets using the 'drop' method.",
          "Descriptive Statistics <br>The code calculates descriptive statistics such as mean, standard deviation, unique values, null values, and data types for the 'train', 'test', and 'orig_train' datasets using the 'describe', 'nunique', 'isna', and 'dtypes' methods.",
          "Checking for Duplicates <br>The code checks for duplicate values in the 'train', 'test', and 'orig_train' datasets using the 'duplicated' method and counts the non-duplicate values.",
          "Data Visualization <br>The code creates various visualizations including KDE plots, pie charts, and count plots to visualize the distribution of features and the target variable in the datasets.",
          "Correlation Analysis <br>The code calculates the correlation matrix and visualizes the correlation between features in the 'train', 'test', and 'orig_train' datasets using the 'corr' method and 'heatmap' function.",
          "Concatenating Datasets <br>The code concatenates the 'train' and 'orig_train' datasets using the 'concat' method to combine the data for further analysis.",
          "Dropping Duplicates <br>The code drops duplicate values from the concatenated dataset using the 'drop_duplicates' method.",
          "Feature Selection <br>The code selects the 'calc' feature from the datasets by using the 'pop' method and creating new datasets with only the selected feature.",
          "Feature Scaling <br>StandardScaler(), MinMaxScaler()",
          "Feature Selection <br>SelectFromModel(LogisticRegression(solver='liblinear')), SelectFromModel(XGBClassifier(n_estimators=500, max_depth=3))",
          "One-Hot Encoding <br>The categorical variables are converted into dummy variables using one-hot encoding, creating binary columns for each category within the categorical variables.",
          "Normalization <br>The numerical features are normalized using Min-Max scaling to bring them within a specific range, typically 0 to 1.",
          "Feature selection <br>The code selects features with names containing 'cont' from the training data to be used for modeling.",
          "Train/eval set split <br>The code splits the training data into training and validation sets using the train_test_split function from sklearn.model_selection.",
          "Standardization <br>The StandardScaler from sklearn.preprocessing was used to standardize the input features by removing the mean and scaling to unit variance.",
          "Data Reshaping <br>The input data was reshaped using numpy's reshape function to convert the data into a format suitable for LSTM model input, with the shape (batch_size, time_steps, features).",
          "GroupKFold Cross-Validation <br>The GroupKFold from sklearn.model_selection was used to split the data into multiple folds while ensuring that sequences from the same subject do not appear in more than one fold, which is important for time series data.",
          "Memory reduction <br>The code reduces memory usage of the dataframe by changing data types of numerical columns to lower memory types.",
          "Feature creation <br>New numerical features are created by performing arithmetic operations on existing numerical columns.",
          "Frequency encoding <br>Certain categorical variables are encoded based on their frequency of occurrence in the dataset.",
          "Label encoding <br>Categorical variables are label encoded using factorization.",
          "Converting date columns to datetime format <br>Using pd.to_datetime() to convert date columns to datetime format",
          "Creating new features from date columns <br>Extracting year and month from the datetime columns",
          "Calculating average of a feature per group <br>Using groupby() and mean() to calculate the average of active microbusinesses per county",
          "Merging dataframes <br>Using pd.merge() to merge train and test data with other datasets",
          "Clustering <br>Using KMeans clustering to cluster cfips, year, and month",
          "One Hot Encoding <br>Using pd.get_dummies() to perform one hot encoding on categorical variables",
          "Handling Missing Values <br>The code replaces missing values in specific columns with predefined values and fills in missing values with the mean of certain columns.",
          "Creating New Features <br>New features like 'ed_all', 'roof_waste_material', 'electricity_other', and 'head_less_18' are created based on certain conditions and calculations.",
          "Data Type Modification <br>The code modifies the data types of certain columns to ensure they are in the correct format for analysis.",
          "Categorical Variable Manipulation <br>The code replaces 'yes' and 'no' with 1 and 0, respectively, in specific columns to convert categorical variables into numerical format.",
          "Feature Selection <br>The code selects specific features for modeling by discarding certain columns that are not relevant for the analysis.",
          "JSON normalization <br>The code uses json_normalize to flatten JSON columns ('device', 'geoNetwork', 'totals', 'trafficSource') into separate columns, and then merges them with the original dataframe.",
          "Handling missing values <br>The code visualizes missing values in the dataset using a bar plot and handles missing values in the 'totals.transactionRevenue' column by filling them with 0 and converting the column to float type.",
          "Log transformation <br>The code applies a log transformation to the 'totals.transactionRevenue' column, replacing positive values with their natural logarithm and leaving non-positive values unchanged.",
          "Data type conversion <br>The code converts object and boolean data types to numerical using pd.factorize for all columns except 'fullVisitorId'.",
          "Feature selection <br>The code selects columns with more than one unique value and keeps only those columns in the dataframe.",
          "Cleaning column names <br>The clean_names() function from the janitor package is used to clean the column names by converting them to lowercase and replacing spaces with underscores.",
          "Handling missing values <br>Missing values are filled in specific columns with the mean value of the respective column using ifelse() and mean() functions.",
          "Mapping categorical variables to numeric values <br>The author column is mapped to numeric values using a predefined mapping. The distinct geometry values are also mapped to numeric values using seq_along() function.",
          "Merging datasets <br>The original dataset is merged with the distinct mapping of geometry values using left_join() function.",
          "Filtering rows based on conditions <br>Rows with non-null and null 'x_e_out' values are filtered using SQL queries with the sqldf() function.",
          "Building a regression model <br>A linear regression model is built using lm() function to predict 'x_e_out' based on multiple features.",
          "Data Loading <br>Loading the training and testing data using pandas read_csv function.",
          "Data Merging <br>Merging multiple dataframes using pandas merge function to combine information from different sources.",
          "Feature Removal <br>Removing the 'description', 'type_x', 'locale', 'locale_name', and 'transferred' features from the dataset.",
          "Categorical Feature Encoding <br>Converting non-numeric categorical labels to numeric using pandas astype('category').cat.codes method.",
          "Missing Value Imputation <br>Filling missing values in categorical features with mode and in numerical features with median.",
          "Outlier Detection and Removal <br>Using LocalOutlierFactor from sklearn to identify and remove outliers from the training data.",
          "One-Hot Encoding <br>Converting categorical variables into a binary vector representation using OneHotEncoder from sklearn.",
          "Model Training <br>Training a CatBoostRegressor model on the preprocessed data.",
          "Drop columns <br>Columns 'loss' and 'id' are dropped from the training data to separate the features and the target variable.",
          "Time series data transformation <br>The code uses the 'ts' function to transform the time series data for each country, setting the start and end dates and frequency.",
          "Auto ARIMA modeling <br>The code applies the 'auto.arima' function to automatically fit ARIMA models to the time series data for each country, allowing for drift.",
          "Feature creation <br>The code creates new features by predicting future values using the ARIMA models and combining them with the existing data.",
          "Data cleaning <br>The code performs data cleaning by removing missing values using the 'colSums(is.na(train))' function.",
          "Summary statistics calculation <br>Summary statistics such as min, max, mean, and standard deviation were calculated for each series_id using the summarise_all function in dplyr.",
          "Data cleaning <br>The code removed unnecessary columns such as 'measurement_number' and 'row_id' from the train and test datasets using the subset function.",
          "Random Forest modeling <br>A random forest model was built using the randomForest package to predict the 'surface' variable based on the engineered features.",
          "Scaling <br>The scale function is used to scale the X and Y coordinates by multiplying them by 100 and then flooring the result to get integer values.",
          "Frequency Counting <br>The code constructs a dictionary to count the frequency of each place_id for every (X,Y) coordinate after scaling.",
          "Top Frequent Places Extraction <br>The code identifies the top 3 frequent places for each (X,Y) coordinate based on the frequency counts.",
          "Creating new features based on date and time <br>The code creates new features such as 'month', 'day_of_week', 'season', 'is_weekend', and 'time_of_day' based on the date and time information in the dataset.",
          "Calculating new feature 'SMC' <br>The code calculates a new feature 'SMC' (Soil Moisture Content) based on the 'absolute_humidity' and 'relative_humidity' features.",
          "Categorical variable manipulation <br>The code specifies the data types of certain features as categorical variables, including 'season', 'time_of_day', 'month', and 'day_of_week'.",
          "Handling missing values <br>The code checks for missing values in the dataset using the isnull() method and sums the missing values for each column using the sum() method.",
          "Identifying constant features <br>The code identifies constant features by finding columns with only one unique value using the nunique() method and filtering out those columns.",
          "Correlation analysis <br>The code calculates the correlation between features and the target variable using the Spearman correlation coefficient. It then filters out features with low correlation and visualizes the correlation coefficients.",
          "Feature importance analysis <br>The code uses Extra Trees Regressor and LightGBM models to analyze feature importances and visualize the top important features.",
          "Date feature transformation <br>The 'Date' column is transformed into separate 'Month' and 'Day' columns for both the train and test datasets.",
          "Missing value imputation <br>The 'Province_State' column is filled with empty strings to handle missing values.",
          "Combining categorical features <br>The 'Country_Region' and 'Province_State' columns are combined into a new 'Place' column for both the train and test datasets.",
          "Creating new feature 'NbDay' <br>A new feature 'NbDay' is created based on the number of days since the first occurrence of a country in the dataset.",
          "Exponential feature creation <br>Exponential features are created using the 'NbDay' feature, and the product of two columns is calculated to create new features.",
          "Label Encoding <br>The 'EJ' column in the train and test data is encoded using LabelEncoder, and the first category is converted to float.",
          "Random Under Sampling <br>A function 'random_under_sampler' is defined to balance the number of samples for each class label by undersampling the majority class.",
          "Balanced Log Loss <br>A custom function 'balanced_log_loss' is defined to calculate the weighted log loss for imbalanced classes.",
          "Ensemble Modeling <br>An ensemble class 'Ensemble' is defined to fit multiple classifiers including XGBoost and TabPFNClassifier, and predict probabilities using weighted averaging.",
          "Date Transformation <br>The 'Epsilon' column in the 'greeks' dataframe is transformed from date format to ordinal format.",
          "Random Over Sampling <br>The RandomOverSampler is used to balance the class distribution by oversampling the minority class.",
          "Label Encoding <br>The 'EJ' column in the training and test data is encoded as 0 for 'A' and 1 for 'B'.",
          "Feature Name Cleaning <br>Spaces in the feature names are removed.",
          "Box-Cox Transformation <br>The Box-Cox transformation is applied to the features, and the transformed features are used for modeling.",
          "Brute Force Feature Generation <br>New features are generated using combinations of existing features, including squared features, square root features, addition, subtraction, multiplication, and division of features.",
          "LightGBM Model Training <br>Multiple LightGBM models are trained with different sets of hyperparameters using k-fold cross-validation.",
          "Random Forest Model Training <br>Multiple Random Forest models are trained with different sets of hyperparameters using k-fold cross-validation.",
          "Logistic Regression Model Training <br>Multiple Logistic Regression models are trained with different sets of hyperparameters using k-fold cross-validation.",
          "Data Checking <br>The function 'data_checking' is used to check the data types, duplicates, missing values, unique values, and descriptive statistics of the dataset.",
          "Outlier Detection <br>The function 'count_outliers' is used to detect outliers in numerical columns using the IQR method.",
          "Histogram and Boxplot Visualization <br>The function 'histogram_boxplot' is used to visualize the distribution and outliers of numerical features in both training and testing datasets.",
          "Categorical Variable Analysis <br>The code visualizes the distribution of the 'Age' variable by the categorical variable 'Sex' using boxplots.",
          "Feature Correlation Analysis <br>The function 'feature_correlation' is used to explore the correlation of features in both training and testing datasets.",
          "Pairplot Visualization <br>The code uses 'sns.pairplot' to visualize pairwise relationships in the dataset, with different colors for different values of the 'Sex' variable.",
          "Feature Transformation <br>The function 'transform' is used to perform feature transformation including log transformation, square root transformation, and box-cox transformation on numerical features.",
          "Feature Selection <br>The function 'feature_selection' is used to select the best features based on their performance in predicting the target variable 'Age'.",
          "Dropping columns <br>The code drops the 'Unnamed: 0' column from the 'df_synthetic' dataframe using the 'drop' method.",
          "Sampling <br>The code samples 969 rows from the 'legit_df' dataframe to balance the classes, and then concatenates the sampled data with the 'fraud_df' and 'df_synthetic' dataframes.",
          "Conversion of categorical variable to integer <br>The first category of the 'EJ' column in the maindf and testdf is converted to integer (0 or 1)",
          "Conversion of time information to integers or NaN <br>The time information in 'greeksdf' is converted to integers using datetime.strptime, and 'Unknown' values are replaced with NaN",
          "One-Hot Encoding <br>The categorical variables in the dataset were converted into binary vectors using one-hot encoding.",
          "Polynomial Features <br>Polynomial features of degree 3 were created from selected numerical features to capture non-linear relationships.",
          "Imputation <br>Missing values in the dataset were imputed using the median strategy.",
          "Feature Scaling <br>The features were scaled using Min-Max scaling to ensure all features have the same scale.",
          "Memory optimization <br>The code uses a function to reduce memory usage of the dataframes by downcasting numeric columns to lower memory types.",
          "Column type conversion <br>The code converts certain columns to string type using the astype method.",
          "Feature selection <br>The code selects specific columns for training and testing datasets based on a predefined list of columns.",
          "Weighted sampling <br>The code samples a subset of the training data using weights based on the frequency of a categorical column.",
          "Categorical encoding <br>The code uses the pycaret setup function to handle categorical features during model training.",
          "Date column manipulation <br>The Date column is manipulated to remove hyphens and converted to an integer format for both the training and testing datasets.",
          "Handling missing values <br>The missing values in the 'Lat' and 'Long' columns of the testing dataset are filled with specific values.",
          "Dropping Columns <br>Columns 'game_num', 'event_id', 'event_time', 'player_scoring_next', 'team_scoring_next' were identified as useless and dropped from the dataset.",
          "Imputing Null Values <br>Null values in the dataset were imputed with 0.",
          "Target Creation <br>Two target dataframes 'team_A_scoring_within_10sec' and 'team_B_scoring_within_10sec' were created and removed from the original dataset.",
          "Outlier removal <br>The clear_outlier_jumps function removes outliers by calculating the difference between consecutive values and removing those that exceed a certain threshold.",
          "Data Splitting <br>The code splits the training data into 80% for training and 20% for validation.",
          "Polynomial Feature Generation <br>The PolynomialFeatures method is used to generate polynomial features for the linear regression model.",
          "Trigonometric Feature Generation <br>The code creates sine and cosine features for the 'sincos' model.",
          "Converting categorical columns to datetime <br>The code converts the 'date_time' column from object type to datetime type for both train and test data.",
          "Creating new time-related features <br>The code creates new features such as 'quarter', 'month', 'hour', 'working_hours', 'is_weekend', 'time', and 'day_of_week' based on the 'date_time' column.",
          "Calculating new derived features <br>The code calculates new features like 'SMC' (specific moisture content) and 'Dew_Point' based on existing columns 'absolute_humidity' and 'relative_humidity'.",
          "Creating lag features <br>The code creates lag features by subtracting the temperature and absolute humidity values at different time intervals from the current values.",
          "Converting GameClock to seconds <br>The GameClock feature is converted to seconds using the strtoseconds function.",
          "Converting PlayerHeight to a dense representation <br>The PlayerHeight feature is converted to a dense representation in inches using the lambda function.",
          "Calculating TimeDelta <br>The TimeDelta feature is calculated as the time difference between TimeHandoff and TimeSnap.",
          "Calculating PlayerAge <br>The PlayerAge feature is calculated based on the PlayerBirthDate and TimeHandoff.",
          "Converting WindSpeed to a dense representation <br>The WindSpeed feature is converted to a dense representation in float using the strtofloat function.",
          "Processing GameWeather <br>The GameWeather feature is processed to categorize different weather conditions.",
          "Identifying Rusher <br>The IsRusher feature is created to identify the player carrying the ball.",
          "Processing Orientation and Dir <br>The Orientation and Dir features are processed to convert to sine and cosine values.",
          "Calculating diffScoreBeforePlay <br>The diffScoreBeforePlay feature is calculated as the difference between HomeScoreBeforePlay and VisitorScoreBeforePlay.",
          "Mapping Turf <br>The Turf feature is mapped to categorize different types of playing surfaces.",
          "Splitting OffensePersonnel and DefensePersonnel <br>The OffensePersonnel and DefensePersonnel features are split into separate categories for each player.",
          "Correlation analysis <br>A heatmap of the correlation matrix was created to identify the relationships between different features in the dataset.",
          "Train-test split <br>The dataset was split into training and testing sets to evaluate the model's performance on unseen data.",
          "XGBoost hyperparameter tuning <br>Hyperparameters such as colsample_bytree, alpha, reg_lambda, learning_rate, max_depth, min_child_weight, n_estimators, and subsample were tuned for the XGBoost model.",
          "LightGBM hyperparameter tuning <br>Hyperparameters such as lambda_l1, lambda_l2, num_leaves, feature_fraction, bagging_fraction, bagging_freq, min_child_samples, learning_rate, early_stopping_round, and num_iterations were tuned for the LightGBM model.",
          "PyTorch model training <br>A custom linear regression model was implemented using PyTorch, and the model was trained using the AdamW optimizer with mean squared error loss.",
          "Ensembling <br>The code combines predictions from multiple models using a weighted average to create a final prediction.",
          "Handling Class Imbalance <br>The code uses various sampling methods such as SMOTE, Near-Miss, RandomOverSampler, and RandomUnderSampler to address the class imbalance in the target variable.",
          "Dimensionality Reduction <br>The code applies dimensionality reduction techniques such as PCA and TruncatedSVD to deal with the multiple anonymized variables in the dataset.",
          "Model Comparison <br>The code compares the performance of multiple machine learning models using cross-validation and evaluates them based on the specified metric (roc_auc).",
          "Hyperparameter Tuning <br>The code performs hyperparameter tuning using GridSearchCV to find the optimal parameters for the selected model.",
          "One-Hot Encoding <br>Applied One-Hot Encoding to categorical variables 'direction', 'week_day', and 'month_name' in the training and test datasets.",
          "Standardization <br>Performed standardization on numerical features 'x', 'y', 'day_no', 'hr_day', 'min_day', and 'year_dayno' using StandardScaler.",
          "Datetime Features Extraction <br>Extracted various datetime features such as 'weekday_no', 'week_day', 'day_no', 'hr_day', 'min_day', 'month_name', 'month', and 'year_dayno' from the 'time' column.",
          "Stratified K-Fold Cross-Validation <br>Used to split the training data into folds while maintaining the same distribution of the target variable in each fold.",
          "XGBoost Model Training <br>Utilized XGBoost classifier for training the model with specified parameters for boosting and regularization.",
          "Feature Selection <br>Selected specific columns from the dataset to be used as features for training the model.",
          "Data Cleaning <br>Handled any missing or inconsistent data in the training and test datasets.",
          "Categorical Variable Manipulation <br>Potentially converted categorical variables into numerical format for model training.",
          "Aggregated Feature Extraction <br>Aggregated features were created using the AggFeatureExtractor class, which calculated statistical measures like mean and standard deviation for specific groups of columns.",
          "Data Preprocessing <br>Preprocessing steps included creating new features based on existing ones, creating categorical combinations, and encoding categorical features using OneHotEncoder and CountEncoder.",
          "Data Splitting <br>The data was split into training and validation sets using KFold cross-validation with 5 splits.",
          "Ensemble Modeling <br>Ensemble modeling was performed using XGBoost, LightGBM, and CatBoost models, and the predictions were combined using OptunaWeights to find the optimal weights for the models.",
          "Feature Importance Visualization <br>Feature importance was visualized for each model using bar plots to identify the top important features.",
          "K-Fold Cross Validation <br>The code uses MultilabelStratifiedKFold to split the data into train/test sets for cross-validation.",
          "TabularDataLoaders <br>TabularDataLoaders is used to create data loaders for tabular data, which includes handling categorical and continuous variables, filling missing values, and normalizing the data.",
          "Feature Shuffling <br>The code shuffles the features using the sample method to create a random order of the data.",
          "Categorical Variable Handling <br>The code uses the Categorify processor to handle categorical variables in the tabular data.",
          "Continuous Variable Normalization <br>The code uses the Normalize processor to normalize continuous variables in the tabular data.",
          "Removing columns with unique values <br>Columns with the same value for all rows or with only one unique value are removed from the dataset.",
          "Converting categorical variables to numerical <br>The 'OverTime' and 'Attrition' columns are converted from categorical (Yes/No) to numerical (1/0) values.",
          "Memory optimization <br>A function is defined to reduce memory usage of the dataframe by converting data types to the smallest possible integer or float type.",
          "One-hot encoding <br>Categorical variables are one-hot encoded using the get_dummies function.",
          "SMOTE oversampling <br>The SMOTE algorithm is used to oversample the minority class in the training data to address class imbalance.",
          "Feature Removal <br>Removed 'Soil_Type7' and 'Soil_Type15' columns from both train and test datasets",
          "Feature Renaming <br>Renamed specific columns in both train and test datasets",
          "Feature Transformation <br>Transformed 'Aspect' column values in both train and test datasets",
          "Feature Creation <br>Created new features 'mnhttn_dist_hydrlgy' and 'ecldn_dist_hydrlgy' in both train and test datasets",
          "Feature Transformation <br>Transformed 'Hillshade' columns values in both train and test datasets",
          "Feature Creation <br>Created new features 'Soil_Count', 'Wilderness_Area_Count', 'Hillshade_mean', and 'amp_Hillshade' in both train and test datasets",
          "Feature Aggregation <br>Aggregated mean, standard deviation, minimum, and maximum values of existing features in both train and test datasets",
          "Splitting datetime into date, year, month, day, hour, minute, and second <br>Applied lambda functions to split the datetime column into separate date and time components",
          "Mapping numerical values to categorical values for 'season' and 'weather' columns <br>Used the map function to convert numerical values to corresponding categorical labels",
          "Extracting weekday from date <br>Used the datetime library to extract the weekday from the date column",
          "Log transformation of the target variable 'count' <br>Applied log transformation to the 'count' variable to handle outliers",
          "Creating bar plots for categorical variables <br>Used seaborn to create bar plots for 'year', 'month', 'day', 'hour', 'minute', 'second', 'season', 'weather', 'holiday', and 'workingday' columns",
          "Creating point plots for time-related variables <br>Used seaborn to create point plots for 'hour' with 'workingday', 'hour' with 'holiday', 'hour' with 'season', and 'hour' with 'weather' columns",
          "Creating regression plots for continuous variables <br>Used seaborn to create regression plots for 'temp', 'atemp', 'windspeed', and 'humidity' variables",
          "Calculating correlation matrix and creating heatmap <br>Used pandas corr function to calculate correlation matrix and seaborn heatmap to visualize the correlations",
          "Text cleaning <br>The clean function is used to remove new line characters and distracting single quotes from the text data.",
          "Text tokenization and lemmatization <br>The doc_to_words function tokenizes the text data and then lemmatizes the tokens to reduce them to their base form.",
          "One-hot encoding for text data <br>The get_dict_with_split and get_dict_without_split functions are used to create dictionaries for the text data, which are then one-hot encoded using DictVectorizer.",
          "Handling missing values <br>The df_fillna function is used to fill missing values in specific columns with 'NAN'.",
          "Feature extraction from categorical variables <br>The df_append_cats function is used to create new columns by duplicating the existing category_name column.",
          "Splitting categorical variables <br>The split_cats function is used to split the category_name column into three separate columns.",
          "Handling specific values in descriptions <br>The fill_na_descriptions function is used to replace specific values in the item_description column with values from another column.",
          "Feature scaling <br>The MinMaxScaler is used to scale the condition and shipping columns.",
          "Min-Max Scaling <br>Applied Min-Max scaling to the features to bring them within a specific range, likely to improve the performance of the models.",
          "Train-Test Split <br>Split the dataset into training and development sets to evaluate the model's performance.",
          "One-Hot Encoding <br>Applied to the 'EJ' column using the category_encoders library's OneHotEncoder",
          "Imputation <br>SimpleImputer used to fill missing values in the dataset",
          "Standardization <br>StandardScaler used to standardize the numerical features",
          "Power Transformation <br>PowerTransformer used to transform the numerical features",
          "Feature Selection <br>RidgeClassifier used to assign weights to the models for the VotingClassifier",
          "One-hot encoding <br>The code applies one-hot encoding to all the features by hashing and one-hot-encoding everything, resulting in a list of hashed and one-hot-encoded 'indices' for each instance.",
          "Feature interaction <br>The code adds feature interactions by combining different features using hashing, such as combining channel and os, channel and app, os and app, and ip, app, and device.",
          "Time-based feature engineering <br>The code creates new features based on the time of the click, such as distinguishing between night, morning, afternoon, and evening, and using these as binary features.",
          "Handling missing values <br>The code checks for potential NaN or Inf values in the train and test data using np.isnan and np.isinf functions.",
          "Categorical variable manipulation <br>The code replaces categorical variables 'EJ' with numerical values using replace and astype functions.",
          "Feature selection <br>The code selects a subset of features from the train data using the final_features list.",
          "Data cleaning <br>The code removes potential NaN or Inf values from the train and test data using np.isnan and np.isinf functions.",
          "Feature creation <br>The code creates new features 'outlier', 'Alpha', 'Beta', 'Gamma', 'Delta' by merging data from greek_loader with train_loader.",
          "Memory reduction <br>The code reduces memory usage of the dataframe by modifying the data type of columns to reduce memory usage.",
          "Feature creation <br>Creation of new features such as '_totalDistance', '_specialKills', '_healthItems', '_headshotKillRate', '_killsOverWalkDistance', '_killsOverDistance', '_killPlaceOverMaxPlace', '_rank.minor.killPlace', '_rank.winPlacePerc', '_pred.winPlacePerc', '_percSum', '_percMean', and '_sum' based on existing features.",
          "Categorical variable manipulation <br>Mapping of 'matchType' to 'matchTypeCat' using a lambda function.",
          "Feature selection <br>Dropping constant columns and columns such as 'Id', 'matchId', and 'groupId'.",
          "Cumulative sum <br>The 'area' feature is created by multiplying 'time_step' and 'u_in', and then calculating the cumulative sum within each 'breath_id'.",
          "Lag features <br>Lag features are created for 'u_in' and 'u_out' by shifting the values by 1, 2, 3, and 4 time steps within each 'breath_id'.",
          "Max and mean differences <br>Differences between 'u_in' and the maximum and mean values of 'u_in' within each 'breath_id' are calculated.",
          "One-hot encoding <br>Categorical variables 'R' and 'C' are converted to dummy variables using one-hot encoding.",
          "Cross features <br>New features 'cross' and 'cross2' are created by multiplying different input variables together.",
          "Memory Usage Reduction <br>The code reduces the memory usage of the data by converting numeric columns to the smallest possible data type that can accommodate the range of values in each column. This is achieved by iterating through the columns and checking the minimum and maximum values to determine the appropriate data type for each column.",
          "Feature Selection <br>The code drops the 'id' column from both the train and test datasets, indicating that the 'id' column is not considered as a feature for modeling.",
          "Normalization <br>The code applies normalization to the data during the setup of the PyCaret classification model. Normalization is a feature transformation method that scales the values of numeric features to a standard range, typically between 0 and 1, to ensure that all features contribute equally to the model training process.",
          "Transformation <br>The code applies transformation to the data during the setup of the PyCaret classification model. Transformation refers to the process of transforming the features using techniques such as power transformations, quantile transformations, and more to make the data more suitable for modeling.",
          "Standard Scaling <br>Applied to features using StandardScaler()",
          "Robust Scaling <br>Applied to features using RobustScaler()",
          "Min-Max Scaling <br>Applied to features using MinMaxScaler()",
          "Logistic Regression <br>Used for label error estimation and as a baseline model",
          "Ridge Regression <br>Used as a baseline model",
          "Linear Discriminant Analysis <br>Used as a baseline model",
          "Stochastic Gradient Descent (SGD) Classifier <br>Used as a baseline model",
          "Naive Bayes Classifier <br>Used as a baseline model",
          "Multi-layer Perceptron (MLP) Classifier <br>Used as a baseline model",
          "XGBoost Classifier <br>Used as a baseline model",
          "Dropping Columns <br>The code drops the 'County' and 'Province_State' columns from the dataframe using the drop() method.",
          "Date Formatting <br>The code converts the 'Date' column to datetime format and then formats it to '%Y%m%d' using the pd.to_datetime() and dt.strftime() methods.",
          "One-Hot Encoding <br>The code applies one-hot encoding to the 'Country_Region' and 'Target' columns using the pd.get_dummies() method.",
          "Concatenation <br>The code concatenates the original dataframe with the one-hot encoded columns using the pd.concat() method.",
          "Duplicate Column Removal <br>The code removes any duplicate columns from the concatenated dataframe using the loc[] and ~final_df.columns.duplicated() methods.",
          "Drop Unnecessary Columns <br>The code drops the 'Id' and 'ForecastId' columns from the final dataframe using the drop() method.",
          "Splitting category name <br>The 'category_name' column is split into three separate columns: 'main_category', 'subcat_1', and 'subcat_2'.",
          "Label Encoding <br>Categorical columns like 'name', 'category_name', 'brand_name', 'main_category', 'subcat_1', and 'subcat_2' are encoded using label encoding.",
          "Handling missing data <br>Missing values in 'category_name', 'brand_name', and 'item_description' are filled with specific values.",
          "Log transformation <br>The 'price' column is transformed using log transformation.",
          "Text preprocessing <br>Text data in 'name' and 'item_description' columns is preprocessed by removing digits, punctuation, and stopwords, and converting to lowercase.",
          "Count Vectorization and TF-IDF <br>Count Vectorization and TF-IDF are applied to 'name' and 'item_description' columns to convert text data into numerical features.",
          "Label Binarization <br>Label Binarization is applied to the 'brand_name' column to convert it into numerical features.",
          "Latent Dirichlet Allocation (LDA) <br>LDA is used to extract topics from the text data in the 'item_description' column.",
          "Label Encoding <br>Applied label encoding to transform categorical variables into numerical values in both the training and test datasets.",
          "Demolitions <br>The function 'demolitions' calculates the number of active players for each team by counting the non-null player positions.",
          "Compute_df <br>The function 'compute_df' computes various features such as distances, speeds, and player-to-ball distances. It also replaces NaN values in player positions with the maximum distance to the ball.",
          "Reducing memory usage of the dataframe <br>The function reduce_mem_usage is used to reduce the memory usage of the dataframe by converting data types to lower memory types where possible.",
          "Handling categorical variables <br>Categorical variables are handled by factorizing them and using the factorized values for analysis.",
          "Feature selection <br>Columns are selected for analysis by retaining only the necessary columns for the analysis.",
          "Date feature transformation <br>The 'Date' feature is transformed using the getTS function, which converts the date to a timestamp. This allows the model to capture the time component of the data.",
          "Categorical variable manipulation <br>The 'Country_Region' and 'Province_State' features are mapped to numerical values using dictionaries. This allows the model to interpret these categorical variables as numerical inputs.",
          "Data cleanup <br>Checking for and handling any missing values in the dataset to ensure data integrity and model compatibility.",
          "Feature selection <br>Selecting specific features ('Country_Region', 'Date', 'Province_State') for input to the model, indicating a deliberate choice of relevant features for prediction.",
          "Handling missing values <br>The code handles missing values by filling them with appropriate values or transforming them into categorical variables.",
          "Log transformation <br>The code applies log transformation to the target variable 'SalePrice' to make its distribution more normal.",
          "Creating new features <br>The code creates new features such as 'TotalSF' by combining existing features like 'TotalBsmtSF', '1stFlrSF', and '2ndFlrSF'.",
          "Label encoding <br>The code uses label encoding to transform categorical variables into numerical format for model training.",
          "Box-Cox transformation <br>The code applies Box-Cox transformation to skewed numerical features to make their distribution more normal.",
          "Lag Features <br>Lag features were created for the target variable 'microbusiness_density' and the 'active' variable with different lag values.",
          "Rolling Mean Features <br>Rolling mean features were created for the lagged target variable 'microbusiness_density' with different window sizes.",
          "Merging External Data <br>External data from 'census' and 'co_est2021-alldata' were merged based on the 'cfips' column to create additional features.",
          "Coordinate Encoding <br>Coordinates were encoded using a combination of exponential transformation and trigonometric functions to create new features.",
          "Rotation Features <br>New features were created by rotating the latitude and longitude coordinates at different angles.",
          "Converting time column to datetime format <br>pd.to_datetime(df['time'],format='%Y/%m/%d %H:%M:%S')",
          "Extracting year, month, weekday, day, hour, and minute from the time column <br>df['time'].dt.year, df['time'].dt.month_name(), df['time'].dt.day_name(), df['time'].dt.day, df['time'].dt.hour, df['time'].dt.minute",
          "Dropping unnecessary columns from the dataset <br>df.drop(['row_id', 'time', 'year', 'congestion', 'month'], axis = 1)",
          "Handling missing values with KNNImputer <br>The code uses KNNImputer from sklearn to fill missing values in the dataset by using the K-nearest neighbors approach.",
          "Label Encoding for categorical variables <br>LabelEncoder from sklearn is used to transform categorical variables into numerical format for model training.",
          "Standard Scaling of features <br>StandardScaler from sklearn is used to scale the features to have a mean of 0 and a standard deviation of 1.",
          "Concatenating important features <br>The code concatenates important features to the scaled features before feeding them into the neural network model.",
          "Feature removal <br>Columns 'Id', 'groupId', 'matchId' were removed from the training data.",
          "Correlation analysis <br>Correlation heatmap and scatter plots were used to analyze the relationship between 'winPlacePerc' and other features.",
          "Data cleaning <br>Rows with missing values in 'winPlacePerc' were removed from the training data.",
          "Feature combination <br>The features 'swimDistance', 'rideDistance', and 'walkDistance' were combined into a new feature 'totalDistance'.",
          "Categorical variable manipulation <br>The 'matchType' column was converted to categorical codes.",
          "Feature combination <br>The features 'boosts' and 'heals' were combined into a new feature 'health'.",
          "Feature combination <br>The features 'headshotKills', 'roadKills', and 'teamKills' were combined into a new feature 'kills'.",
          "Creation of headshotRate feature <br>Calculated as the ratio of headshotKills to kills, only for cases where kills are greater than 0",
          "Creation of killStreaksPerc feature <br>Calculated as the ratio of killStreaks to kills, only for cases where kills are greater than 0",
          "Creation of roadKillPerc feature <br>Calculated as the ratio of roadKills to kills, only for cases where kills are greater than 0",
          "Transformation of matchDuration feature <br>Mapped matchDuration values less than 1600 to 0 and greater than or equal to 1600 to 1",
          "Transformation of vehicleDestroys and teamKills features <br>Mapped values greater than 0 to 1 and 0 otherwise",
          "Transformation of matchType feature <br>Mapped matchType values to numerical categories",
          "Creation of n_team feature <br>Counted the number of teams in each match and added as a new feature",
          "Creation of killPlacePerc feature <br>Ranked killPlace values as a percentage within each match",
          "Creation of Points feature <br>Calculated as the sum of killPoints, rankPoints, and winPoints, then ranked as a percentage within each match",
          "Group-wise aggregation of mean, sum, min, and max features <br>Calculated mean, sum, min, and max values for various features within each group and match",
          "Feature selection <br>Selecting specific columns from the dataset for training and evaluation",
          "Categorical variable manipulation <br>Using CatBoostClassifier which inherently handles categorical variables",
          "Automatic extraction of features <br>Using CatBoostClassifier to automatically handle feature extraction and selection",
          "Normalization <br>The 'damageDealt', 'killPoints', 'longestKill', 'matchDuration', 'walkDistance', and 'winPoints' features were normalized by dividing each by its maximum value.",
          "Feature Removal <br>The 'swimDistance', 'killPoints', 'maxPlace', 'roadKills', 'teamKills', 'vehicleDestroys', 'winPoints', 'killPlace', 'revives', 'killStreaks', 'headshotKills', 'DBNOs', and 'assists' features were removed from the dataset.",
          "Missing Value Imputation <br>Missing values in the dataset were filled with the mean of each column.",
          "Concatenation of dataframes <br>The code concatenates the 'train' and 'bulk' dataframes using the pd.concat() method to combine the data for further analysis.",
          "Dropping columns <br>The code drops the 'UDI' column from the 'bulk' dataframe using the drop() method to remove unnecessary data.",
          "Categorical variable manipulation <br>The code converts the columns 'Type' and 'Product ID' to categorical data type using the astype() method to prepare them for modeling.",
          "Creation of new features 'Hour' and 'Day' from 'click_time' <br>DstTrain$Hour <- hour(ymd_hms(DstTrain$click_time))\nDstTrain$Day <- day(ymd_hms(DstTrain$click_time))",
          "Visualization of features using ggplot2 <br>createplot function and its usage to visualize 'Day', 'Hour', 'app', 'device', 'os', and 'channel'",
          "Conversion of 'is_attributed' to factor <br>DstTrainTest$is_attributed <- as.factor(DstTrainTest$is_attributed)\nlevels(DstTrainTest$is_attributed) = make.names(unique(DstTrainTest$is_attributed))",
          "Handling missing values <br>Missing values in the features are filled with the mean of the respective feature columns.",
          "Standardization <br>The features are standardized using RobustScaler to make them have a mean of 0 and a standard deviation of 1.",
          "Feature creation <br>Two new features 'n_missing' and 'std' are created. 'n_missing' represents the count of missing values in the original features, and 'std' represents the standard deviation of the original features.",
          "Outlier removal <br>The code removes outliers from the 'Cover_Type' column by filtering out values above the 99th percentile.",
          "Data visualization <br>The code uses seaborn and matplotlib to visualize the distribution of 'Cover_Type' and the unique values of the target variable.",
          "Correlation analysis <br>The code calculates the correlation matrix and selects the top correlated features with the target variable 'Cover_Type'.",
          "Missing value handling <br>The code checks for missing values in the dataset and fills them with a default value of 1.",
          "Feature creation <br>The code creates new features by counting the occurrences of each value in the original features and adding them as new columns in the dataset.",
          "Dropping redundant columns <br>Columns 'id' and 'Row#' were dropped as they were considered redundant.",
          "Handling missing values <br>SimpleImputer was used to replace missing values with the mean for the concatenated dataset.",
          "Data duplication handling <br>Duplicate rows were dropped from the 'train' dataset and the original dataset.",
          "Synthetic data generation <br>Synthetic data was generated using scikit-learn's make_regression function.",
          "Concatenating datasets <br>The synthetic dataset was concatenated with the original dataset for training and evaluation.",
          "Creating holiday type column <br>A function was created to assign a holiday type to each date, based on predefined holiday dates. The function creates a new column 'HolidayType' and assigns a specific value for each holiday type.",
          "Creating features from date <br>The function 'create_features' was created to extract features like Week, Month, Year, and HolidayType from the 'Date' column. These features can be used to capture seasonal patterns and holiday effects in the data.",
          "Handling categorical features <br>Categorical features were handled using the 'OrdinalEncoder' from scikit-learn, which transforms categorical features into ordinal categories. Missing values were also filled with a constant value.",
          "Handling numeric features <br>Numeric features were handled by filling missing values with a constant value and standardizing the features by removing the mean and scaling to unit variance using 'StandardScaler' from scikit-learn.",
          "Dropping 'row_id' column <br>Removed the 'row_id' column from both the train and test dataframes.",
          "Checking for missing values <br>Checked for missing values in the 'train' dataframe using the 'info' method.",
          "Converting 'date' to datetime type <br>Converted the 'date' column to datetime type for easy handling.",
          "Creating monthly trend <br>Created a monthly trend by calculating the mean of 'num_sold' for each month.",
          "Creating monthly trend by country <br>Created a monthly trend by country by calculating the mean of 'num_sold' for each month and country.",
          "Creating trend by day of the week <br>Created a trend by day of the week by calculating the mean of 'num_sold' for each month and day of the week.",
          "Creating weekend vs. weekday trend comparison <br>Created a comparison of weekend vs. weekday trend by calculating the mean of 'num_sold' for each month and weekend/weekday.",
          "Checking if date is a holiday <br>Checked if the date is a holiday for each entry in the 'train' and 'test' dataframes.",
          "Extracting date features <br>Extracted various date features such as year, quarter, month, week, day, dayofyear, daysinmonth, dayofweek, and weekend from the 'date' column.",
          "Dropping 'date' column <br>Removed the 'date' column from both the train and test dataframes.",
          "Encoding categorical variables <br>Encoded categorical variables using OneHotEncoder and StandardScaler.",
          "Cumulative sum <br>Applied to 'u_in' feature for both train and test data",
          "First value transformation <br>Transformed 'u_in' feature to represent the first value within each 'breath_id' group for both train and test data",
          "Min value transformation <br>Transformed 'u_in' feature to represent the minimum value within each 'breath_id' group for both train and test data",
          "Mean value transformation <br>Transformed 'u_in' feature to represent the mean value within each 'breath_id' group for both train and test data",
          "Median value transformation <br>Transformed 'u_in' feature to represent the median value within each 'breath_id' group for both train and test data",
          "Max value transformation <br>Transformed 'u_in' feature to represent the maximum value within each 'breath_id' group for both train and test data",
          "Last value transformation <br>Transformed 'u_in' feature to represent the last value within each 'breath_id' group for both train and test data",
          "Area calculation <br>Calculated the area by multiplying 'time_step' and 'u_in' features for both train and test data",
          "Lag feature creation <br>Created lag features for 'u_in' with lag values of 2 and 6 for both train and test data",
          "Exponential weighted mean <br>Calculated the exponential weighted mean for 'u_in' feature with a halflife of 10 for both train and test data",
          "Exponential weighted standard deviation <br>Calculated the exponential weighted standard deviation for 'u_in' feature with a halflife of 10 for both train and test data",
          "Exponential weighted correlation <br>Calculated the exponential weighted correlation for 'u_in' feature with a halflife of 10 for both train and test data",
          "Rolling mean <br>Calculated the rolling mean for 'u_in' feature with a window size of 10 for both train and test data",
          "Rolling maximum <br>Calculated the rolling maximum for 'u_in' feature with a window size of 10 for both train and test data",
          "Rolling standard deviation <br>Calculated the rolling standard deviation for 'u_in' feature with a window size of 10 for both train and test data",
          "Label Encoding <br>Applied to 'country', 'store', and 'product' columns in both training and test datasets",
          "Train-Test Split <br>Performed to split the data into training and test sets for model evaluation",
          "Drop unnecessary columns <br>Columns 'id' and 'Class' are dropped from the training data as they are not needed for modeling.",
          "Train-test split <br>The data is split into training and testing sets using the train_test_split method from scikit-learn.",
          "LightGBM model training <br>A LightGBM model is trained using the training data with specified parameters and early stopping rounds.",
          "Prediction <br>The model is used to make predictions on the test data.",
          "Handling missing values <br>Filling missing values in the 'winPlacePerc' column with the mean of the column",
          "One-hot encoding <br>Using category_encoders library to perform one-hot encoding on the 'matchType' column in both training and test datasets",
          "Normalization <br>Using sklearn's normalize function to normalize the feature variables in both training and test datasets",
          "One-Hot Encoding <br>The 'Sex' column was one-hot encoded using pd.get_dummies() method.",
          "Outlier Removal <br>Outliers were removed from the 'train' dataset based on specific conditions for columns like 'Height', 'Weight', 'Shucked Weight', 'Viscera Weight', and 'Shell Weight'.",
          "Feature Scaling <br>The code uses various methods for feature scaling such as MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, PowerTransformer, and QuantileTransformer.",
          "Feature Visualization <br>The code includes visualization methods like histograms, boxplots, and heatmaps to understand the distribution and relationships of features.",
          "Creating new features based on existing features <br>New features like 'lead_time_per_adult', 'lead_time_per_child', 'lead_time_per_person', 'price__by__special_request', 'total_night' were created based on existing features.",
          "Date manipulation <br>New features like 'arrival_year_month', 'arrival_date', 'arrival', 'days', 'arrival__x__days' were created by manipulating date-related features.",
          "Handling missing values <br>There is no explicit code for handling missing values, but the presence of code to check for NULL values in the dataset indicates that some form of handling may have been done.",
          "Adding time-related features <br>The code adds features such as day of the week, odd/even hour, trend features, and Fourier features based on the date_time column. It also creates lag features for sensor data and calculates the hour of the day, working hours, weekend indicator, and SMC (Saturation Mixing Ratio).",
          "Handling outliers <br>The code includes a function to clean outliers in the dataset, which uses the IQR (Interquartile Range) method to identify and cap extreme values for each feature.",
          "Standardization <br>The code uses the StandardScaler from scikit-learn to standardize the input features before training the models.",
          "Converting datetime to pandas datetime format <br>pd.to_datetime(train.datetime)",
          "Creating new features from datetime (year, month, day, hour) <br>train['year'] = train['datetime'].dt.year, train['month'] = train['datetime'].dt.month, train['day'] = train['datetime'].dt.day, train['hour'] = train['datetime'].dt.hour",
          "One-hot encoding for categorical variables (season, weather, holiday, workingday) <br>pd.get_dummies(train[each], prefix=each, drop_first=False)",
          "Dropping unnecessary fields <br>train.drop(fields_to_drop, axis=1)",
          "Scaling numerical features (temp, humidity, windspeed) <br>(data[each] - mean)/std",
          "Data Imputation <br>The code fills in missing values in the dataset by using the average of neighboring values or by using a specific value.",
          "One-Hot Encoding <br>The code converts categorical variables into binary vectors to be used as input for machine learning algorithms.",
          "Box-Cox Transformation <br>The code applies a power transformation to the data to make the distribution more normal or stabilize the variance across different levels of a variable.",
          "Feature Scaling <br>The code standardizes the values of certain features to have a mean of 0 and a standard deviation of 1, making them comparable and preventing features with large values from dominating the model.",
          "Feature Creation <br>The code creates new features by combining existing features or by performing mathematical operations on them to capture additional information.",
          "Imputing missing values <br>The code uses the 'fillna' method to impute missing values with 0 and then converts the data type to 'float16'.",
          "Scaling position values <br>The code scales the position values by dividing them by specific constants (120.0, 82.0, 40.0) for 'pos_x', 'pos_y', and 'pos_z' respectively.",
          "Scaling boost timer values <br>The code scales the boost timer values by dividing them by 10.0.",
          "Adding new features <br>The code adds new features such as 'p{i}_ball_distance', 'p{i}_pos', and 'p{i}_vel' for each player 'i' based on the existing features.",
          "Label Encoding <br>The code uses label encoding to convert categorical variables into numerical format for 'topic', 'channel', 'content', 'language', 'category', and 'kind' columns.",
          "Text Cleaning <br>The code applies text cleaning techniques such as converting to lowercase, removing tabs and new lines, removing brackets and punctuation, removing numbers, and reducing multiple spaces to single space.",
          "Tokenization <br>The code tokenizes text data using the AutoTokenizer from the transformers library, and applies padding and truncation to ensure a fixed length of tokens.",
          "Mean Pooling <br>The code defines a MeanPooling class to calculate mean pooled embedding vectors from the last hidden state of the model.",
          "Exact Nearest Neighbor Search <br>The code uses Faiss library to perform exact nearest neighbor search with GPU for retrieving similar content based on embeddings.",
          "Threshold Optimization <br>The code performs threshold optimization to find the best threshold for cosine similarity scores, which is used for content recommendation."
         ],
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          11.127025604248047,
          6.926647663116455,
          10.22852611541748,
          4.2998738288879395,
          2.7228968143463135,
          8.782083511352539,
          10.35552978515625,
          0.7702338695526123,
          4.418792247772217,
          8.713650703430176,
          0.3420690596103668,
          2.5791192054748535,
          11.53229808807373,
          10.169928550720215,
          4.623219013214111,
          4.225422382354736,
          4.7646803855896,
          0.7614378333091736,
          0.8381550312042236,
          0.8384415507316589,
          10.009723663330078,
          6.017025470733643,
          5.983144283294678,
          6.025923728942871,
          5.929927825927734,
          5.65531063079834,
          6.766861915588379,
          5.7715277671813965,
          5.876160144805908,
          4.1220479011535645,
          4.1550679206848145,
          4.126550197601318,
          4.089919567108154,
          4.108311176300049,
          4.096189975738525,
          4.1868391036987305,
          4.168347358703613,
          4.145851135253906,
          4.113644123077393,
          4.0534162521362305,
          9.069293975830078,
          9.399133682250977,
          9.357501983642578,
          9.457255363464355,
          9.369561195373535,
          12.143511772155762,
          8.876289367675781,
          7.344920635223389,
          2.0578866004943848,
          4.902817726135254,
          8.784430503845215,
          2.321413040161133,
          10.14771556854248,
          10.489753723144531,
          13.04146957397461,
          8.36230182647705,
          12.36865234375,
          8.180471420288086,
          8.481039047241211,
          3.89841890335083,
          12.192778587341309,
          12.863364219665527,
          8.757821083068848,
          9.212780952453613,
          5.593483924865723,
          5.09520149230957,
          6.920573711395264,
          4.30106782913208,
          4.811438083648682,
          8.196911811828613,
          10.10042667388916,
          10.890450477600098,
          9.354255676269531,
          8.803647994995117,
          4.741732597351074,
          4.6033406257629395,
          4.943113803863525,
          3.4119787216186523,
          9.53397274017334,
          2.333399534225464,
          2.7644758224487305,
          9.925618171691895,
          9.344501495361328,
          9.315378189086914,
          9.212827682495117,
          9.20093059539795,
          9.76931095123291,
          9.850055694580078,
          9.33610725402832,
          10.172685623168945,
          9.185393333435059,
          5.935568332672119,
          5.630930423736572,
          6.009511470794678,
          5.998645305633545,
          6.009522438049316,
          5.9976301193237305,
          6.145952224731445,
          5.793195724487305,
          5.92663049697876,
          5.940108776092529,
          5.781858921051025,
          5.855119228363037,
          5.757967472076416,
          4.431944370269775,
          6.019588470458984,
          12.369797706604004,
          12.071040153503418,
          10.664998054504395,
          8.759629249572754,
          4.2344560623168945,
          4.738381862640381,
          12.216801643371582,
          12.54235553741455,
          6.896267414093018,
          10.849184036254883,
          8.707324981689453,
          4.247345924377441,
          9.05692195892334,
          6.586031436920166,
          6.531519412994385,
          10.340753555297852,
          6.409453392028809,
          6.768764495849609,
          12.224151611328125,
          7.426976680755615,
          8.5404691696167,
          10.495580673217773,
          12.385804176330566,
          10.234094619750977,
          10.642851829528809,
          10.454339981079102,
          8.926727294921875,
          6.164801120758057,
          10.14358901977539,
          8.490449905395508,
          6.590439319610596,
          9.903641700744629,
          5.33412504196167,
          6.325375080108643,
          6.3906707763671875,
          7.431696891784668,
          10.626832008361816,
          8.768877029418945,
          7.939538478851318,
          12.349905014038086,
          9.261590003967285,
          8.68810749053955,
          7.510459899902344,
          5.318428993225098,
          2.375885486602783,
          6.7322096824646,
          6.755330562591553,
          6.656824588775635,
          6.681101322174072,
          6.5743842124938965,
          6.533154010772705,
          6.697376251220703,
          5.971347808837891,
          6.683924674987793,
          6.655517578125,
          1.9238240718841553,
          9.816705703735352,
          3.0005300045013428,
          1.8149372339248657,
          6.8087477684021,
          12.66012191772461,
          2.854280471801758,
          0.38619792461395264,
          3.362283229827881,
          9.235589027404785,
          8.276384353637695,
          13.03897476196289,
          7.477623462677002,
          7.937673568725586,
          10.257055282592773,
          4.249449253082275,
          9.018564224243164,
          8.961343765258789,
          10.292166709899902,
          8.796751976013184,
          10.422750473022461,
          8.125828742980957,
          8.50263786315918,
          11.221718788146973,
          7.602099895477295,
          5.994014263153076,
          2.8198227882385254,
          2.9186055660247803,
          4.841585159301758,
          2.8461129665374756,
          12.645709991455078,
          10.837681770324707,
          7.503324031829834,
          4.252323627471924,
          2.3919219970703125,
          6.872498512268066,
          4.859759330749512,
          9.079022407531738,
          9.128701210021973,
          11.358223915100098,
          2.564345359802246,
          6.217547416687012,
          8.987202644348145,
          4.504817008972168,
          0.7905715107917786,
          7.762253761291504,
          10.258970260620117,
          8.322200775146484,
          11.132966995239258,
          7.571159839630127,
          5.78465461730957,
          4.814423561096191,
          0.32219061255455017,
          1.4990757703781128,
          3.702988862991333,
          9.966383934020996,
          9.732048988342285,
          11.923588752746582,
          6.363960266113281,
          6.550082683563232,
          0.7770324349403381,
          8.00105094909668,
          10.70557975769043,
          7.9813456535339355,
          3.583801507949829,
          4.267130374908447,
          0.7679644227027893,
          6.842329978942871,
          5.524467945098877,
          6.782663822174072,
          8.831143379211426,
          5.940926551818848,
          5.309965133666992,
          10.419751167297363,
          5.013693332672119,
          8.035113334655762,
          0.3188803493976593,
          2.191554069519043,
          2.246732473373413,
          2.0088565349578857,
          9.191438674926758,
          10.248826026916504,
          8.534497261047363,
          9.816462516784668,
          9.358370780944824,
          8.388558387756348,
          10.756265640258789,
          11.944068908691406,
          4.232414245605469,
          3.193052291870117,
          12.899206161499023,
          9.414277076721191,
          7.4258832931518555,
          10.637720108032227,
          4.291597843170166,
          4.997891902923584,
          4.7700300216674805,
          4.811083793640137,
          4.255860328674316,
          5.229773998260498,
          8.221602439880371,
          9.636853218078613,
          4.2609758377075195,
          4.298791885375977,
          4.572449684143066,
          10.117349624633789,
          6.5760178565979,
          4.950396537780762,
          3.0164246559143066,
          2.179960250854492,
          2.1156296730041504,
          5.011590480804443,
          8.67634391784668,
          8.079290390014648,
          5.36873197555542,
          8.523577690124512,
          7.8417816162109375,
          10.321949005126953,
          12.931598663330078,
          9.756945610046387,
          4.347850322723389,
          8.22382926940918,
          2.92838454246521,
          2.4795777797698975,
          5.085956573486328,
          1.5711313486099243,
          9.04278564453125,
          9.036871910095215,
          13.098011016845703,
          5.961433410644531,
          10.347983360290527,
          6.7196807861328125,
          5.6862897872924805,
          0.7681678533554077,
          8.643108367919922,
          10.71197509765625,
          10.700404167175293,
          9.823001861572266,
          6.724358558654785,
          4.951024532318115,
          12.504385948181152,
          8.391629219055176,
          2.6597511768341064,
          10.191986083984375,
          9.848243713378906,
          11.429010391235352,
          9.437004089355469,
          10.744834899902344,
          2.5936367511749268,
          12.407195091247559,
          10.236211776733398,
          7.572105884552002,
          7.521190166473389,
          6.2962493896484375,
          6.175010681152344,
          5.688967704772949,
          8.404461860656738,
          8.342044830322266,
          10.745767593383789,
          10.681589126586914,
          10.457459449768066,
          8.80397891998291,
          4.7094011306762695,
          7.848071575164795,
          6.930354118347168,
          9.3710355758667,
          10.93281364440918,
          12.9783353805542,
          8.123800277709961,
          12.760003089904785,
          12.912283897399902,
          10.512239456176758,
          5.790506839752197,
          5.7292985916137695,
          5.648184776306152,
          5.795541763305664,
          5.593930244445801,
          5.914957523345947,
          8.66895580291748,
          10.432973861694336,
          4.665903568267822,
          7.0327229499816895,
          10.09929370880127,
          12.477944374084473,
          2.534888982772827,
          5.675507545471191,
          9.051505088806152,
          4.225198745727539,
          0.8310006260871887,
          8.994324684143066,
          2.950489044189453,
          1.4970542192459106,
          0.35733383893966675,
          9.961610794067383,
          11.441678047180176,
          8.575162887573242,
          9.987668991088867,
          9.015172958374023,
          10.139693260192871,
          8.85344409942627,
          9.0454683303833,
          11.505782127380371,
          4.901451587677002,
          12.453107833862305,
          12.448124885559082,
          6.187713146209717,
          4.0301103591918945,
          4.096041202545166,
          7.623825550079346,
          10.896355628967285,
          13.019027709960938,
          10.063880920410156,
          4.87689208984375,
          4.9144134521484375,
          6.226341247558594,
          4.360715866088867,
          2.189497947692871,
          2.4228734970092773,
          2.4798858165740967,
          2.7219338417053223,
          10.87807846069336,
          10.097387313842773,
          10.137784957885742,
          4.4777326583862305,
          9.399874687194824,
          9.87159252166748,
          2.984935760498047,
          8.496805191040039,
          6.125244140625,
          6.128942966461182,
          6.154653072357178,
          5.280614376068115,
          12.18919849395752,
          6.793179988861084,
          6.784700870513916,
          3.307483196258545,
          8.380966186523438,
          4.762684345245361,
          4.710396766662598,
          7.7051897048950195,
          9.073779106140137,
          3.398590326309204,
          2.697523593902588,
          3.866487741470337,
          2.8673202991485596,
          12.129793167114258,
          8.228829383850098,
          9.136611938476562,
          7.426777362823486,
          8.374405860900879,
          12.658817291259766,
          7.547633647918701,
          9.450937271118164,
          10.48453426361084,
          4.163685321807861,
          4.889891147613525,
          4.754810333251953,
          5.207521915435791,
          8.752392768859863,
          1.494848370552063,
          9.606192588806152,
          4.234151363372803,
          6.061063766479492,
          4.939222812652588,
          2.5570547580718994,
          7.845372200012207,
          0.3305682837963104,
          2.308351755142212,
          12.538031578063965,
          4.317071437835693,
          1.8059570789337158,
          10.806941986083984,
          8.310736656188965,
          4.764934539794922,
          4.765907287597656,
          3.469127893447876,
          7.656360149383545,
          10.7593355178833,
          10.245387077331543,
          4.371845722198486,
          9.526028633117676,
          13.007386207580566,
          3.00825834274292,
          3.347210645675659,
          5.283990859985352,
          8.304494857788086,
          2.3146209716796875,
          1.5054244995117188,
          3.906625986099243,
          5.129059791564941,
          6.997760772705078,
          5.409083366394043,
          4.808129787445068,
          7.008542060852051,
          7.08331823348999,
          6.966687202453613,
          4.937990665435791,
          11.979734420776367,
          9.619572639465332,
          9.371676445007324,
          9.780921936035156,
          8.381326675415039,
          9.353004455566406,
          9.314720153808594,
          9.332489967346191,
          10.796924591064453,
          11.015421867370605,
          9.85519027709961,
          12.438072204589844,
          2.5134692192077637,
          5.784198760986328,
          5.544093132019043,
          5.300276756286621,
          8.600170135498047,
          10.822870254516602,
          9.728168487548828,
          9.884978294372559,
          8.149141311645508,
          6.882540702819824,
          11.082016944885254,
          6.869348526000977,
          9.716315269470215,
          9.866510391235352,
          10.500761985778809,
          11.327857971191406,
          8.694784164428711,
          12.388653755187988,
          10.514738082885742,
          4.763095378875732,
          10.406725883483887,
          4.921084880828857,
          2.4226906299591064,
          5.121804714202881,
          2.5730035305023193,
          6.735623836517334,
          6.251556873321533,
          9.07868480682373,
          8.991462707519531,
          13.119285583496094,
          5.971066951751709,
          6.561866283416748,
          6.653132438659668,
          7.1837897300720215,
          5.666631698608398,
          6.559995174407959,
          0.791139543056488,
          8.753228187561035,
          8.946660041809082,
          2.966193199157715,
          4.937765598297119,
          10.011103630065918,
          6.2914018630981445,
          4.9304656982421875,
          4.492316722869873,
          8.96103286743164,
          11.465575218200684,
          6.559622287750244,
          4.144917964935303,
          8.683686256408691,
          7.771676063537598,
          6.183681964874268,
          7.609885215759277,
          4.222418785095215,
          0.26923155784606934,
          1.808680534362793,
          1.4915220737457275,
          4.259616374969482,
          2.932816505432129,
          0.29666584730148315,
          10.259628295898438,
          8.952637672424316,
          9.016144752502441,
          13.160183906555176,
          5.984775543212891,
          6.548411846160889,
          6.721549034118652,
          5.853752136230469,
          6.561932563781738,
          6.601749897003174,
          0.7889631986618042,
          6.834316253662109,
          8.719012260437012,
          11.377941131591797,
          6.675487995147705,
          6.334633827209473,
          6.203897953033447,
          6.301397800445557,
          6.36064338684082,
          4.309561252593994,
          10.288788795471191,
          9.957463264465332,
          11.725936889648438,
          4.2383527755737305,
          8.781096458435059,
          10.093563079833984,
          2.8774473667144775,
          8.512807846069336,
          12.60332202911377,
          8.953274726867676,
          6.485438346862793,
          10.036982536315918,
          10.697920799255371,
          4.759304523468018,
          4.553069591522217,
          4.895809173583984,
          0.3128244876861572,
          8.799284934997559,
          6.4114089012146,
          10.447380065917969,
          11.980117797851562,
          6.889221668243408,
          4.6194539070129395,
          1.5104230642318726,
          7.846672534942627,
          9.672048568725586,
          9.724279403686523,
          12.603940963745117,
          8.906086921691895,
          10.75287914276123,
          4.246081829071045,
          2.6475768089294434,
          1.5206445455551147,
          5.974697113037109,
          10.080699920654297,
          10.074091911315918,
          8.166280746459961,
          8.220361709594727,
          8.505413055419922,
          8.414995193481445,
          8.35300064086914,
          8.366312980651855,
          9.475907325744629,
          7.3561015129089355,
          12.25682258605957,
          9.549308776855469,
          12.41840934753418,
          8.815356254577637,
          8.805978775024414,
          8.831897735595703,
          9.50693416595459,
          0.3664425313472748,
          2.416029691696167,
          2.272209644317627,
          12.355112075805664,
          10.969088554382324,
          5.564079284667969,
          4.254683494567871,
          2.354621648788452,
          9.933011054992676,
          2.9920647144317627,
          0.30026888847351074,
          9.297521591186523,
          4.831940174102783,
          0.4202716052532196,
          2.2448811531066895,
          3.3983495235443115,
          2.2878975868225098,
          4.343458652496338,
          6.112577438354492,
          12.219014167785645,
          6.116573333740234,
          2.5611095428466797,
          2.795064926147461,
          6.503418445587158,
          6.241151809692383,
          8.485480308532715,
          6.2283616065979,
          6.307125091552734,
          4.9846649169921875,
          6.460123062133789,
          4.723303318023682,
          12.325958251953125,
          6.811742782592773,
          9.924448013305664,
          8.946401596069336,
          8.558566093444824,
          4.61681604385376,
          7.069869041442871,
          10.093929290771484,
          10.337543487548828,
          4.676128387451172,
          1.9147706031799316,
          4.607660293579102,
          12.191659927368164,
          4.336616039276123,
          4.326838493347168,
          10.221555709838867,
          9.133255958557129,
          1.847454309463501,
          10.33930778503418,
          9.015923500061035,
          8.504307746887207,
          6.011035442352295,
          10.275917053222656,
          5.636377811431885,
          11.561166763305664,
          10.164107322692871,
          4.842881202697754,
          7.585117816925049,
          4.295077323913574,
          7.59695291519165,
          7.115289688110352,
          11.984699249267578,
          4.270134925842285,
          4.657472133636475,
          8.431061744689941,
          8.75976276397705,
          12.145673751831055,
          2.6081855297088623,
          2.271360158920288,
          12.413079261779785,
          8.358672142028809,
          6.918152332305908,
          9.31008529663086,
          6.949137210845947,
          6.951078414916992,
          3.006462812423706,
          6.988606929779053,
          6.88353157043457,
          7.043492794036865,
          4.955052852630615,
          0.33213454484939575,
          1.7653213739395142,
          8.94838809967041,
          8.716266632080078,
          12.472564697265625,
          8.811524391174316,
          7.944662570953369,
          10.874082565307617,
          12.355224609375,
          8.712373733520508,
          9.053370475769043,
          5.201025485992432,
          4.106593608856201,
          6.8710808753967285,
          13.068825721740723,
          4.339814186096191,
          10.306587219238281,
          12.1690034866333,
          4.353931903839111,
          7.536500930786133,
          10.07545280456543,
          9.887206077575684,
          9.698139190673828,
          12.480981826782227,
          7.430783748626709,
          10.62059211730957,
          5.088494777679443,
          8.7822904586792,
          8.376623153686523,
          4.6376423835754395,
          4.256240367889404,
          6.792876243591309,
          3.6137795448303223,
          5.459284782409668,
          4.764481544494629,
          6.337329387664795,
          4.426680088043213,
          4.679971694946289,
          4.718314170837402,
          4.253554344177246,
          4.1968183517456055,
          4.22112512588501,
          7.2295002937316895,
          4.255866050720215,
          4.923975467681885,
          4.167556285858154,
          1.8951456546783447,
          1.9132813215255737,
          9.92129135131836,
          10.43301010131836,
          2.248243570327759,
          6.614095687866211,
          6.625491142272949,
          6.585203647613525,
          6.651428699493408,
          6.5769548416137695,
          8.733040809631348,
          4.151460647583008,
          12.65970230102539,
          10.98873519897461,
          9.487176895141602,
          12.432415008544922,
          8.791757583618164,
          8.750619888305664,
          2.461761713027954,
          0.7655134201049805,
          6.845660209655762,
          6.644858360290527,
          4.6549973487854,
          8.248186111450195,
          10.43392276763916,
          9.02229118347168,
          3.031879425048828,
          2.290426015853882,
          8.168913841247559,
          4.311051368713379,
          2.3721611499786377,
          6.9257683753967285,
          13.231707572937012,
          13.18696403503418,
          13.21550178527832,
          8.721415519714355,
          7.474650859832764,
          10.408432960510254,
          4.167538166046143,
          5.0041728019714355,
          4.971512317657471,
          10.106208801269531,
          6.571547985076904,
          12.762733459472656,
          10.496050834655762,
          7.699044227600098,
          9.2422513961792,
          5.031705856323242,
          2.3494887351989746,
          0.8020951747894287,
          7.678056716918945,
          6.465587139129639,
          4.695379734039307,
          7.725068092346191,
          6.554476737976074,
          9.551206588745117,
          11.484671592712402,
          8.311579704284668,
          10.218047142028809,
          10.914511680603027,
          10.22028636932373,
          10.251270294189453,
          6.875727653503418,
          6.780755043029785,
          8.68489933013916,
          6.529234409332275,
          5.98762321472168,
          6.166490077972412,
          9.15696907043457,
          8.276103019714355,
          7.989809513092041,
          8.407364845275879,
          8.398387908935547,
          9.644248008728027,
          9.65263843536377,
          9.722585678100586,
          9.754128456115723,
          9.620416641235352,
          12.169562339782715,
          8.96249771118164,
          9.35580825805664,
          9.256149291992188,
          9.52237319946289,
          8.388998985290527,
          8.342263221740723,
          7.647125720977783,
          10.116408348083496,
          9.823866844177246,
          9.910746574401855,
          6.786890506744385,
          6.7942962646484375,
          6.398104667663574,
          0.7930949926376343,
          0.7482724189758301,
          0.8563908338546753,
          0.7107147574424744,
          0.7007049918174744,
          8.991604804992676,
          6.368361949920654,
          10.584959030151367,
          12.367539405822754,
          8.910483360290527,
          9.768400192260742,
          4.837305545806885,
          2.1938931941986084,
          9.077020645141602,
          8.61392879486084,
          8.587702751159668,
          6.972860813140869,
          5.694049835205078,
          6.774650573730469,
          3.203456401824951,
          6.557684421539307,
          6.537388324737549,
          11.964802742004395,
          4.66725492477417,
          6.8518805503845215,
          9.119050025939941,
          4.552695274353027,
          1.8290963172912598,
          2.054378032684326,
          11.440250396728516,
          5.208606719970703,
          10.39433479309082,
          8.681828498840332,
          13.058094024658203,
          8.121561050415039,
          5.519015789031982,
          6.501958847045898,
          5.186466217041016,
          6.035543918609619,
          8.673404693603516,
          4.740867614746094,
          5.057437896728516,
          4.913909912109375,
          5.031754970550537,
          4.900482654571533,
          4.830593585968018,
          4.335545539855957,
          8.453523635864258,
          8.447623252868652,
          8.420166969299316,
          4.16921854019165,
          10.311576843261719,
          2.319934844970703,
          6.898855686187744,
          10.872027397155762,
          12.667179107666016,
          12.6784086227417,
          4.458319664001465,
          10.535242080688477,
          7.510867118835449,
          4.826661109924316,
          9.411605834960938,
          6.338318347930908,
          8.967913627624512,
          7.683265209197998,
          0.2747405767440796,
          2.7622227668762207,
          10.361665725708008,
          12.379725456237793,
          9.742640495300293,
          5.0288286209106445,
          2.9457311630249023,
          8.787382125854492,
          12.669769287109375,
          10.782493591308594,
          12.473209381103516,
          4.752555847167969,
          9.224552154541016,
          6.938765525817871,
          4.796494960784912,
          2.266552209854126,
          1.4962990283966064,
          10.104852676391602,
          4.564798831939697,
          0.7884711623191833,
          2.9832870960235596,
          6.896014213562012,
          5.241631031036377,
          11.64330768585205,
          4.6609320640563965,
          6.795875072479248,
          6.109365940093994,
          2.2545032501220703,
          4.181037902832031,
          4.424267768859863,
          6.110689163208008,
          7.934079170227051,
          10.107576370239258,
          4.860597133636475,
          4.711680889129639,
          11.645814895629883,
          4.107000827789307,
          2.8638522624969482,
          3.188713550567627,
          2.538104772567749,
          2.1366875171661377,
          2.21561861038208,
          2.3089232444763184,
          2.08212947845459,
          8.3797025680542,
          5.050586223602295,
          7.542954921722412,
          9.990808486938477,
          7.9405694007873535,
          8.10116195678711,
          7.858475208282471,
          10.052353858947754,
          10.002808570861816,
          9.467551231384277,
          10.35042667388916,
          10.959635734558105,
          6.485146522521973,
          4.403686046600342,
          8.12095832824707,
          9.672761917114258,
          3.0183584690093994,
          4.669332027435303,
          2.1447973251342773,
          2.2758591175079346,
          0.3764078915119171,
          9.239846229553223,
          9.735936164855957,
          9.602604866027832,
          9.876923561096191,
          13.1098051071167,
          10.056779861450195,
          9.478385925292969,
          4.823738098144531,
          4.837240219116211,
          9.9404935836792,
          4.651679992675781,
          8.399812698364258,
          4.81662654876709,
          11.510055541992188,
          12.364053726196289,
          8.507233619689941,
          4.089977741241455,
          9.39505672454834,
          8.387901306152344,
          10.552621841430664,
          8.575289726257324,
          12.290993690490723,
          8.130593299865723,
          6.684366703033447,
          6.393364429473877,
          10.88472843170166,
          8.767101287841797,
          2.646531343460083,
          9.099198341369629,
          3.9934499263763428,
          12.235477447509766,
          5.015987873077393,
          10.590368270874023,
          6.140195369720459,
          6.115966796875,
          6.133615493774414,
          6.169214248657227,
          6.153481483459473,
          10.40040111541748,
          7.739664554595947,
          8.202786445617676,
          8.944913864135742,
          9.880158424377441,
          6.838860511779785,
          4.056208610534668,
          8.905245780944824,
          8.558527946472168,
          10.495888710021973,
          5.728396892547607,
          8.982919692993164,
          10.347284317016602,
          10.989157676696777,
          6.9021525382995605,
          6.797113418579102,
          6.828299522399902,
          8.702953338623047,
          9.401191711425781,
          8.732748031616211,
          12.325567245483398,
          6.396240711212158,
          4.240472316741943,
          2.5889759063720703,
          6.8795342445373535,
          4.039494037628174,
          13.15514850616455,
          4.388577938079834,
          7.603697299957275,
          10.43071174621582,
          10.015254974365234,
          5.096012115478516,
          10.200174331665039,
          4.634566783905029,
          4.280364990234375,
          9.546921730041504,
          10.083867073059082,
          8.342808723449707,
          4.233910083770752,
          8.18249225616455,
          10.380313873291016,
          4.545360088348389,
          8.360540390014648,
          3.0167343616485596,
          5.020867347717285,
          5.134620666503906,
          0.36252978444099426,
          11.382457733154297,
          6.459582805633545,
          9.235520362854004,
          9.200128555297852,
          9.049065589904785,
          8.803080558776855,
          6.485807418823242,
          9.47264289855957,
          6.577435493469238,
          10.960416793823242,
          12.411218643188477,
          7.576766490936279,
          11.139455795288086,
          10.21924114227295,
          8.295611381530762,
          10.594050407409668,
          12.112435340881348,
          3.995568037033081,
          10.749714851379395,
          8.405592918395996,
          9.430630683898926,
          12.413622856140137,
          9.331435203552246,
          9.72650146484375,
          11.026802062988281,
          3.0804803371429443,
          10.099799156188965,
          9.579977035522461,
          9.114904403686523,
          9.064045906066895,
          12.960450172424316,
          6.8634934425354,
          10.397852897644043,
          2.3412930965423584,
          9.69795036315918,
          7.745759963989258,
          5.597833633422852,
          6.615250110626221,
          11.10831069946289,
          6.229403018951416,
          10.612174987792969,
          2.547851085662842,
          4.5988264083862305,
          6.570454120635986,
          6.38064432144165,
          8.22769832611084,
          7.072654724121094,
          10.080044746398926,
          11.901921272277832,
          8.538745880126953,
          4.210525989532471,
          3.4030232429504395,
          8.557732582092285,
          12.569588661193848,
          8.4158935546875,
          7.54142951965332,
          7.465851306915283,
          8.938115119934082,
          2.562596082687378,
          2.4423649311065674,
          1.7961320877075195,
          8.62138843536377,
          2.548612594604492,
          9.240279197692871,
          9.236041069030762,
          4.323209285736084,
          6.09271764755249,
          2.0737431049346924,
          2.2402350902557373,
          2.3182709217071533,
          10.871572494506836,
          6.804591655731201,
          4.754552841186523,
          4.745649814605713,
          4.289007663726807,
          4.616466045379639,
          4.78257417678833,
          8.203228950500488,
          10.15767765045166,
          9.544777870178223,
          9.8831787109375,
          8.773567199707031,
          10.367071151733398,
          5.248603820800781,
          13.055044174194336,
          4.269894123077393,
          11.422740936279297,
          10.95463752746582,
          8.383539199829102,
          2.646233558654785,
          9.782657623291016,
          8.634430885314941,
          12.26386547088623,
          9.778817176818848,
          12.845355987548828,
          9.739824295043945,
          6.811864852905273,
          2.863231658935547,
          5.2350029945373535,
          5.6971049308776855,
          8.973243713378906,
          8.463262557983398,
          6.970530033111572,
          6.517622947692871,
          7.330940246582031,
          5.6338419914245605,
          7.2481465339660645,
          7.235093593597412,
          5.693691253662109,
          7.342139720916748,
          7.088968753814697,
          5.696500301361084,
          6.948328018188477,
          7.141467094421387,
          7.114358425140381,
          4.24570894241333,
          2.9367778301239014,
          1.52871835231781,
          1.570573329925537,
          2.202528953552246,
          1.8395555019378662,
          2.574293613433838,
          0.7611410021781921,
          2.493845224380493,
          1.5287953615188599,
          10.263059616088867,
          4.939636707305908,
          8.680356979370117,
          0.3325037360191345,
          2.200382709503174,
          8.414820671081543,
          10.882547378540039,
          10.199604034423828,
          6.365511417388916,
          9.81773853302002,
          2.739473819732666,
          1.7791004180908203,
          3.8655714988708496,
          0.3507433533668518,
          10.348640441894531,
          6.289619445800781,
          10.342268943786621,
          10.335577964782715,
          9.842427253723145,
          10.129504203796387,
          11.4287109375,
          10.63596248626709,
          2.5772898197174072,
          9.318437576293945,
          9.275646209716797,
          5.166781425476074,
          7.2529449462890625,
          5.050935745239258,
          7.23347282409668,
          6.276116847991943,
          8.885588645935059,
          9.450618743896484,
          8.671741485595703,
          4.1547064781188965,
          4.630311012268066,
          4.619290828704834,
          4.627946376800537,
          4.351280212402344,
          10.726197242736816,
          10.636616706848145,
          10.639659881591797,
          12.252416610717773,
          10.527409553527832,
          10.481283187866211,
          12.339244842529297,
          4.2031378746032715,
          4.133843421936035,
          2.9044859409332275,
          10.502351760864258,
          13.117384910583496,
          4.930910587310791,
          5.0152106285095215,
          8.217623710632324,
          10.324277877807617,
          6.55621337890625,
          8.224421501159668,
          11.605345726013184,
          9.898529052734375,
          8.431648254394531,
          11.072711944580078,
          6.906033992767334,
          11.325654983520508,
          6.814157962799072,
          10.047863960266113,
          9.022882461547852,
          6.589197635650635,
          6.586224555969238,
          6.643075942993164,
          10.537904739379883,
          6.226558208465576,
          11.386167526245117,
          8.983925819396973,
          4.713809013366699,
          4.8316192626953125,
          4.807347774505615,
          4.232897758483887,
          4.180540561676025,
          2.639791488647461,
          2.7214317321777344,
          2.66325306892395,
          2.505680799484253,
          2.545658588409424,
          2.540361166000366,
          2.4444634914398193,
          10.112070083618164,
          9.01130199432373,
          10.911260604858398,
          9.441222190856934,
          10.071533203125,
          10.125207901000977,
          9.903191566467285,
          9.053808212280273,
          12.402328491210938,
          4.043265342712402,
          10.607229232788086,
          10.46428394317627,
          8.998678207397461,
          10.455759048461914,
          9.109735488891602,
          6.52987813949585,
          6.655501365661621,
          11.452566146850586,
          10.218552589416504,
          8.416714668273926,
          8.364994049072266,
          10.039246559143066,
          10.934706687927246,
          8.317131996154785,
          12.398163795471191,
          4.0580153465271,
          7.553259372711182,
          9.039566993713379,
          4.333123683929443,
          6.498566627502441,
          6.241976261138916,
          9.257501602172852,
          6.047493934631348,
          6.102669715881348,
          9.047614097595215,
          8.796247482299805,
          10.165575981140137,
          12.917143821716309,
          8.747203826904297,
          4.944545745849609,
          7.852258205413818,
          9.538041114807129,
          4.311113357543945,
          10.923847198486328,
          6.915167331695557,
          10.081754684448242,
          6.817251205444336,
          6.639572620391846,
          6.225988388061523,
          6.232339859008789,
          6.186444282531738,
          5.773611068725586,
          6.056548118591309,
          10.044465065002441,
          6.428555011749268,
          6.167957305908203,
          6.20379638671875,
          6.159940242767334,
          8.426063537597656,
          10.201695442199707,
          3.493635654449463,
          4.767327308654785,
          8.68628978729248,
          12.811906814575195,
          9.512656211853027,
          10.120753288269043,
          10.11159896850586,
          8.757746696472168,
          4.648819446563721,
          10.080971717834473,
          12.514396667480469,
          4.904828071594238,
          7.303775787353516,
          6.946590900421143,
          4.644965171813965,
          4.227533340454102,
          12.072625160217285,
          6.737823009490967,
          9.783689498901367,
          12.793376922607422,
          10.063035011291504,
          2.950608015060425,
          9.495523452758789,
          8.518817901611328,
          8.455838203430176,
          8.857831001281738,
          4.745381832122803,
          9.917457580566406,
          11.51209545135498,
          8.931110382080078,
          6.154147624969482,
          6.100097179412842,
          6.147877216339111,
          6.175457000732422,
          8.32339859008789,
          8.649449348449707,
          10.148054122924805,
          9.234865188598633,
          6.506749629974365,
          6.632155895233154,
          6.694343090057373,
          6.637373447418213,
          6.61473274230957,
          6.698402404785156,
          6.640753746032715,
          6.448218822479248,
          6.583061218261719,
          6.450107574462891,
          6.4392242431640625,
          6.488258361816406,
          6.417559623718262,
          6.420800685882568,
          6.410320281982422,
          9.06697940826416,
          2.953503131866455,
          9.850434303283691,
          2.9499034881591797,
          2.1001479625701904,
          2.8375465869903564,
          12.433968544006348,
          10.526873588562012,
          4.89536714553833,
          10.816596031188965,
          6.924501419067383,
          4.263688087463379,
          4.547463417053223,
          7.665295600891113,
          8.577892303466797,
          12.049744606018066,
          8.270546913146973,
          6.842261791229248,
          5.016002178192139,
          9.04694652557373,
          8.873510360717773,
          10.883742332458496,
          10.218768119812012,
          5.054359436035156,
          13.130422592163086,
          10.342787742614746,
          4.324472904205322,
          4.989401340484619,
          6.50850248336792,
          12.690545082092285,
          4.655401706695557,
          4.670200347900391,
          6.85962438583374,
          9.07246208190918,
          10.694427490234375,
          10.653077125549316,
          1.8009108304977417,
          2.3002023696899414,
          3.4702963829040527
         ],
         "xaxis": "x",
         "y": [
          5.171086311340332,
          0.9533178806304932,
          6.522958755493164,
          -1.9102429151535034,
          0.9343259930610657,
          2.9472601413726807,
          7.703753471374512,
          -2.0006916522979736,
          -1.3489127159118652,
          7.130103588104248,
          5.306893825531006,
          3.3161473274230957,
          -0.5666924118995667,
          -1.2296513319015503,
          3.602187156677246,
          2.0680270195007324,
          -1.7891837358474731,
          -1.947388768196106,
          -1.93449866771698,
          -1.9379442930221558,
          6.437108039855957,
          1.2213983535766602,
          1.1888129711151123,
          2.1795337200164795,
          2.4679207801818848,
          2.7009849548339844,
          -1.6254537105560303,
          2.6811604499816895,
          2.7061336040496826,
          2.857250452041626,
          2.865438938140869,
          2.86205792427063,
          2.8498032093048096,
          2.8614532947540283,
          2.870211362838745,
          2.9256398677825928,
          2.9195899963378906,
          2.8906166553497314,
          2.872934103012085,
          2.9272408485412598,
          3.3979928493499756,
          6.069337844848633,
          6.130496025085449,
          6.062360763549805,
          6.10108757019043,
          0.21807248890399933,
          2.6362245082855225,
          2.345287322998047,
          1.7510195970535278,
          0.28665390610694885,
          7.15071439743042,
          3.7165956497192383,
          -0.1811188906431198,
          -0.6677792072296143,
          -0.19908884167671204,
          -1.8013017177581787,
          -0.14570710062980652,
          0.7087539434432983,
          3.066985607147217,
          2.8976433277130127,
          -0.03718707337975502,
          0.10336927324533463,
          7.096969127655029,
          6.197663307189941,
          0.29339075088500977,
          -1.4208050966262817,
          0.5872393250465393,
          -1.5152524709701538,
          -1.4416093826293945,
          -1.7463150024414062,
          -0.13767267763614655,
          5.2326130867004395,
          5.823376655578613,
          2.6990785598754883,
          3.3558459281921387,
          3.419833183288574,
          3.1891441345214844,
          3.83259654045105,
          -0.28440648317337036,
          3.5323281288146973,
          3.388092279434204,
          -2.136012315750122,
          6.136911869049072,
          6.270138740539551,
          6.257472038269043,
          6.225755214691162,
          6.121954917907715,
          5.90875768661499,
          5.921858787536621,
          7.5905680656433105,
          6.578201770782471,
          2.2935376167297363,
          2.871455669403076,
          2.6469626426696777,
          2.7158474922180176,
          2.61812424659729,
          2.6789515018463135,
          2.7402143478393555,
          2.2686843872070312,
          2.284669876098633,
          2.2431023120880127,
          2.3024885654449463,
          2.1612961292266846,
          2.190070867538452,
          2.157085418701172,
          2.062500476837158,
          0.23045210540294647,
          0.022743457928299904,
          7.135530471801758,
          -1.2917839288711548,
          2.188086986541748,
          3.464478015899658,
          0.46417248249053955,
          -0.008239952847361565,
          -1.8344879150390625,
          5.36496114730835,
          7.227953910827637,
          -1.5286527872085571,
          7.132330894470215,
          0.5291335582733154,
          1.2450426816940308,
          7.815361499786377,
          1.2443617582321167,
          0.4048319160938263,
          0.42315685749053955,
          2.20096492767334,
          1.1156502962112427,
          7.3512773513793945,
          -0.2805372476577759,
          6.364915370941162,
          0.7966606616973877,
          1.006332278251648,
          7.295638084411621,
          2.833016872406006,
          6.6157097816467285,
          -1.7233834266662598,
          3.1963934898376465,
          -2.1955626010894775,
          0.2498914748430252,
          0.49684959650039673,
          0.38626959919929504,
          2.298391342163086,
          7.694377899169922,
          -1.3661906719207764,
          1.4384565353393555,
          0.3057088553905487,
          6.325784683227539,
          1.25149667263031,
          2.0316765308380127,
          -1.4971216917037964,
          3.5442018508911133,
          2.4730260372161865,
          2.507054328918457,
          2.506747245788574,
          2.624359369277954,
          2.88480806350708,
          3.0294835567474365,
          3.0690975189208984,
          2.8590033054351807,
          2.6126956939697266,
          2.704603433609009,
          1.8841297626495361,
          -2.197741985321045,
          6.433867454528809,
          1.9552123546600342,
          -1.8697750568389893,
          0.19686217606067657,
          6.322473049163818,
          5.242059707641602,
          3.809828519821167,
          6.872216701507568,
          2.737456798553467,
          -0.17157205939292908,
          0.7310277223587036,
          -1.456739902496338,
          6.840329647064209,
          -1.551551342010498,
          3.336200714111328,
          2.737081289291382,
          -2.3367559909820557,
          7.118993759155273,
          7.810298919677734,
          -1.5941104888916016,
          2.4727323055267334,
          -0.5105976462364197,
          2.3581087589263916,
          2.396500587463379,
          6.138147830963135,
          2.9344141483306885,
          -1.2040563821792603,
          6.040524959564209,
          0.5010099411010742,
          7.427525997161865,
          0.9460091590881348,
          -1.881650686264038,
          3.204728603363037,
          -1.7363479137420654,
          -1.8962290287017822,
          3.388932466506958,
          -1.3214302062988281,
          5.048712730407715,
          0.9865309596061707,
          0.07442164421081543,
          -1.3619022369384766,
          0.3498694598674774,
          -1.9826431274414062,
          -1.8571021556854248,
          7.65130615234375,
          0.5557715892791748,
          -0.44364699721336365,
          1.4322749376296997,
          2.639559745788574,
          3.3066632747650146,
          5.326245307922363,
          3.0482516288757324,
          3.7846312522888184,
          -2.196065902709961,
          -2.168646812438965,
          -0.3997749090194702,
          1.7564375400543213,
          -0.31341004371643066,
          -1.993409514427185,
          0.8046017289161682,
          -0.5557551980018616,
          2.349932909011841,
          3.3263745307922363,
          -1.5330390930175781,
          -2.001009941101074,
          -1.757907748222351,
          0.2972366213798523,
          0.9782927632331848,
          -1.828586220741272,
          2.2841756343841553,
          -1.3396854400634766,
          7.833260536193848,
          -1.635151743888855,
          -1.8320527076721191,
          5.333044528961182,
          2.843353271484375,
          2.7033042907714844,
          2.4175689220428467,
          7.103315353393555,
          7.676453590393066,
          2.6101157665252686,
          1.3246278762817383,
          1.3828331232070923,
          1.1242895126342773,
          5.288388252258301,
          -0.336670845746994,
          0.423884779214859,
          3.761342763900757,
          -0.22915521264076233,
          6.754533767700195,
          0.6613690257072449,
          7.876007556915283,
          0.054526232182979584,
          0.1486627161502838,
          0.2777142822742462,
          0.23336374759674072,
          -1.6470867395401,
          0.7641232013702393,
          -1.7999356985092163,
          -2.1901986598968506,
          1.7412596940994263,
          0.4779571294784546,
          -1.2452490329742432,
          6.656905651092529,
          -0.33101412653923035,
          -1.8657047748565674,
          6.474717140197754,
          3.050208806991577,
          3.0531768798828125,
          -1.0500848293304443,
          3.286132574081421,
          3.075485944747925,
          2.898932933807373,
          3.005584239959717,
          3.1351864337921143,
          7.6837310791015625,
          -0.0019346390618011355,
          1.4169912338256836,
          0.46506160497665405,
          -1.7269413471221924,
          6.40458345413208,
          3.2259223461151123,
          -0.8472739458084106,
          3.1366870403289795,
          7.084909439086914,
          6.507265567779541,
          -0.04985656589269638,
          0.33173611760139465,
          6.451368808746338,
          1.1097204685211182,
          0.5608075857162476,
          -1.9944040775299072,
          6.84091854095459,
          0.695558488368988,
          0.700340211391449,
          -2.159179925918579,
          0.4093882441520691,
          -1.7526103258132935,
          -0.2181348204612732,
          -1.731055498123169,
          1.0656559467315674,
          6.602341175079346,
          -2.2387924194335938,
          4.969170093536377,
          -1.4741500616073608,
          7.446888446807861,
          0.9707263112068176,
          0.38018926978111267,
          6.049560070037842,
          1.0402741432189941,
          0.9796515107154846,
          2.5890674591064453,
          2.830498218536377,
          2.7794110774993896,
          2.2407901287078857,
          2.300746440887451,
          0.7752441167831421,
          0.8045943975448608,
          1.000923991203308,
          2.6003122329711914,
          3.559495687484741,
          1.2151044607162476,
          -1.8317276239395142,
          0.038436636328697205,
          7.436367034912109,
          0.21870316565036774,
          0.9756870269775391,
          0.31912684440612793,
          0.24837155640125275,
          7.316134929656982,
          2.221337080001831,
          2.2839365005493164,
          2.5663771629333496,
          2.198899269104004,
          2.306849479675293,
          2.4349238872528076,
          -1.8554649353027344,
          7.6829400062561035,
          -1.6868149042129517,
          0.8148770928382874,
          -2.311140298843384,
          0.22834181785583496,
          1.0141819715499878,
          1.8401345014572144,
          7.24628210067749,
          -1.895735502243042,
          -1.9455769062042236,
          6.425253391265869,
          6.380331039428711,
          3.0469582080841064,
          5.291641712188721,
          -2.1725428104400635,
          4.979276657104492,
          -1.725738763809204,
          -1.7607394456863403,
          3.3212528228759766,
          -2.225796937942505,
          3.0184316635131836,
          6.5874786376953125,
          -0.6101184487342834,
          3.8889479637145996,
          0.10949186980724335,
          0.23939739167690277,
          1.882767677307129,
          0.012966246344149113,
          0.211018368601799,
          0.9153156280517578,
          7.431797504425049,
          0.06469936668872833,
          -0.10499588400125504,
          2.74550199508667,
          3.0107040405273438,
          1.8683958053588867,
          2.190920352935791,
          3.520794153213501,
          3.257899761199951,
          3.402141571044922,
          3.365122079849243,
          7.587958335876465,
          -1.993324875831604,
          6.647620677947998,
          -0.07580049335956573,
          7.135446548461914,
          6.832876205444336,
          6.432611465454102,
          -1.7827181816101074,
          1.8617362976074219,
          1.8926233053207397,
          1.9183406829833984,
          0.11693205684423447,
          -0.11290281265974045,
          -0.1913614571094513,
          -0.4668375551700592,
          2.357884168624878,
          -1.7035534381866455,
          -0.12450393289327621,
          -0.8649104237556458,
          0.5768619179725647,
          -1.7202277183532715,
          3.831768751144409,
          3.09391188621521,
          3.749751329421997,
          2.8279693126678467,
          0.39121609926223755,
          1.0445771217346191,
          6.82201623916626,
          2.28918719291687,
          -1.5127977132797241,
          0.5620858073234558,
          0.9285106062889099,
          -2.0370802879333496,
          7.721463203430176,
          0.12617729604244232,
          -1.4942080974578857,
          -1.7328590154647827,
          0.7942605018615723,
          7.140140056610107,
          3.0881388187408447,
          -0.5030642151832581,
          -1.4417791366577148,
          0.14428064227104187,
          -1.8121764659881592,
          3.0982918739318848,
          -1.8472236394882202,
          5.332913398742676,
          2.744643449783325,
          -0.05061168968677521,
          -1.567965030670166,
          1.9961525201797485,
          -0.4585377275943756,
          -1.7486722469329834,
          0.08872460573911667,
          3.7728168964385986,
          3.713700532913208,
          0.9250612854957581,
          -0.49651089310646057,
          6.62143087387085,
          0.0031711577903479338,
          6.390746116638184,
          -0.03063899837434292,
          3.208909034729004,
          3.720651149749756,
          0.808463454246521,
          -1.7489283084869385,
          3.008113384246826,
          3.0885426998138428,
          3.742906093597412,
          0.10812148451805115,
          0.24651238322257996,
          -0.20129422843456268,
          -0.7684683203697205,
          0.177117258310318,
          0.4307887852191925,
          0.7180131077766418,
          -1.7123427391052246,
          0.3944215774536133,
          1.3411983251571655,
          1.3967432975769043,
          1.0650634765625,
          1.1366275548934937,
          1.3926719427108765,
          1.40912663936615,
          1.4355028867721558,
          7.5301833152771,
          -0.6697633266448975,
          6.232923984527588,
          0.1999228447675705,
          1.1671135425567627,
          2.886050224304199,
          2.897585868835449,
          2.8616554737091064,
          -1.854401707649231,
          -0.3917337954044342,
          -1.413292646408081,
          -1.3207124471664429,
          -1.7629272937774658,
          -0.02248299866914749,
          -0.832348108291626,
          -0.5041056871414185,
          -1.4123690128326416,
          -1.3828455209732056,
          7.259936332702637,
          5.068264007568359,
          -1.757394552230835,
          -0.19083793461322784,
          0.9653688669204712,
          3.4684438705444336,
          7.472546100616455,
          1.6720868349075317,
          3.5238518714904785,
          -1.442161202430725,
          0.9877167344093323,
          1.0250972509384155,
          0.19824010133743286,
          6.892325401306152,
          6.561450958251953,
          0.024465836584568024,
          0.33928975462913513,
          0.45402517914772034,
          1.1514052152633667,
          1.175461769104004,
          0.33992281556129456,
          -0.3413986265659332,
          -1.9774937629699707,
          6.904269695281982,
          -1.8868061304092407,
          6.404510974884033,
          0.3208509683609009,
          -2.26837420463562,
          2.0747931003570557,
          -1.8593755960464478,
          -1.2042051553726196,
          7.355953693389893,
          4.956775665283203,
          -0.299763947725296,
          -1.3513442277908325,
          2.5398919582366943,
          1.2124876976013184,
          0.27880993485450745,
          0.445565789937973,
          -1.9351149797439575,
          5.375617980957031,
          2.117729902267456,
          2.959756851196289,
          -1.925659418106079,
          6.386631011962891,
          5.34604549407959,
          6.606806755065918,
          7.140591621398926,
          6.556041717529297,
          0.014571147970855236,
          0.32602056860923767,
          0.4410408139228821,
          1.0422089099884033,
          0.4264511466026306,
          -0.34500038623809814,
          -0.29125550389289856,
          -1.980042815208435,
          -1.6278609037399292,
          6.825451374053955,
          5.0178608894348145,
          0.996232271194458,
          1.0121822357177734,
          1.248447060585022,
          1.1016860008239746,
          2.0297958850860596,
          0.1424294412136078,
          7.682380676269531,
          -2.164008140563965,
          -0.5753237009048462,
          2.1555521488189697,
          -1.6620296239852905,
          -0.12793181836605072,
          6.273423671722412,
          6.508733749389648,
          0.26958727836608887,
          6.961182117462158,
          0.37785404920578003,
          -2.330718994140625,
          7.557160377502441,
          3.5303516387939453,
          -1.7199108600616455,
          -0.9074272513389587,
          5.334892749786377,
          7.1259446144104,
          1.6637773513793945,
          -1.205225944519043,
          -0.3622470200061798,
          -1.7190626859664917,
          -1.7463892698287964,
          3.0801749229431152,
          0.7993272542953491,
          1.4491673707962036,
          1.4557911157608032,
          0.13918639719486237,
          7.176025390625,
          7.557096004486084,
          -1.4437090158462524,
          3.0793211460113525,
          3.1080567836761475,
          2.071037769317627,
          -0.11288730055093765,
          6.702620029449463,
          0.31611260771751404,
          0.2646595239639282,
          -1.664275884628296,
          3.0097293853759766,
          2.940558671951294,
          2.951695442199707,
          -0.2588231861591339,
          1.106479525566101,
          0.21661792695522308,
          -1.6559964418411255,
          -0.09280379116535187,
          2.981105089187622,
          7.279823303222656,
          7.0391950607299805,
          -0.37289711833000183,
          5.285871505737305,
          3.2570226192474365,
          3.109423875808716,
          0.24255035817623138,
          5.26891565322876,
          1.962567687034607,
          2.1031386852264404,
          3.361497402191162,
          -2.1884353160858154,
          6.436927795410156,
          5.3458943367004395,
          7.66529655456543,
          -1.8641105890274048,
          5.262584209442139,
          3.52412748336792,
          3.827803134918213,
          2.7920844554901123,
          -1.2905083894729614,
          1.6125028133392334,
          -0.2911224067211151,
          1.426119327545166,
          3.3730242252349854,
          3.330533027648926,
          3.1894702911376953,
          2.904811382293701,
          2.467622995376587,
          1.820130467414856,
          0.5346922874450684,
          -1.3909862041473389,
          2.0996882915496826,
          3.5138938426971436,
          -0.24526439607143402,
          1.008842945098877,
          -2.2177858352661133,
          6.651160717010498,
          -1.624805212020874,
          -0.00718756765127182,
          0.8420994877815247,
          -0.15312328934669495,
          -0.48208266496658325,
          -0.8416541218757629,
          1.8748208284378052,
          0.01941319741308689,
          0.1255623698234558,
          0.41202986240386963,
          -1.881071925163269,
          6.751662254333496,
          6.377749443054199,
          1.976807713508606,
          7.592261791229248,
          7.106386661529541,
          2.4364829063415527,
          2.7589149475097656,
          7.635335445404053,
          2.837675094604492,
          -0.5872288346290588,
          -1.1784271001815796,
          3.2711105346679688,
          0.8928380608558655,
          2.2014718055725098,
          1.4661948680877686,
          0.587677001953125,
          -0.26294052600860596,
          2.2627453804016113,
          3.6740307807922363,
          -1.8113843202590942,
          7.275135517120361,
          0.25454118847846985,
          3.2358906269073486,
          3.546379327774048,
          -0.18158535659313202,
          0.554715096950531,
          3.240884780883789,
          -0.08776926249265671,
          3.2403175830841064,
          3.249504327774048,
          6.289763927459717,
          3.233219861984253,
          3.2424473762512207,
          3.257441759109497,
          -1.957383394241333,
          5.318674564361572,
          1.9945238828659058,
          3.336916923522949,
          2.7451915740966797,
          -0.02884439192712307,
          7.219517230987549,
          0.5978546738624573,
          -0.5988649129867554,
          -0.11561736464500427,
          2.802969455718994,
          7.230905532836914,
          0.766817569732666,
          0.04060785844922066,
          -1.7656928300857544,
          0.07285545021295547,
          0.41823217272758484,
          7.552170276641846,
          -0.2765498459339142,
          0.05281110852956772,
          0.9601772427558899,
          6.826819896697998,
          -2.089959144592285,
          1.40049409866333,
          0.36988043785095215,
          2.2199254035949707,
          7.335502624511719,
          -1.7252743244171143,
          3.1543962955474854,
          0.5139315128326416,
          3.476036787033081,
          2.306752920150757,
          3.2070720195770264,
          3.0215137004852295,
          0.1838429570198059,
          -1.8326703310012817,
          2.0325217247009277,
          3.2374186515808105,
          3.4399120807647705,
          3.5479161739349365,
          2.223510503768921,
          -1.4766147136688232,
          0.39760738611221313,
          0.8870110511779785,
          -1.4636752605438232,
          -1.8304195404052734,
          -1.416433572769165,
          1.8958839178085327,
          1.8703149557113647,
          -2.282996654510498,
          -0.3282955586910248,
          3.7278356552124023,
          4.140700340270996,
          4.148771286010742,
          3.656615734100342,
          3.9210164546966553,
          4.036477088928223,
          7.079797267913818,
          -1.4138704538345337,
          0.20750798285007477,
          5.310428619384766,
          -0.33721455931663513,
          -0.12030036002397537,
          2.902796983718872,
          7.102349281311035,
          3.4789223670959473,
          -2.0103659629821777,
          -1.3723852634429932,
          -0.40429142117500305,
          -1.780022382736206,
          -1.7700343132019043,
          7.651355743408203,
          3.377248764038086,
          6.476438522338867,
          3.663564443588257,
          -1.549170970916748,
          3.5853216648101807,
          3.6745994091033936,
          -1.8285733461380005,
          0.0329587385058403,
          -0.02130032330751419,
          0.04005780071020126,
          6.973965167999268,
          0.9638883471488953,
          7.803457736968994,
          0.14920952916145325,
          -1.3973233699798584,
          -1.6215133666992188,
          7.539290904998779,
          1.2863926887512207,
          0.04865030199289322,
          7.227961540222168,
          0.9996150135993958,
          6.292605876922607,
          -1.8652596473693848,
          2.776602029800415,
          -1.96824049949646,
          0.5015884041786194,
          2.1103408336639404,
          3.646366596221924,
          1.6181081533432007,
          3.19950270652771,
          6.862456798553467,
          4.933596611022949,
          2.2724053859710693,
          -1.764915943145752,
          -0.8928185701370239,
          -1.7481703758239746,
          -1.705798625946045,
          -0.5469160079956055,
          -0.1414778232574463,
          2.470465898513794,
          3.2323102951049805,
          2.710658550262451,
          2.7832467555999756,
          6.8814005851745605,
          2.0633246898651123,
          1.4264476299285889,
          1.1484057903289795,
          1.1531169414520264,
          1.3961375951766968,
          1.439618706703186,
          1.0762726068496704,
          1.029657006263733,
          6.443599224090576,
          0.4296971559524536,
          1.2265619039535522,
          1.3912606239318848,
          1.377962350845337,
          1.3969627618789673,
          1.1426424980163574,
          1.112066626548767,
          1.6483134031295776,
          -1.3592265844345093,
          -1.3833872079849243,
          -1.342759609222412,
          1.0677378177642822,
          1.041845440864563,
          1.7702076435089111,
          -1.9746674299240112,
          -2.025609254837036,
          -1.911656141281128,
          -2.0606091022491455,
          -2.068315029144287,
          7.00978422164917,
          0.520575761795044,
          -1.186498761177063,
          -0.02142973244190216,
          -1.9203071594238281,
          -2.1946139335632324,
          -1.8189873695373535,
          3.556277275085449,
          3.3447072505950928,
          2.6365933418273926,
          2.6075925827026367,
          0.6454729437828064,
          2.786648988723755,
          -1.6503596305847168,
          3.0676729679107666,
          3.272796630859375,
          -0.2786661386489868,
          -0.37043145298957825,
          3.5545692443847656,
          -1.7580735683441162,
          6.715646743774414,
          -1.7501857280731201,
          2.126495838165283,
          1.811697006225586,
          4.9839701652526855,
          -0.7891693711280823,
          7.810853958129883,
          6.919687747955322,
          -0.12116048485040665,
          -1.7898386716842651,
          0.1876360923051834,
          3.24778151512146,
          0.06332331150770187,
          1.908463716506958,
          -1.7605161666870117,
          3.5083258152008057,
          3.194279432296753,
          3.2934255599975586,
          3.215219736099243,
          3.310816764831543,
          3.3047924041748047,
          2.189181327819824,
          2.4087259769439697,
          2.8036017417907715,
          2.960968255996704,
          2.035688638687134,
          7.6497626304626465,
          3.7229299545288086,
          -1.7595877647399902,
          5.491745948791504,
          0.4535922706127167,
          0.43816837668418884,
          0.12779611349105835,
          6.543215751647949,
          0.9090920090675354,
          0.432407021522522,
          -0.3211611211299896,
          1.9288911819458008,
          -1.8062546253204346,
          -1.852881669998169,
          5.368232250213623,
          2.7456560134887695,
          6.003667831420898,
          0.3020271062850952,
          1.4501694440841675,
          -1.3902662992477417,
          6.382720470428467,
          7.078100204467773,
          0.09007355570793152,
          5.438251972198486,
          0.2746262550354004,
          0.17354461550712585,
          6.928022384643555,
          -1.7073086500167847,
          -1.7840654850006104,
          3.651503801345825,
          3.090648889541626,
          -0.13575443625450134,
          3.619985580444336,
          -1.9795737266540527,
          6.3884735107421875,
          0.3211788833141327,
          -0.8459075689315796,
          -0.27191656827926636,
          3.4976768493652344,
          1.0705090761184692,
          0.023199934512376785,
          3.6426737308502197,
          1.9562331438064575,
          1.044885516166687,
          0.3431130647659302,
          -1.473894476890564,
          -0.12156091630458832,
          2.6501896381378174,
          2.5476677417755127,
          -0.30392518639564514,
          1.9588974714279175,
          6.244442462921143,
          3.0264124870300293,
          3.2736473083496094,
          3.5667450428009033,
          3.6375670433044434,
          2.718031167984009,
          3.5484259128570557,
          -1.5366742610931396,
          -1.664372205734253,
          0.8733079433441162,
          -1.2936301231384277,
          2.509608268737793,
          2.561879873275757,
          2.585017204284668,
          -2.2114782333374023,
          -2.282712936401367,
          -0.2580057680606842,
          7.653822898864746,
          5.231963634490967,
          1.4837983846664429,
          -1.7387051582336426,
          -1.6346267461776733,
          6.582948207855225,
          6.400804042816162,
          -1.5084514617919922,
          2.9476840496063232,
          2.9955952167510986,
          5.297698497772217,
          6.792588710784912,
          1.0785506963729858,
          1.5395203828811646,
          -1.9883543252944946,
          -0.17510388791561127,
          -0.08722180128097534,
          -0.22685383260250092,
          3.2855331897735596,
          3.2793660163879395,
          -1.2881975173950195,
          -1.912006139755249,
          -1.7713485956192017,
          -0.03983761742711067,
          4.914900779724121,
          0.2606070339679718,
          2.3909406661987305,
          0.021000705659389496,
          6.9425129890441895,
          -1.8089823722839355,
          -0.7284287810325623,
          2.51816463470459,
          0.04761195182800293,
          1.885252833366394,
          1.1738752126693726,
          1.9548194408416748,
          7.447872161865234,
          7.067037582397461,
          3.405261993408203,
          -1.860708475112915,
          0.001989710610359907,
          -0.18438681960105896,
          0.3414691686630249,
          7.36499547958374,
          1.3773210048675537,
          1.4293917417526245,
          1.4068406820297241,
          1.3354212045669556,
          1.3243887424468994,
          7.783348083496094,
          1.5842299461364746,
          -1.7635095119476318,
          3.0972747802734375,
          6.37032413482666,
          -1.799014687538147,
          0.023684408515691757,
          3.0740580558776855,
          2.7589328289031982,
          7.727223873138428,
          2.7268807888031006,
          3.2188773155212402,
          -0.4322943389415741,
          -0.7064449191093445,
          -0.6253744959831238,
          -0.09563572704792023,
          -0.1193234845995903,
          2.558154344558716,
          6.211654186248779,
          2.827514171600342,
          -0.15171729028224945,
          0.299435555934906,
          2.2252755165100098,
          3.472797393798828,
          -1.780433177947998,
          0.038150910288095474,
          0.06178133189678192,
          0.40813079476356506,
          0.9229958653450012,
          6.689715385437012,
          -2.325974464416504,
          2.517362117767334,
          -1.081660270690918,
          3.5607025623321533,
          2.1774277687072754,
          -0.40634047985076904,
          -1.259900689125061,
          -1.7681002616882324,
          -1.5132532119750977,
          -1.6558706760406494,
          7.746942043304443,
          -1.299005150794983,
          -1.630367398262024,
          6.460155487060547,
          -1.9041885137557983,
          -0.7913288474082947,
          5.291989803314209,
          5.020590305328369,
          0.5396755337715149,
          6.883426189422607,
          7.209771156311035,
          3.3679606914520264,
          2.6753995418548584,
          2.2319602966308594,
          -0.22276757657527924,
          -0.33554744720458984,
          7.496426582336426,
          0.14226551353931427,
          0.8314672708511353,
          5.09635591506958,
          6.259006977081299,
          -1.642405390739441,
          -0.7199220657348633,
          -0.24946948885917664,
          -0.0053397854790091515,
          5.609498500823975,
          -1.8150882720947266,
          -1.3401070833206177,
          0.20257271826267242,
          6.176189422607422,
          -0.5087977647781372,
          -0.8776929378509521,
          3.1430211067199707,
          -0.13075576722621918,
          -0.2562311589717865,
          -1.6796000003814697,
          6.983358860015869,
          0.023988237604498863,
          -1.7643578052520752,
          7.830600261688232,
          3.595860004425049,
          -2.2671847343444824,
          2.5899431705474854,
          2.7441461086273193,
          0.856619119644165,
          -0.5448713302612305,
          2.1499807834625244,
          -0.7014315128326416,
          3.3412396907806396,
          -1.296665906906128,
          0.9233182072639465,
          0.7499731183052063,
          2.1509926319122314,
          0.6508858799934387,
          6.45427942276001,
          -0.40521058440208435,
          -1.939112663269043,
          2.014641046524048,
          3.83736515045166,
          2.917699098587036,
          -0.04642404988408089,
          0.570770800113678,
          1.501867413520813,
          1.4003125429153442,
          7.0761613845825195,
          0.9960839152336121,
          2.652880907058716,
          2.1513378620147705,
          3.0685391426086426,
          1.011496901512146,
          6.867215633392334,
          -1.3985817432403564,
          0.47261905670166016,
          0.4443090260028839,
          3.4105260372161865,
          3.3831214904785156,
          3.203902006149292,
          -0.3634057641029358,
          -1.8414654731750488,
          3.5042192935943604,
          3.6827967166900635,
          2.173957586288452,
          3.654738426208496,
          0.209530770778656,
          -1.70590341091156,
          -2.299879550933838,
          -0.30121877789497375,
          6.441858768463135,
          3.121800184249878,
          7.7635817527771,
          0.7770364880561829,
          -0.06456036120653152,
          -1.4153785705566406,
          4.993530750274658,
          5.21481466293335,
          -1.7607364654541016,
          1.0617951154708862,
          6.90903902053833,
          3.102403163909912,
          -0.014690800569951534,
          -2.1353838443756104,
          -0.21615827083587646,
          -1.9671605825424194,
          -1.7874537706375122,
          6.269450664520264,
          0.7745704650878906,
          0.3327365815639496,
          3.2944607734680176,
          2.3535239696502686,
          0.6235823035240173,
          3.144486665725708,
          1.8246482610702515,
          -0.12035150080919266,
          1.794410228729248,
          1.6775659322738647,
          -0.0839313194155693,
          1.554284691810608,
          1.3297263383865356,
          0.23539194464683533,
          1.6227041482925415,
          1.367246389389038,
          1.3257813453674316,
          2.1637911796569824,
          6.37791109085083,
          3.0794901847839355,
          3.1464414596557617,
          3.0522897243499756,
          1.9486072063446045,
          1.0078285932540894,
          -2.0104243755340576,
          3.2149081230163574,
          3.079277992248535,
          7.662262916564941,
          -1.7998729944229126,
          2.558075428009033,
          5.346134662628174,
          3.5905396938323975,
          -1.738491415977478,
          -0.514778196811676,
          6.500305652618408,
          1.6970067024230957,
          7.274638652801514,
          6.182839870452881,
          2.2955822944641113,
          3.765249252319336,
          5.30519437789917,
          6.141458988189697,
          0.07603051513433456,
          6.410237789154053,
          6.07314920425415,
          -1.3419400453567505,
          6.374640941619873,
          4.98734712600708,
          7.28701639175415,
          0.9961636066436768,
          -1.7528618574142456,
          -1.5185184478759766,
          -0.2958885431289673,
          0.4660746455192566,
          -0.35312941670417786,
          0.6649240255355835,
          1.8179121017456055,
          2.9623615741729736,
          6.237198829650879,
          2.7250218391418457,
          0.061609312891960144,
          3.860750675201416,
          3.842136859893799,
          3.922534465789795,
          2.341867208480835,
          0.6676385998725891,
          0.8364840149879456,
          7.333148002624512,
          0.05627666786313057,
          6.649092197418213,
          6.564621448516846,
          -0.29210546612739563,
          -1.4228618144989014,
          -1.332951307296753,
          6.37867546081543,
          7.775231838226318,
          -0.1100190281867981,
          -1.5679677724838257,
          0.052625786513090134,
          -1.6730966567993164,
          7.7140936851501465,
          0.5299796462059021,
          2.1712100505828857,
          -0.38974466919898987,
          6.3695502281188965,
          -1.668413758277893,
          -0.4812898337841034,
          0.8720065355300903,
          5.070708751678467,
          0.7388788461685181,
          6.454755783081055,
          -1.8650535345077515,
          4.102544784545898,
          3.716085195541382,
          4.181683540344238,
          7.304008960723877,
          0.3733833134174347,
          5.00002908706665,
          -1.9378894567489624,
          -1.196673035621643,
          -0.1557437926530838,
          -1.822471261024475,
          -1.9601409435272217,
          -1.4361140727996826,
          3.007920265197754,
          2.9297051429748535,
          2.827052593231201,
          2.933213949203491,
          2.94464373588562,
          2.959311008453369,
          3.178337812423706,
          -2.2510218620300293,
          3.3086800575256348,
          7.529993534088135,
          -0.1581110656261444,
          -1.5677924156188965,
          -2.3299155235290527,
          1.4251644611358643,
          7.0943522453308105,
          -0.23473094403743744,
          0.016599182039499283,
          0.8155709505081177,
          1.0008667707443237,
          7.294185161590576,
          0.9985942244529724,
          7.0261549949646,
          1.215113639831543,
          1.0580036640167236,
          4.969502925872803,
          6.632562637329102,
          -1.7071882486343384,
          2.7129030227661133,
          6.566176891326904,
          -0.4736761748790741,
          -1.686111569404602,
          -0.11671808362007141,
          0.021320797502994537,
          0.9273539185523987,
          7.195969581604004,
          0.4598422646522522,
          3.229707956314087,
          2.9045827388763428,
          -0.2611065208911896,
          0.18780717253684998,
          0.140968456864357,
          3.39790415763855,
          2.678833246231079,
          -2.262777328491211,
          -0.31791719794273376,
          7.089986324310303,
          -1.8625319004058838,
          -1.5639089345932007,
          -2.082806348800659,
          2.19476056098938,
          -0.756740927696228,
          0.5060253739356995,
          6.401100158691406,
          0.6160756945610046,
          0.7318297624588013,
          0.9322214722633362,
          0.9651168584823608,
          0.9260755181312561,
          0.8546738624572754,
          0.9046324491500854,
          6.248203277587891,
          1.2046403884887695,
          0.972450315952301,
          1.026170253753662,
          1.4858101606369019,
          -1.7378342151641846,
          6.624260902404785,
          3.582120656967163,
          -1.1728694438934326,
          -1.339573621749878,
          -0.029507843777537346,
          -0.2755846381187439,
          -2.232351779937744,
          6.32757568359375,
          2.53818678855896,
          3.7334768772125244,
          6.693118095397949,
          0.23840899765491486,
          -1.595481276512146,
          0.7230168581008911,
          -1.7813204526901245,
          3.7356808185577393,
          2.0386803150177,
          -0.36164018511772156,
          0.9272804856300354,
          -2.0995826721191406,
          -0.15038996934890747,
          -1.2886502742767334,
          3.2535107135772705,
          -0.378991037607193,
          2.6345276832580566,
          2.3749327659606934,
          6.800816535949707,
          -1.9010108709335327,
          -2.2104508876800537,
          -0.5083978772163391,
          3.2136857509613037,
          2.9454774856567383,
          2.9797801971435547,
          3.0133769512176514,
          3.0320496559143066,
          2.8921070098876953,
          2.5717642307281494,
          -2.118201732635498,
          7.047224044799805,
          4.050220489501953,
          4.223716735839844,
          4.268043518066406,
          4.234485626220703,
          4.206238746643066,
          4.2954559326171875,
          4.25191593170166,
          4.009249687194824,
          3.5604023933410645,
          4.057816028594971,
          4.097305774688721,
          4.085048675537109,
          3.9799551963806152,
          3.995856761932373,
          3.9735186100006104,
          7.153225898742676,
          6.401512622833252,
          -2.2304346561431885,
          6.3670783042907715,
          3.58402419090271,
          3.300766944885254,
          0.23157668113708496,
          7.44114351272583,
          -1.442831039428711,
          7.585755825042725,
          -1.6952494382858276,
          -1.5824241638183594,
          3.4504568576812744,
          0.922290027141571,
          2.6406614780426025,
          -0.38563740253448486,
          2.2167999744415283,
          -1.8741662502288818,
          -1.6311205625534058,
          3.362504243850708,
          2.559419631958008,
          7.499603748321533,
          -2.3302958011627197,
          -1.4885756969451904,
          -0.1399027705192566,
          7.840421199798584,
          0.47942912578582764,
          -1.374284029006958,
          0.5698431134223938,
          -0.04285813495516777,
          -1.2084910869598389,
          -1.2725176811218262,
          0.8388266563415527,
          7.157568454742432,
          0.7117542624473572,
          0.807746946811676,
          1.949820637702942,
          2.50020432472229,
          2.189007520675659
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "autosize": false,
        "height": 1400,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "b": 50,
         "l": 50,
         "pad": 4,
         "r": 50,
         "t": 50
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert reduced embeddings into a DataFrame for easier plotting\n",
    "df_embeddings_umap = pd.DataFrame(embeddings_umap, columns=['x', 'y'])\n",
    "\n",
    "# Create an interactive scatter plot using Plotly\n",
    "fig = px.scatter(df_embeddings_umap, x='x', y='y')\n",
    "\n",
    "# Update the hovertemplate to only show 'text'\n",
    "fig.update_traces(hovertemplate='%{hovertext}', hovertext=data)\n",
    "\n",
    "# set size of the plot\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1400,\n",
    "    height=1400,\n",
    "    margin=dict(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=50,\n",
    "        pad=4\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
